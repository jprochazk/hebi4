#![allow(unsafe_op_in_unsafe_fn)]

//! ## Safety in the VM
//!
//! The VM uses a _LOT_ of unsafe code. Too much, in fact. Some of it can likely be
//! removed without introducing performance overhead, but performance is the primary goal
//! of doing this in the first place. The usual approach of "take safe code, and sprinkle
//! in unsafe carefully to improve performance" never worked out for me. My understanding
//! for why is that it was always a death by a thousand cuts: Bounds checks, drop glue,
//! overly large types, extra allocations, runtime assertions, panic machinery, etc.
//!
//! Instead, the VM here attempts to start from a relatively clean slate. We try as much
//! as possible to reduce the chances for error by using type-safe wrappers and limiting
//! the usage of Rust references, which would require us to uphold their strong guarantees
//! everywhere.
//!
//! We're making a wild assumption: The code generated by the Hebi compiler is valid.
//! I believe that the compiler is simple enough that it's possible to comprehensively check
//! all of it. Extensive codegen and runtime tests (including fuzzing) are used to help
//! validate those assumptions.
//!
//! Assuming the compiler is indeed correct, it upholds certain invariants which we can use
//! to reduce runtime overhead in the VM.
//!
//! All accesses to the VM stack are guaranteed to be safe:
//! - The stack always has enough space for the maximum possible number of registers that
//!   a given function needs.
//! - A value on the stack is guaranteed to be initialized before it is first read.
//!
//! A concrete example of this is the `Reg` (register) newtype used to offset `Sp` (stack pointer),
//! the operation will _always_ yield a properly-aligned pointer into a memory located within bounds
//! of an allocation, removing the need for a bounds check, or dynamically growing the stack
//! on demand when writing to it.
//!
//! Not just that, but because a value in a register is never read before it's initialized,
//! we also don't need to pre-initialize the contents of the stack with default values.
//! It is allocated using `alloc_zeroed` (though technically `0` is a valid bit pattern for `Value`),
//! and then pointers to this allocation are handed out as needed.
//!
//! Taking advantage of all of this, Hebi's `mov` instruction can compile down to just:
//! - A decode of the instruction operands (two zero-extended movs from a register)
//! - The actual `mov` of the value between two memory locations
//! - A dispatch of the next instruction (a few `mov`s and a `jmp rax`)
//!
//! That's as low-overhead as it gets, which is the ultimate goal here.

mod array;
mod disasm;

#[macro_use]
pub mod gc;

pub mod value;

use array::{DynArray, DynStack};
use beef::lean::Cow;
use gc::Heap;
use value::{List, String, Table, thin::ThinStr};

use std::{marker::PhantomData, ptr::null, sync::Arc};

use crate::{
    codegen::opcodes::*,
    error::{Error, Result, error},
    span::Span,
    vm::value::{Literal, ValueRaw},
};

/// Represents the result of code generation: A list of functions with
/// one of them marked as the entrypoint.
///
/// Can be executed in a [`Vm`].
///
/// Modules are immutable and internally reference counted.
pub struct Module {
    repr: Arc<ModuleRepr>,
}

struct ModuleRepr {
    main: FnId,
    functions: Box<[FuncInfo]>,
}

impl Module {
    #[inline]
    pub(crate) fn new(main: FnId, functions: impl IntoIterator<Item = FuncInfo>) -> Self {
        // Each module has a list of functions belonging to it. The bytecode of each function
        // declared within a given module can directly reference other functions declared in
        // the same module by their ID. This makes "static" function calls very fast.
        //
        // The problem is that we want to allow more than one module to exist, so when executing
        // a function from some module, we must know where to look for other functions belonging
        // to the same module.
        //
        // For that reason, we give functions a pointer to their parent module. The VM uses this
        // to lookup functions by their ID.

        // First, give the module a stable address:
        let mut this = Self {
            repr: Arc::new(ModuleRepr {
                main,
                functions: Vec::from_iter(functions).into_boxed_slice(),
            }),
        };

        // Next, set module pointers in each function.
        let module = Arc::get_mut(&mut this.repr).expect("some");
        let parent_ptr = module as *mut ModuleRepr as *const ModuleRepr;

        for function in &mut module.functions {
            function.module = parent_ptr;
        }

        // Now the module is ready for execution
        this
    }

    #[inline]
    fn inner(&self) -> *const ModuleRepr {
        &*self.repr
    }

    #[inline]
    fn main(&self) -> FnId {
        self.repr.main
    }

    #[inline]
    pub(crate) fn functions(&self) -> &[FuncInfo] {
        &*self.repr.functions
    }
}

// TODO(opt): colocate data (?)
/// A function "prototype".
///
/// Stores the bytecode and associated constants generated by the compiler.
#[must_use]
pub struct FuncInfo {
    name: Cow<'static, str>,

    nparams: u8,
    nstack: u8,

    /// Bytecode.
    code: Box<[Instruction]>,

    /// Constants referenced by the function's bytecode.
    literals: Box<[Literal]>,

    /// Non-owning pointer to the parent module.
    ///
    /// Only initialized once the function has been placed into a module.
    ///
    /// Modules are always behind `Arc`, so their addresses are stable.
    module: *const ModuleRepr,

    dbg: Option<Box<dbg::FuncDebugInfo>>,
}

impl FuncInfo {
    #[inline]
    pub(crate) fn new(
        name: impl Into<Cow<'static, str>>,
        nparams: u8,
        nstack: u8,
        code: Vec<Instruction>,
        literals: Vec<Literal>,
        dbg: dbg::FuncDebugInfo,
    ) -> Self {
        Self {
            name: name.into(),
            nparams,
            nstack,
            code: code.into_boxed_slice(),
            literals: literals.into_boxed_slice(),
            dbg: Some(Box::new(dbg)),
            module: std::ptr::null(),
        }
    }

    #[inline]
    pub fn name(&self) -> &str {
        self.name.as_ref()
    }

    #[inline]
    pub fn arity(&self) -> u8 {
        self.nparams
    }
}

pub mod dbg {
    use super::*;

    pub struct FuncDebugInfo {
        pub(crate) spans: Box<[Span]>,
        pub(crate) locals: Box<[Local]>,
    }

    pub struct Local {
        pub(crate) span: Span,
        pub(crate) reg: Reg,
    }
}

impl Literal {
    #[inline]
    pub fn int(&self) -> Option<i64> {
        match self {
            Self::Int(v) => Some(*v),
            _ => None,
        }
    }

    #[inline]
    pub fn float(&self) -> Option<f64> {
        match self {
            Self::Float(v) => Some(*v),
            _ => None,
        }
    }

    #[inline]
    pub fn str(&self) -> Option<&str> {
        match self {
            Self::String(v) => Some(v),
            _ => None,
        }
    }
}

#[derive(Clone, Copy)]
struct FuncInfoPtr(*const FuncInfo);

impl FuncInfoPtr {
    #[inline]
    unsafe fn code(self) -> Ip {
        Ip(
            (*self.0)
                .code
                .as_ptr()
                .cast::<Instruction>() // drop the length,
                .cast::<RawInstruction>(), // _then_ cast to raw
        )
    }

    #[inline]
    unsafe fn literals(self) -> Lp {
        Lp(
            (*self.0).literals.as_ptr().cast::<Literal>(), // drop the length
        )
    }

    #[inline]
    unsafe fn nstack(self) -> u8 {
        (*self.0).nstack
    }

    #[inline]
    unsafe fn dbg(self) -> Option<*const dbg::FuncDebugInfo> {
        match &(*self.0).dbg {
            Some(dbg) => Some(&raw const **dbg),
            None => None,
        }
    }
}

impl Sp {
    #[inline(always)]
    pub unsafe fn at(self, r: Reg) -> *mut ValueRaw {
        self.0.offset(r.sz())
    }
}

trait LpIdx {
    fn idx(self) -> isize;
}

impl LpIdx for Lit {
    #[inline(always)]
    fn idx(self) -> isize {
        self.sz()
    }
}

impl LpIdx for Lit8 {
    #[inline(always)]
    fn idx(self) -> isize {
        self.sz()
    }
}

impl Lp {
    #[inline(always)]
    unsafe fn _at(self, r: isize) -> *const Literal {
        self.0.offset(r)
    }

    #[inline(always)]
    pub unsafe fn int_or_float_unchecked(self, r: impl LpIdx) -> ValueRaw {
        let v = self._at(r.idx());

        // NOTE: `ValueRaw` and `Literal` have the same representation,
        // and `Int` and `Float` have the same tag in both, so we can
        // just reinterpret.

        debug_assert!(matches!(&*v, Literal::Int(..) | Literal::Float(..)));
        v.cast::<ValueRaw>().read()
    }

    #[inline(always)]
    pub unsafe fn int_unchecked(self, r: impl LpIdx) -> i64 {
        let v = self._at(r.idx());

        // we're doing this instead of matching to avoid a call to `Literal::drop`.
        // TODO: use some kind of `offset_of` thing instead?

        debug_assert!(matches!(&*v, Literal::Int(..)));
        v.cast::<u64>().add(1).cast::<i64>().read()
    }

    #[inline(always)]
    pub unsafe fn float_unchecked(self, r: impl LpIdx) -> f64 {
        let v = self._at(r.idx());

        // we're doing this instead of matching to avoid a call to `Literal::drop`.
        // TODO: use some kind of `offset_of` thing instead?

        debug_assert!(matches!(&*v, Literal::Float(..)));
        v.cast::<u64>().add(1).cast::<f64>().read()
    }

    #[inline(always)]
    pub unsafe fn str_unchecked(self, r: impl LpIdx) -> *const ThinStr {
        let v = self._at(r.idx());

        // we're doing this instead of matching to avoid a call to `Literal::drop`.
        // TODO: use some kind of `offset_of` thing instead?

        debug_assert!(matches!(&*v, Literal::String(..)));
        v.cast::<u64>().add(1).cast::<ThinStr>()
    }
}

impl Jt {
    #[inline(always)]
    pub unsafe fn at(self, inst: RawInstruction) -> OpaqueOp {
        self.0.offset(inst.tag as isize).read()
    }
}

impl Ip {
    #[inline]
    unsafe fn offset_from_unsigned(self, other: Self) -> usize {
        (self.0).offset_from_unsigned(other.0)
    }

    #[inline]
    unsafe fn offset(self, n: isize) -> Self {
        Ip((self.0).offset(n as isize))
    }

    #[inline(always)]
    unsafe fn next(self) -> Self {
        Self(self.0.offset(1))
    }

    #[inline(always)]
    unsafe fn get(self) -> RawInstruction {
        self.0.read()
    }
}

#[derive(Clone, Copy)]
struct CallFrame {
    callee: FuncInfoPtr,

    /// Stack base of _this_ frame.
    stack_base: u32,

    /// Address of the next instruction to execute
    /// after returning from _this_ call frame.
    ///
    /// Points into the previous call frame's callee,
    /// if there is one.
    return_addr: u32,
}

#[derive(Clone, Copy)]
struct CallFramePtr(*mut CallFrame);

impl CallFramePtr {
    #[inline]
    unsafe fn raw(self) -> *mut CallFrame {
        self.0
    }

    #[inline]
    unsafe fn callee(self) -> FuncInfoPtr {
        (*self.0).callee
    }

    #[inline]
    unsafe fn stack_base(self) -> u32 {
        (*self.0).stack_base
    }

    #[inline]
    unsafe fn return_addr(self) -> u32 {
        (*self.0).return_addr
    }
}

#[repr(C)]
pub(crate) struct Context {
    vm: *mut Vm,
    current_module: *const ModuleRepr,
    error: *mut Option<Error>,
    saved_ip: Ip,
    current_frame: CallFrame,
}

impl Ctx {
    #[inline]
    unsafe fn current_frame(self) -> CallFramePtr {
        CallFramePtr(&raw mut (*self.0).current_frame)
    }

    #[inline]
    unsafe fn push_frame(self, frame: CallFrame) {
        let frames = &mut (*(*self.0).vm).frames;
        frames.push(frame);
    }

    #[inline]
    unsafe fn pop_frame_unchecked(self) -> CallFrame {
        let frames = &mut (*(*self.0).vm).frames;
        frames.pop_unchecked()
    }

    #[inline]
    unsafe fn has_enough_stack_space(self, stack_base: usize, nstack: usize) -> bool {
        let stack = &mut (*(*self.0).vm).stack;
        let remaining = stack.remaining(stack_base);
        remaining >= (nstack as isize)
    }

    #[inline]
    unsafe fn stack_at(self, stack_base: usize) -> Sp {
        let stack = &mut (*(*self.0).vm).stack;
        Sp(stack.offset(stack_base))
    }

    #[inline]
    unsafe fn get_function_in_current_module(self, id: FnId) -> FuncInfoPtr {
        let module = (*self.0).current_module;
        // remove len
        let functions = (*module).functions.as_ptr().cast::<FuncInfo>();
        FuncInfoPtr(functions.offset(id.sz()))
    }

    #[inline]
    unsafe fn write_error(self, error: Error) {
        *(*self.0).error = Some(error);
    }

    #[inline]
    unsafe fn get_span(self, ip: Ip) -> Option<Span> {
        let pc = ip.offset_from_unsigned(self.current_frame().callee().code());
        match self.current_frame().callee().dbg() {
            Some(dbg) => Some((*dbg).spans[pc]),
            None => None,
        }
    }

    #[inline]
    unsafe fn set_saved_ip(self, ip: Ip) {
        let ptr = &raw mut (*self.0).saved_ip;
        ptr.write(ip);
    }

    #[inline]
    unsafe fn saved_ip(self) -> Option<Ip> {
        let ip = (*self.0).saved_ip;
        if ip.0.is_null() { None } else { Some(ip) }
    }

    #[inline]
    unsafe fn get_span_for_saved_ip(self) -> Option<Span> {
        let ip = self.saved_ip()?;
        self.get_span(ip)
    }

    #[inline]
    unsafe fn heap(self) -> *mut Heap {
        &raw mut (*(*self.0).vm).heap
    }

    #[inline]
    unsafe fn maybe_gc(self) {
        // TODO: GC thresholds
        // for now, we always GC

        self.full_gc();
    }

    #[cold]
    unsafe fn full_gc(self) {
        let vm = (*self.0).vm;
        let roots = VmRoots { vm };
        (*vm).heap.collect_with_external_roots(&roots);
    }
}

#[derive(Clone, Copy)]
#[repr(u8)]
pub enum VmError {
    DivisionByZero,
    ArithTypeError,
    UnopInvalidType,
    CmpTypeError,
    NotAList,
    InvalidArrayIndex,
    IndexOutOfBounds,
    NotATable,
    InvalidTableKey,
}

impl VmError {
    // NOTE: assumes `ctx.saved_ip` is set
    unsafe fn with_context(&self, ctx: Ctx) -> Error {
        let span = ctx.get_span_for_saved_ip().unwrap_or_default();
        match self {
            Self::DivisionByZero => error("division by zero", span),
            Self::ArithTypeError => error("type mismatch", span),
            Self::UnopInvalidType => error("invalid type", span),
            Self::CmpTypeError => error("type mismatch", span),
            Self::NotAList => error("not a list", span),
            Self::InvalidArrayIndex => error("value is not a valid array index", span),
            Self::IndexOutOfBounds => error("index out of bounds", span),
            Self::NotATable => error("not a table", span),
            Self::InvalidTableKey => error("value is not a valid table key", span),
        }
    }

    #[inline]
    fn is_external(self) -> bool {
        match self {
            Self::DivisionByZero
            | Self::ArithTypeError
            | Self::UnopInvalidType
            | Self::CmpTypeError
            | Self::NotAList
            | Self::InvalidArrayIndex
            | Self::IndexOutOfBounds
            | Self::NotATable
            | Self::InvalidTableKey => false,
        }
    }
}

macro_rules! vm_exit {
    ($ctx:ident, $ip:ident, $error:ident) => {{
        $ctx.set_saved_ip($ip);
        return Control::error(VmError::$error);
    }};
}

static JT: JumpTable = jump_table! {
    nop,
    mov,

    lmvar,
    smvar,
    lcap,
    scap,
    lidx,
    lidxn,
    sidx,
    sidxn,
    lkey,
    lkeyc,
    skey,
    skeyc,

    lnil,
    lsmi,
    ltrue,
    lfalse,
    lint,
    lnum,
    lstr,
    lclosure,
    lfunc,
    llist,
    ltable,
    jmp,
    istrue,
    istruec,
    isfalse,
    isfalsec,
    islt,
    isle,
    isgt,
    isge,
    iseq,
    isne,
    iseqs,
    isnes,
    iseqn,
    isnen,
    iseqp,
    isnep,
    isltv,
    islev,
    isgtv,
    isgev,
    iseqv,
    isnev,
    addvv,
    addvn,
    addnv,
    subvv,
    subvn,
    subnv,
    mulvv,
    mulvn,
    mulnv,
    divvv,
    divvn,
    divnv,
    unm,
    not,
    call,
    fastcall,
    ret,
    stop,
};

#[cfg(not(debug_assertions))]
#[derive(Clone, Copy)]
#[repr(transparent)]
pub(crate) struct Control(u8);

#[cfg(not(debug_assertions))]
impl Control {
    #[inline]
    fn stop() -> Control {
        Control(0)
    }

    #[inline]
    fn error(code: VmError) -> Control {
        Control(1 & ((code as u8) << 1))
    }

    #[inline]
    fn is_stop(self) -> bool {
        self.0 == 0
    }

    #[inline]
    fn is_error(self) -> bool {
        self.0 & 1 == 1
    }

    #[inline]
    unsafe fn error_code(self) -> VmError {
        core::mem::transmute(self.0 >> 1)
    }
}

#[cfg(debug_assertions)]
#[derive(Clone, Copy)]
#[repr(C)]
pub(crate) enum Control {
    Stop,
    Error(VmError),
    Continue(Sp, Lp, Ip),
}

#[cfg(debug_assertions)]
impl Control {
    #[inline]
    fn stop() -> Control {
        Control::Stop
    }

    #[inline]
    fn error(code: VmError) -> Control {
        Control::Error(code)
    }
}

/// Dispatch instruction at `ip`
#[inline(always)]
unsafe fn dispatch_current(jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    #[cfg(debug_assertions)]
    {
        Control::Continue(sp, lp, ip)
    }

    #[cfg(not(debug_assertions))]
    {
        let inst = ip.get();
        let op = jt.at(inst);
        op(inst, jt, sp, lp, ip, ctx)
    }
}

/// Dispatch instruction at `ip+1`
#[inline(always)]
unsafe fn dispatch_next(jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    let ip = ip.next();
    dispatch_current(jt, sp, lp, ip, ctx)
}

#[inline(always)]
unsafe fn nop(args: Nop, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    dispatch_next(jt, sp, lp, ip, ctx)
}

#[inline(always)]
unsafe fn mov(args: Mov, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    *sp.at(args.dst) = *sp.at(args.src);

    dispatch_next(jt, sp, lp, ip, ctx)
}

#[inline(always)]
unsafe fn lmvar(args: Lmvar, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn smvar(args: Smvar, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn lcap(args: Lcap, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn scap(args: Scap, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn lidx(args: Lidx, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn lidxn(args: Lidxn, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn sidx(args: Sidx, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    let target = *sp.at(args.target);
    let idx = *sp.at(args.idx);
    let src = *sp.at(args.src);

    let ValueRaw::List(list) = target else {
        return not_a_list_error(ip, ctx);
    };

    let ValueRaw::Int(idx) = idx else {
        return invalid_array_index_error(ip, ctx);
    };
    let idx = idx as usize;

    // UNROOTED: reachable through the stack
    let len = list.as_ref().len();
    if idx >= len {
        return index_out_of_bounds_error(ip, ctx);
    }

    list.as_mut().set_raw_unchecked(idx, src);

    dispatch_next(jt, sp, lp, ip, ctx)
}

#[cold]
unsafe fn invalid_array_index_error(ip: Ip, ctx: Ctx) -> Control {
    vm_exit!(ctx, ip, InvalidArrayIndex);
}

#[inline(always)]
unsafe fn sidxn(args: Sidxn, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    let idx = lp.int_unchecked(args.idx) as usize;
    let list = *sp.at(args.target);
    let src = *sp.at(args.src);

    let ValueRaw::List(list) = list else {
        return not_a_list_error(ip, ctx);
    };

    // UNROOTED: reachable through the stack
    let len = list.as_ref().len();
    if idx >= len {
        return index_out_of_bounds_error(ip, ctx);
    }

    list.as_mut().set_raw_unchecked(idx, src);

    dispatch_next(jt, sp, lp, ip, ctx)
}

#[cold]
unsafe fn not_a_list_error(ip: Ip, ctx: Ctx) -> Control {
    vm_exit!(ctx, ip, NotAList);
}

#[cold]
unsafe fn index_out_of_bounds_error(ip: Ip, ctx: Ctx) -> Control {
    vm_exit!(ctx, ip, IndexOutOfBounds);
}

#[inline(always)]
unsafe fn lkey(args: Lkey, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn lkeyc(args: Lkeyc, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn skey(args: Skey, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    let target = *sp.at(args.target);
    let key = *sp.at(args.key);
    let src = *sp.at(args.src);

    let ValueRaw::Table(table) = target else {
        return not_a_table_error(ip, ctx);
    };

    let ValueRaw::String(key) = key else {
        return invalid_table_key_error(ip, ctx);
    };

    // UNROOTED: reachable through the stack
    table.as_mut().insert_raw(key, src);

    dispatch_next(jt, sp, lp, ip, ctx)
}

#[cold]
unsafe fn not_a_table_error(ip: Ip, ctx: Ctx) -> Control {
    vm_exit!(ctx, ip, NotATable);
}

#[cold]
unsafe fn invalid_table_key_error(ip: Ip, ctx: Ctx) -> Control {
    vm_exit!(ctx, ip, InvalidTableKey);
}

#[inline(always)]
unsafe fn skeyc(args: Skeyc, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    let target = *sp.at(args.target);
    let key = lp.str_unchecked(args.key);
    let src = *sp.at(args.src);

    let ValueRaw::Table(table) = target else {
        return not_a_table_error(ip, ctx);
    };

    let heap = ctx.heap();
    // TODO: this is awful. we're allocating _every single time_
    // someone inserts with a const key.
    let key = String::alloc(&*heap, (*key).as_str());

    // UNROOTED: reachable through the stack
    table.as_mut().insert_raw(key, src);

    dispatch_next(jt, sp, lp, ip, ctx)
}

#[inline(always)]
unsafe fn lnil(args: Lnil, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    *sp.at(args.dst) = ValueRaw::Nil;

    dispatch_next(jt, sp, lp, ip, ctx)
}

#[inline(always)]
unsafe fn lsmi(args: Lsmi, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    *sp.at(args.dst) = ValueRaw::Int(args.v.get() as i64);

    dispatch_next(jt, sp, lp, ip, ctx)
}

#[inline(always)]
unsafe fn lint(args: Lint, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    *sp.at(args.dst) = ValueRaw::Int(lp.int_unchecked(args.id));

    dispatch_next(jt, sp, lp, ip, ctx)
}

#[inline(always)]
unsafe fn lnum(args: Lnum, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    *sp.at(args.dst) = ValueRaw::Float(lp.float_unchecked(args.id));

    dispatch_next(jt, sp, lp, ip, ctx)
}

#[inline(always)]
unsafe fn lstr(args: Lstr, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn ltrue(args: Ltrue, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    *sp.at(args.dst) = ValueRaw::Bool(true);

    dispatch_next(jt, sp, lp, ip, ctx)
}

#[inline(always)]
unsafe fn lfalse(args: Lfalse, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    *sp.at(args.dst) = ValueRaw::Bool(false);

    dispatch_next(jt, sp, lp, ip, ctx)
}

#[inline(always)]
unsafe fn lclosure(args: Lclosure, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn lfunc(args: Lfunc, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn llist(args: Llist, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    ctx.maybe_gc();

    let len = args.cap.zx();

    // UNROOTED: immediately written to the stack
    let heap = ctx.heap();
    let list = List::alloc_zeroed(&*heap, len);

    *sp.at(args.dst) = ValueRaw::List(list);

    dispatch_next(jt, sp, lp, ip, ctx)
}

#[inline(always)]
unsafe fn ltable(args: Ltable, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    ctx.maybe_gc();

    let len = args.cap.zx();

    // UNROOTED: immediately written to the stack
    let heap = ctx.heap();
    let table = Table::alloc(&*heap, len);

    *sp.at(args.dst) = ValueRaw::Table(table);

    dispatch_next(jt, sp, lp, ip, ctx)
}

#[inline(always)]
unsafe fn jmp(args: Jmp, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    let ip = ip.offset(args.rel.sz());

    dispatch_current(jt, sp, lp, ip, ctx)
}

#[inline(always)]
unsafe fn istrue(args: Istrue, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    // istrue v
    // jmp offset

    let v = *sp.at(args.v);
    if v.coerce_bool() {
        // skip `jmp`
        let ip = ip.offset(2);
        dispatch_current(jt, sp, lp, ip, ctx)
    } else {
        // execute `jmp`
        let ip = ip.offset(1);
        dispatch_current(jt, sp, lp, ip, ctx)
    }
}

#[inline(always)]
unsafe fn istruec(args: Istruec, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    // isfalsec dst, v
    // jmp offset

    let v = *sp.at(args.v);
    if v.coerce_bool() {
        // set `dst` to `v`
        *sp.at(args.dst) = v;

        // skip `jmp`
        let ip = ip.offset(2);
        dispatch_current(jt, sp, lp, ip, ctx)
    } else {
        // execute `jmp`
        let ip = ip.offset(1);
        dispatch_current(jt, sp, lp, ip, ctx)
    }
}

#[inline(always)]
unsafe fn isfalse(args: Isfalse, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    // isfalse v
    // jmp offset

    let v = *sp.at(args.v);
    if !v.coerce_bool() {
        // skip `jmp`
        let ip = ip.offset(2);
        dispatch_current(jt, sp, lp, ip, ctx)
    } else {
        // execute `jmp`
        let ip = ip.offset(1);
        dispatch_current(jt, sp, lp, ip, ctx)
    }
}

#[inline(always)]
unsafe fn isfalsec(args: Isfalsec, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    // isfalsec dst, v
    // jmp offset

    let v = *sp.at(args.v);
    if !v.coerce_bool() {
        // set `dst` to `v`
        *sp.at(args.dst) = v;

        // skip `jmp`
        let ip = ip.offset(2);
        dispatch_current(jt, sp, lp, ip, ctx)
    } else {
        // execute `jmp`
        let ip = ip.offset(1);
        dispatch_current(jt, sp, lp, ip, ctx)
    }
}

macro_rules! try_cmp_eval {
    ($lhs:ident, $rhs:ident, $ctx:ident, $ip:ident, $op:tt) => {{
        use ValueRaw::*;
        match ($lhs, $rhs) {
            // TODO: other types support comparisons too:
            //       nil, bool, str
            (Int(lhs), Int(rhs)) => lhs $op rhs,
            (Float(lhs), Float(rhs)) => lhs $op rhs,
            (Float(lhs), Int(rhs)) => lhs $op (rhs as f64),
            (Int(lhs), Float(rhs)) => (lhs as f64) $op rhs,
            _ => vm_exit!($ctx, $ip, CmpTypeError),
        }
    }};
}

#[inline(always)]
unsafe fn islt(args: Islt, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    let lhs = *sp.at(args.lhs);
    let rhs = *sp.at(args.rhs);

    if try_cmp_eval!(lhs, rhs, ctx, ip, <) {
        let ip = ip.offset(2);
        dispatch_current(jt, sp, lp, ip, ctx)
    } else {
        let ip = ip.offset(1);
        dispatch_current(jt, sp, lp, ip, ctx)
    }
}

#[inline(always)]
unsafe fn isle(args: Isle, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    let lhs = *sp.at(args.lhs);
    let rhs = *sp.at(args.rhs);

    if try_cmp_eval!(lhs, rhs, ctx, ip, <=) {
        let ip = ip.offset(2);
        dispatch_current(jt, sp, lp, ip, ctx)
    } else {
        let ip = ip.offset(1);
        dispatch_current(jt, sp, lp, ip, ctx)
    }
}

#[inline(always)]
unsafe fn isgt(args: Isgt, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    let lhs = *sp.at(args.lhs);
    let rhs = *sp.at(args.rhs);

    if try_cmp_eval!(lhs, rhs, ctx, ip, >) {
        let ip = ip.offset(2);
        dispatch_current(jt, sp, lp, ip, ctx)
    } else {
        let ip = ip.offset(1);
        dispatch_current(jt, sp, lp, ip, ctx)
    }
}

#[inline(always)]
unsafe fn isge(args: Isge, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    let lhs = *sp.at(args.lhs);
    let rhs = *sp.at(args.rhs);

    if try_cmp_eval!(lhs, rhs, ctx, ip, >=) {
        let ip = ip.offset(2);
        dispatch_current(jt, sp, lp, ip, ctx)
    } else {
        let ip = ip.offset(1);
        dispatch_current(jt, sp, lp, ip, ctx)
    }
}

#[inline(always)]
unsafe fn iseq(args: Iseq, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    let lhs = *sp.at(args.lhs);
    let rhs = *sp.at(args.rhs);

    if try_cmp_eval!(lhs, rhs, ctx, ip, ==) {
        let ip = ip.offset(2);
        dispatch_current(jt, sp, lp, ip, ctx)
    } else {
        let ip = ip.offset(1);
        dispatch_current(jt, sp, lp, ip, ctx)
    }
}

#[inline(always)]
unsafe fn isne(args: Isne, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    let lhs = *sp.at(args.lhs);
    let rhs = *sp.at(args.rhs);

    if try_cmp_eval!(lhs, rhs, ctx, ip, !=) {
        let ip = ip.offset(2);
        dispatch_current(jt, sp, lp, ip, ctx)
    } else {
        let ip = ip.offset(1);
        dispatch_current(jt, sp, lp, ip, ctx)
    }
}

#[inline(always)]
unsafe fn iseqs(args: Iseqs, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn isnes(args: Isnes, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn iseqn(args: Iseqn, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn isnen(args: Isnen, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn iseqp(args: Iseqp, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn isnep(args: Isnep, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn isltv(args: Isltv, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn islev(args: Islev, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn isgtv(args: Isgtv, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn isgev(args: Isgev, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn iseqv(args: Iseqv, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn isnev(args: Isnev, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

macro_rules! try_arith_eval {
    ($dst:ident, $lhs:ident, $rhs:ident, $ctx:ident, $ip:ident, $op:tt) => {
        use ValueRaw::*;
        match ($lhs, $rhs) {
            (Int(lhs), Int(rhs)) => {
                *$dst = Int(lhs $op rhs);
            }
            (Float(lhs), Float(rhs)) => {
                *$dst = Float(lhs $op rhs);
            }
            (Float(lhs), Int(rhs)) => {
                *$dst = Float(lhs $op (rhs as f64));
            }
            (Int(lhs), Float(rhs)) => {
                *$dst = Float((lhs as f64) $op rhs);
            }
            _ => {
                vm_exit!($ctx, $ip, ArithTypeError);
            }
        }
    };
}

// #[inline(never)]
// #[cold]
// unsafe fn arith_type_error(lhs: ValueRaw, rhs: ValueRaw, ip: Ip, ctx: Ctx) -> ArithResult {
//     let lhs_ty = lhs.type_name();
//     let rhs_ty = rhs.type_name();
//     let err = format!("failed to {op_name} {lhs_ty} and {rhs_ty}");
//     let span = ctx.get_span(ip).unwrap_or_default();
//     ctx.write_error(error(err, span));

//     Err(())
// }

#[inline(always)]
unsafe fn addvv(args: Addvv, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    let dst = sp.at(args.dst);
    let lhs = *sp.at(args.lhs);
    let rhs = *sp.at(args.rhs);

    try_arith_eval!(dst, lhs, rhs, ctx, ip, +);

    dispatch_next(jt, sp, lp, ip, ctx)
}

#[inline(always)]
unsafe fn addvn(args: Addvn, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    let dst = sp.at(args.dst);
    let lhs = *sp.at(args.lhs);
    let rhs = lp.int_or_float_unchecked(args.rhs);

    try_arith_eval!(dst, lhs, rhs, ctx, ip, +);

    dispatch_next(jt, sp, lp, ip, ctx)
}

#[inline(always)]
unsafe fn addnv(args: Addnv, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    let dst = sp.at(args.dst);
    let lhs = lp.int_or_float_unchecked(args.lhs);
    let rhs = *sp.at(args.rhs);

    try_arith_eval!(dst, lhs, rhs, ctx, ip, +);

    dispatch_next(jt, sp, lp, ip, ctx)
}

#[inline(always)]
unsafe fn subvv(args: Subvv, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    let dst = sp.at(args.dst);
    let lhs = *sp.at(args.lhs);
    let rhs = *sp.at(args.rhs);

    try_arith_eval!(dst, lhs, rhs, ctx, ip, -);

    dispatch_next(jt, sp, lp, ip, ctx)
}

#[inline(always)]
unsafe fn subvn(args: Subvn, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    let dst = sp.at(args.dst);
    let lhs = *sp.at(args.lhs);
    let rhs = lp.int_or_float_unchecked(args.rhs);

    try_arith_eval!(dst, lhs, rhs, ctx, ip, -);

    dispatch_next(jt, sp, lp, ip, ctx)
}

#[inline(always)]
unsafe fn subnv(args: Subnv, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    let dst = sp.at(args.dst);
    let lhs = lp.int_or_float_unchecked(args.lhs);
    let rhs = *sp.at(args.rhs);

    try_arith_eval!(dst, lhs, rhs, ctx, ip, -);

    dispatch_next(jt, sp, lp, ip, ctx)
}

#[inline(always)]
unsafe fn mulvv(args: Mulvv, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    let dst = sp.at(args.dst);
    let lhs = *sp.at(args.lhs);
    let rhs = *sp.at(args.rhs);

    try_arith_eval!(dst, lhs, rhs, ctx, ip, *);

    dispatch_next(jt, sp, lp, ip, ctx)
}

#[inline(always)]
unsafe fn mulvn(args: Mulvn, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    let dst = sp.at(args.dst);
    let lhs = *sp.at(args.lhs);
    let rhs = lp.int_or_float_unchecked(args.rhs);

    try_arith_eval!(dst, lhs, rhs, ctx, ip, *);

    dispatch_next(jt, sp, lp, ip, ctx)
}

#[inline(always)]
unsafe fn mulnv(args: Mulnv, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    let dst = sp.at(args.dst);
    let lhs = lp.int_or_float_unchecked(args.lhs);
    let rhs = *sp.at(args.rhs);

    try_arith_eval!(dst, lhs, rhs, ctx, ip, *);

    dispatch_next(jt, sp, lp, ip, ctx)
}

// #[cold]
// unsafe fn int_div_by_zero_error(ip: Ip, ctx: Ctx) -> ArithResult {
//     let err = "integer division by zero";
//     let span = ctx.get_span(ip).unwrap_or_default();
//     ctx.write_error(error(err, span));

//     Err(())
// }

// #[cold]
// unsafe fn div_type_error(lhs: ValueRaw, rhs: ValueRaw, ip: Ip, ctx: Ctx) -> ArithResult {
//     let lhs_ty = lhs.type_name();
//     let rhs_ty = rhs.type_name();
//     let err = format!("cannot divide {lhs_ty} and {rhs_ty}");
//     let span = ctx.get_span(ip).unwrap_or_default();
//     ctx.write_error(error(err, span));

//     Err(())
// }

macro_rules! try_div_eval {
    ($dst:ident, $lhs:ident, $rhs:ident, $ctx:ident, $ip:ident) => {
        use ValueRaw::*;
        match ($lhs, $rhs) {
            (Int(lhs), Int(rhs)) => match lhs.checked_div(rhs) {
                Some(v) => *$dst = Int(v),
                None => {
                    vm_exit!($ctx, $ip, DivisionByZero)
                }
            },

            // float div by zero = inf
            (Float(lhs), Float(rhs)) => {
                *$dst = Float(lhs / rhs);
            }
            (Float(lhs), Int(rhs)) => {
                *$dst = Float(lhs / (rhs as f64));
            }
            (Int(lhs), Float(rhs)) => {
                *$dst = Float((lhs as f64) / rhs);
            }
            _ => {
                vm_exit!($ctx, $ip, ArithTypeError);
            }
        }
    };
}

#[inline(always)]
unsafe fn divvv(args: Divvv, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    let dst = sp.at(args.dst);
    let lhs = *sp.at(args.lhs);
    let rhs = *sp.at(args.rhs);

    try_div_eval!(dst, lhs, rhs, ctx, ip);

    dispatch_next(jt, sp, lp, ip, ctx)
}

#[inline(always)]
unsafe fn divvn(args: Divvn, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    let dst = sp.at(args.dst);
    let lhs = *sp.at(args.lhs);
    let rhs = lp.int_or_float_unchecked(args.rhs);

    try_div_eval!(dst, lhs, rhs, ctx, ip);

    dispatch_next(jt, sp, lp, ip, ctx)
}

#[inline(always)]
unsafe fn divnv(args: Divnv, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    let dst = sp.at(args.dst);
    let lhs = lp.int_or_float_unchecked(args.lhs);
    let rhs = *sp.at(args.rhs);

    try_div_eval!(dst, lhs, rhs, ctx, ip);

    dispatch_next(jt, sp, lp, ip, ctx)
}

#[inline(always)]
unsafe fn unm(args: Unm, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    let dst = sp.at(args.dst);
    let rhs = *sp.at(args.rhs);

    use ValueRaw::*;
    match rhs {
        Int(rhs) => {
            *dst = Int(-rhs);
        }
        ValueRaw::Float(rhs) => {
            *dst = Float(-rhs);
        }
        _ => {
            vm_exit!(ctx, ip, ArithTypeError);
        }
    }

    dispatch_next(jt, sp, lp, ip, ctx)
}

#[inline(always)]
unsafe fn not(args: Not, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    let dst = sp.at(args.dst);
    let rhs = *sp.at(args.rhs);

    *dst = ValueRaw::Bool(!rhs.coerce_bool());

    dispatch_next(jt, sp, lp, ip, ctx)
}

#[inline(always)]
unsafe fn call(args: Call, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    // need closures before this can be implemented
    todo!()
}

#[inline(always)]
unsafe fn fastcall(args: Fastcall, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    let ret = args.dst;
    let callee = ctx.get_function_in_current_module(args.id);

    let (sp, lp, ip) = do_call(callee, ret, ip, ctx);

    dispatch_current(jt, sp, lp, ip, ctx)
}

#[inline(always)]
unsafe fn ret(args: Ret, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    let (sp, lp, ip) = return_from_call(ctx);

    dispatch_current(jt, sp, lp, ip, ctx)
}

#[inline(always)]
unsafe fn stop(args: Stop, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    Control::stop()
}

/// call procedure:
/// 1. grow stack if needed
/// 2. allocate new call frame
/// 3. jump to start of callee
///
/// new call frame's stack overlaps with the current frame's stack
/// example: assuming 3 args, with ret at r6:
/// ```text,ignore
///   frame N:   [ 0 1 2 3 4 5 6 7 8 9 ]
///                            ^ ^
///                            | args
///                            ret
///   frame N+1: [ 6 7 8 9 ... ]
///                ^ ^
///                | args
///                ret
/// ```
/// `r0` in the new frame will be in the same location as `r6`
/// in the previous frame.
#[inline(always)]
unsafe fn do_call(callee: FuncInfoPtr, ret: Reg, ip: Ip, ctx: Ctx) -> (Sp, Lp, Ip) {
    // See doc comment.
    let stack_base = ctx.current_frame().stack_base() + (ret.get() as u32);

    // Return addr points to the next instruction after the call instruction.
    let current_function_start = ctx.current_frame().callee().code();
    let return_addr = 1 + (ip.offset_from_unsigned(current_function_start) as u32);

    let new_frame = CallFrame {
        callee,
        stack_base,
        return_addr,
    };

    let sp: Sp = maybe_grow_stack(ctx, &new_frame);
    let lp: Lp = new_frame.callee.literals();
    let ip: Ip = new_frame.callee.code();

    let prev_frame = core::ptr::replace(ctx.current_frame().raw(), new_frame);
    ctx.push_frame(prev_frame);

    (sp, lp, ip)
}

#[inline(always)]
unsafe fn maybe_grow_stack(ctx: Ctx, new_frame: &CallFrame) -> Sp {
    let new_stack_base = new_frame.stack_base as usize;
    let new_frame_size = new_frame.callee.nstack() as usize;
    if !ctx.has_enough_stack_space(new_stack_base, new_frame_size) {
        grow_stack(ctx, new_frame)
    }

    ctx.stack_at(new_stack_base)
}

#[cold]
unsafe fn grow_stack(ctx: Ctx, new_frame: &CallFrame) {
    // NOTE: We allocate more than we need here, capacity doubles each time anyway.
    let stack = &mut (*(*ctx.0).vm).stack;
    stack.grow(new_frame.callee.nstack() as usize)
}

#[inline(always)]
unsafe fn return_from_call(ctx: Ctx) -> (Sp, Lp, Ip) {
    // Only called from `ret`, meaning we are guaranteed to have
    // at least the one call frame which is currently being executed.

    let returning_from = core::ptr::replace(ctx.current_frame().raw(), ctx.pop_frame_unchecked());
    let returning_to = ctx.current_frame();

    let stack_base = returning_to.stack_base() as usize;
    let return_addr = returning_from.return_addr as usize;

    let sp: Sp = ctx.stack_at(stack_base);
    let lp: Lp = returning_to.callee().literals();
    let ip: Ip = returning_to.callee().code().offset(return_addr as isize);

    (sp, lp, ip)
}

type Invariant<'a> = PhantomData<fn(&'a ()) -> &'a ()>;

pub struct Vm {
    heap: gc::Heap,

    stack: DynArray<ValueRaw>,

    /// Invariant: Should never be empty.
    ///
    /// The current frame is stored inline in `Context`
    /// to reduce indirection, as it is frequently accessed,
    /// and the interpreter always holds a pointer to `Context`
    /// directly in a register.
    frames: DynStack<CallFrame>,
}

impl Vm {
    pub fn new() -> Self {
        // 1 MiB
        const INITIAL_STACK_SIZE: usize = (1024 * 1024) / std::mem::size_of::<ValueRaw>();
        const STACK_DEPTH: usize = INITIAL_STACK_SIZE / 16;

        Vm {
            heap: gc::Heap::new(),
            stack: DynArray::new(INITIAL_STACK_SIZE),
            frames: DynStack::new(STACK_DEPTH),
        }
    }

    #[inline(always)]
    pub fn with<F>(&mut self, f: F)
    where
        F: for<'gc> FnOnce(Runtime<'gc>),
    {
        f(Runtime {
            vm: self,
            _lifetime: PhantomData,
        })
    }
}

struct VmRoots {
    vm: *mut Vm,
}

impl gc::ExternalRoots for VmRoots {
    unsafe fn trace(&self, tracer: &gc::Tracer) {
        // iterate over VM stack
        let frames = &(*self.vm).frames;
        eprintln!("{}", frames.len());
        let sp = (*self.vm).stack.offset(0);
        for frame in (*self.vm).frames.iter() {
            let base = frame.stack_base as usize;
            let nstack = frame.callee.nstack() as usize;
            for i in base..base + nstack {
                tracer.visit_value(sp.add(i).read());
            }
        }
    }
}

pub struct Runtime<'gc> {
    vm: *mut Vm,

    _lifetime: Invariant<'gc>,
}

// TODO: none of the public APIs should return `ValueRaw`.
// Currently they are kept alive by the fact that we only
// run gc during allocating instructions (`larr` and friends).
impl<'gc> Runtime<'gc> {
    /// Execute the main entrypoint of the module to completion.
    pub fn run(&mut self, m: &Module) -> Result<ValueRaw> {
        self.run_inner(m)?;

        // SAFETY: Functions always store their return values in slot 0,
        // and are guaranteed to always allocate at least that slot.
        //
        // TODO: The resulting value is alive only because of implementation
        // details - we need to guarantee this _somehow_. Currently no GC happens
        // between the last `ret` and this point, and all stack values used
        // by the main entrypoint are guaranteed to not have been collected yet
        // until after its `ret`, at which point the stack frame is considered
        // dead.
        let vm = unsafe { &mut *self.vm };
        let value = unsafe { *vm.stack.offset(0) };
        Ok(value)
    }

    fn run_inner(&mut self, m: &Module) -> Result<()> {
        unsafe {
            let mut error = None;

            // `ret`, assumes that it always returns _into_ something.
            // That allows `ret` to `pop_frame_unchecked`, removing a
            // branch from a very hot code path.
            // For the `main` entrypoint of a module, this is what it
            // will `ret` into. All it does is call the module's `main`,
            // and then halt the interpreter.
            let mut entry = [
                // call the entrypoint
                asm::fastcall(Reg::new_unchecked(0), m.main()),
                // halt the VM
                asm::stop(),
            ];

            let entry = core::mem::ManuallyDrop::new(FuncInfo {
                name: "@entry".into(),
                nparams: 0,
                nstack: 1,
                // SAFETY: never dropped
                code: Box::from_raw(&mut entry),
                literals: Box::from_raw(&mut []),
                dbg: None,
                module: m.inner(),
            });

            let mut ctx = Context {
                vm: self.vm,
                current_module: m.inner(),
                error: &raw mut error,
                saved_ip: Ip(null()),
                current_frame: CallFrame {
                    callee: FuncInfoPtr(&raw const *entry),
                    stack_base: 0,
                    return_addr: 0,
                },
            };
            let ctx: Ctx = Ctx(&mut ctx);

            let jt: Jt = JT.as_ptr();
            let sp: Sp = ctx.stack_at(0);
            let lp: Lp = ctx.current_frame().callee().literals();
            let ip: Ip = ctx.current_frame().callee().code();

            // In debug mode, fall back to loop+match.
            //
            // Each instruction will return `Control::Continue`
            // instead of tail-calling the next handler.
            #[cfg(debug_assertions)]
            {
                let mut sp: Sp = sp;
                let mut lp: Lp = lp;
                let mut ip: Ip = ip;

                loop {
                    let inst = ip.get();
                    let op = jt.at(inst);
                    match op(inst, jt, sp, lp, ip, ctx) {
                        Control::Stop => return Ok(()),
                        Control::Error(err) if err.is_external() => return Err(error.unwrap()),
                        Control::Error(err) => return Err(err.with_context(ctx)),

                        #[cfg(debug_assertions)]
                        Control::Continue(new_sp, new_lp, new_ip) => {
                            sp = new_sp;
                            lp = new_lp;
                            ip = new_ip;
                            continue;
                        }
                    }
                }
            }

            #[cfg(not(debug_assertions))]
            {
                let ctrl = dispatch_current(jt, sp, lp, ip, ctx);

                if ctrl.is_error() {
                    let err = ctrl.error_code();
                    if err.is_external() {
                        return Err(error.unwrap());
                    } else {
                        return Err(err.with_context(ctx));
                    }
                }

                return Ok(());
            }
        }
    }

    pub fn fmt(&self, value: ValueRaw) -> impl std::fmt::Display + '_ {
        struct ValueFormatter<'a, 'gc> {
            this: &'a Runtime<'gc>,
            value: ValueRaw,
        }

        impl<'gc> std::fmt::Display for ValueFormatter<'_, 'gc> {
            fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
                match self.value {
                    ValueRaw::Nil => write!(f, "nil"),
                    ValueRaw::Bool(v) => write!(f, "{v}"),
                    ValueRaw::Int(v) => write!(f, "{v}"),
                    ValueRaw::Float(v) => write!(f, "{v}"),
                    ValueRaw::String(v) => {
                        let str = unsafe { v.as_ref() };
                        write!(f, "{}", str.as_str())
                    }
                    ValueRaw::List(v) => {
                        let list = unsafe { v.as_ref() };

                        write!(f, "[")?;
                        let mut comma = false;
                        for item in list {
                            if comma {
                                write!(f, ", ")?;
                            }
                            write!(
                                f,
                                "{}",
                                ValueFormatter {
                                    this: self.this,
                                    value: item.raw(),
                                }
                            )?;
                            comma = true;
                        }
                        write!(f, "]")?;

                        Ok(())
                    }
                    ValueRaw::Table(v) => {
                        let table = unsafe { v.as_ref() };

                        write!(f, "{{")?;
                        let mut comma = false;
                        for (key, value) in table {
                            if comma {
                                write!(f, ", ")?;
                            }
                            write!(
                                f,
                                "{}={}",
                                key.as_str(),
                                ValueFormatter {
                                    this: self.this,
                                    value: value.raw(),
                                }
                            )?;
                            comma = true;
                        }
                        write!(f, "}}")?;

                        Ok(())
                    }
                    ValueRaw::UserData(v) => {
                        todo!()
                    }
                }
            }
        }

        ValueFormatter { this: self, value }
    }
}

#[cfg(test)]
mod tests;
