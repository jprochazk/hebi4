//! ## Safety in the VM
//!
//! The VM uses a _LOT_ of unsafe code. Too much, in fact. Some of it can likely be
//! removed without introducing performance overhead, but performance is the primary goal
//! of doing this in the first place. The usual approach of "take safe code, and sprinkle
//! in unsafe carefully to improve performance" never worked out for me. My understanding
//! for why is that it was always a death by a thousand cuts: Bounds checks, drop glue,
//! overly large types, extra allocations, runtime assertions, panic machinery, etc.
//!
//! Instead, the VM here attempts to start from a relatively clean slate. We try as much
//! as possible to reduce the chances for error by using type-safe wrappers and limiting
//! the usage of Rust references, which would require us to uphold their strong guarantees
//! everywhere.
//!
//! We're making a wild assumption: The code generated by the Hebi compiler is valid.
//! I believe that the compiler is simple enough that it's possible to comprehensively check
//! all of it. Extensive codegen and runtime tests (including fuzzing) are used to help
//! validate those assumptions.
//!
//! Assuming the compiler is indeed correct, it upholds certain invariants which we can use
//! to reduce runtime overhead in the VM.
//!
//! All accesses to the VM stack are guaranteed to be safe:
//! - The stack always has enough space for the maximum possible number of registers that
//!   a given function needs.
//! - A value on the stack is guaranteed to be initialized before it is first read.
//!
//! A concrete example of this is the `Reg` (register) newtype used to offset `Sp` (stack pointer),
//! the operation will _always_ yield a properly-aligned pointer into a memory located within bounds
//! of an allocation, removing the need for a bounds check, or dynamically growing the stack
//! on demand when writing to it.
//!
//! Not just that, but because a value in a register is never read before it's initialized,
//! we also don't need to pre-initialize the contents of the stack with default values.
//! It is allocated using `alloc_zeroed` (though technically `0` is a valid bit pattern for `Value`),
//! and then pointers to this allocation are handed out as needed.
//!
//! Taking advantage of all of this, Hebi's `mov` instruction can compile down to just:
//! - A decode of the instruction operands (two zero-extended movs from a register)
//! - The actual `mov` of the value between two memory locations
//! - A dispatch of the next instruction (a few `mov`s and a `jmp rax`)
//!
//! That's as low-overhead as it gets, which is the ultimate goal here.

mod array;

#[macro_use]
pub mod gc;

pub mod value;

use std::{marker::PhantomData, ptr::NonNull};

use array::{DynArray, DynStack};
use gc::{GcPtr, Heap};
use value::{List, ModuleProto, String, Table};

use crate::{
    codegen::opcodes::*,
    core::RuntimeCoreLib,
    error::{Error, Result, error},
    module,
    span::Span,
    value::{
        Closure,
        closure::{CaptureInfo, ClosureProto},
        host_function::{Context, HostFunction, HostFunctionCallback},
    },
    vm::value::{FunctionProto, ValueRaw, module::ModuleRegistry},
};

impl Sp {
    #[inline(always)]
    pub unsafe fn at(self, r: Reg) -> *mut ValueRaw {
        self.0.offset(r.sz()).as_ptr()
    }

    #[inline(always)]
    pub unsafe fn ret(self) -> *mut ValueRaw {
        self.0.as_ptr()
    }
}

trait LpIdx {
    fn idx(self) -> isize;
}

impl LpIdx for Lit {
    #[inline(always)]
    fn idx(self) -> isize {
        self.sz()
    }
}

impl LpIdx for Lit8 {
    #[inline(always)]
    fn idx(self) -> isize {
        self.sz()
    }
}

impl Lp {
    #[inline]
    fn from_fn(f: GcPtr<FunctionProto>) -> Self {
        unsafe {
            let ptr = f.as_ref().literals().as_ptr().cast_mut();
            Self(NonNull::new_unchecked(ptr))
        }
    }

    #[inline(always)]
    unsafe fn _at(self, r: isize) -> *const ValueRaw {
        self.0.offset(r).as_ptr()
    }

    // note: these use pointer reads to avoid matching with an `unreachable` branch,
    // which for some reason still generat branches
    // TODO: could be made safer with some kind of enum `offset_of`

    #[inline(always)]
    pub unsafe fn int_or_float_unchecked(self, r: impl LpIdx) -> ValueRaw {
        let v = self._at(r.idx());

        debug_assert!(matches!(&*v, ValueRaw::Int(..) | ValueRaw::Float(..)));
        v.cast::<ValueRaw>().read()
    }

    #[inline(always)]
    pub unsafe fn int_unchecked(self, r: impl LpIdx) -> i64 {
        let v = self._at(r.idx());

        debug_assert!(matches!(&*v, ValueRaw::Int(..)));
        v.cast::<u64>().add(1).cast::<i64>().read()
    }

    #[inline(always)]
    pub unsafe fn float_unchecked(self, r: impl LpIdx) -> f64 {
        let v = self._at(r.idx());

        debug_assert!(matches!(&*v, ValueRaw::Float(..)));
        v.cast::<u64>().add(1).cast::<f64>().read()
    }

    #[inline(always)]
    pub unsafe fn str_unchecked(self, r: impl LpIdx) -> GcPtr<String> {
        let v = self._at(r.idx());

        debug_assert!(matches!(&*v, ValueRaw::Object(gc) if gc.is::<String>()));
        v.cast::<u64>().add(1).cast::<GcPtr<String>>().read()
    }

    #[inline(always)]
    pub unsafe fn closure_unchecked(self, r: impl LpIdx) -> GcPtr<ClosureProto> {
        let v = self._at(r.idx());

        debug_assert!(matches!(&*v, ValueRaw::Object(gc) if gc.is::<ClosureProto>()));
        v.cast::<u64>().add(1).cast::<GcPtr<ClosureProto>>().read()
    }
}

impl Jt {
    #[inline(always)]
    pub unsafe fn at(self, insn: Insn) -> OpaqueHandler {
        self.0.offset(insn.tag()).read()
    }
}

impl Ip {
    #[inline]
    fn from_fn(f: GcPtr<FunctionProto>) -> Self {
        unsafe {
            let ptr = f.as_mut().code_mut().as_mut_ptr();
            Self(NonNull::new_unchecked(ptr))
        }
    }

    #[inline]
    unsafe fn offset_from_unsigned(self, other: Self) -> usize {
        (self.0).offset_from_unsigned(other.0)
    }

    #[inline]
    unsafe fn offset(self, n: isize) -> Self {
        Ip((self.0).offset(n as isize))
    }

    #[inline(always)]
    unsafe fn next(self) -> Self {
        Self(self.0.offset(1))
    }

    #[inline(always)]
    unsafe fn get(self) -> Insn {
        self.0.read()
    }
}

#[derive(Clone, Copy)]
#[repr(transparent)]
struct Captures(NonNull<ValueRaw>);

impl Captures {
    #[inline(always)]
    pub unsafe fn at(self, idx: Cap) -> *mut ValueRaw {
        self.0.offset(idx.sz()).as_ptr()
    }
}

// TODO: native call frames
#[derive(Clone, Copy)]
struct CallFrame {
    callee: GcPtr<FunctionProto>,

    /// Stack base of _this_ frame.
    stack_base: u32,

    /// Address of the next instruction to execute
    /// after returning from _this_ call frame.
    ///
    /// Points into the previous call frame's callee,
    /// if there is one.
    return_addr: u32,
}

const _: () =
    assert!(core::mem::size_of::<Option<CallFrame>>() == core::mem::size_of::<CallFrame>());

#[derive(Clone, Copy)]
struct CallFramePtr(*mut CallFrame);

impl CallFramePtr {
    #[inline]
    unsafe fn raw(self) -> *mut CallFrame {
        self.0
    }

    #[inline]
    unsafe fn callee(self) -> GcPtr<FunctionProto> {
        (*self.0).callee
    }

    #[inline]
    unsafe fn stack_base(self) -> u32 {
        (*self.0).stack_base
    }

    #[inline]
    unsafe fn return_addr(self) -> u32 {
        (*self.0).return_addr
    }
}

impl Vm {
    #[inline]
    unsafe fn current_frame(self) -> CallFramePtr {
        let ptr = (*self.0.as_ptr()).frames.top_unchecked();
        CallFramePtr(ptr)
    }

    #[inline]
    unsafe fn push_frame(self, frame: CallFrame) {
        let frames = &mut (*self.0.as_ptr()).frames;
        frames.push(frame);
    }

    #[inline]
    unsafe fn pop_frame_unchecked(self) -> CallFrame {
        let frames = &mut (*self.0.as_ptr()).frames;
        frames.pop_unchecked()
    }

    #[inline]
    unsafe fn has_enough_stack_space(self, stack_base: usize, frame_size: usize) -> bool {
        let stack = &mut (*self.0.as_ptr()).stack;
        let remaining = stack.remaining(stack_base);
        remaining >= (frame_size as isize)
    }

    #[inline]
    unsafe fn stack_at(self, stack_base: usize) -> Sp {
        let stack = &mut (*self.0.as_ptr()).stack;
        Sp(NonNull::new_unchecked(stack.offset(stack_base)))
    }

    #[inline]
    unsafe fn grow_stack(self, additional: usize) {
        let stack = &mut (*self.0.as_ptr()).stack;
        stack.grow(additional)
    }

    #[inline]
    unsafe fn get_function_in_current_module(self, id: FnId) -> GcPtr<FunctionProto> {
        self.current_module()
            .as_ref()
            .get_function_unchecked(id)
            .as_ptr()
    }

    #[inline]
    unsafe fn var_in_current_module(self, idx: Mvar) -> *mut ValueRaw {
        self.current_module()
            .as_mut()
            .module_vars
            .as_mut_ptr()
            .offset(idx.sz())
    }

    #[inline]
    unsafe fn current_module(self) -> GcPtr<ModuleProto> {
        let module = (*self.0.as_ptr()).current_module;
        debug_assert!(module.is_some());
        module.unwrap_unchecked()
    }

    #[inline]
    unsafe fn set_current_module_for(self, f: GcPtr<FunctionProto>) {
        let module = f.as_ref().module().as_ptr();
        (*self.0.as_ptr()).current_module = Some(module);
    }

    #[inline]
    unsafe fn get_host_function(self, id: HostId) -> GcPtr<HostFunction> {
        (*self.0.as_ptr())
            .core
            .functions
            .as_mut_ptr()
            .offset(id.sz())
            .read()
    }

    #[inline]
    unsafe fn current_captures(self) -> Captures {
        let captures = (*self.0.as_ptr()).current_captures;
        debug_assert!(captures.is_some());
        captures.unwrap_unchecked()
    }

    #[inline]
    unsafe fn set_current_captures_for(self, f: GcPtr<Closure>) {
        let captures = NonNull::new_unchecked(f.as_mut().captures.as_mut_ptr());
        (*self.0.as_ptr()).current_captures = Some(Captures(captures));
    }

    #[inline]
    unsafe fn write_error(self, error: Error) {
        (*self.0.as_ptr()).error = Some(error);
    }

    #[inline]
    unsafe fn take_error(mut self) -> Option<Error> {
        self.0.as_mut().error.take()
    }

    #[inline]
    unsafe fn get_span(self, ip: Ip) -> Option<Span> {
        let pc = ip.offset_from_unsigned(Ip::from_fn(self.current_frame().callee()));
        match self.current_frame().callee().as_ref().dbg() {
            Some(dbg) => Some(dbg.spans[pc]),
            None => None,
        }
    }

    #[inline]
    unsafe fn set_saved_ip(self, ip: Ip) {
        let ptr = &raw mut (*self.0.as_ptr()).saved_ip;
        ptr.write(Some(ip));
    }

    #[inline]
    unsafe fn saved_ip(self) -> Option<Ip> {
        (*self.0.as_ptr()).saved_ip
    }

    #[inline]
    unsafe fn get_span_for_saved_ip(self) -> Option<Span> {
        let ip = self.saved_ip()?;
        self.get_span(ip)
    }

    #[inline]
    unsafe fn heap(self) -> *mut Heap {
        &raw mut (*self.0.as_ptr()).heap
    }

    #[inline]
    unsafe fn stdio(self) -> *mut Stdio {
        &raw mut (*self.0.as_ptr()).stdio
    }

    #[inline]
    unsafe fn maybe_gc(self) {
        // TODO: GC thresholds
        // for now, we always GC

        self.full_gc();
    }

    #[cold]
    unsafe fn full_gc(self) {
        let heap = &raw mut (*self.0.as_ptr()).heap;
        let roots = VmRoots { state: self.0 };
        Heap::collect_with_external_roots(heap, roots);
    }
}

#[derive(Clone, Copy)]
#[repr(u8)]
pub enum VmError {
    DivisionByZero,
    ArithTypeError,
    UnopInvalidType,
    CmpTypeError,
    NotAList,
    NotAString,
    InvalidArrayIndex,
    IndexOutOfBounds,
    NotATable,
    InvalidTableKey,
    MissingKey,
    NotIndexable,
    ArityMismatch,
    NotCallable,

    Host,
}

impl VmError {
    // TODO: additional error context, somehow?
    //       for example, try reading the current `ip`, reading the operands,
    //       and checking values and other things. assuming that failures
    //       happen before any writes.
    //       or maybe there's a smarter way to transfer context, without
    //       too much overhead.
    // NOTE: assumes `vm.saved_ip` is set
    unsafe fn annotate(&self, vm: Vm) -> Error {
        let span = vm.get_span_for_saved_ip().unwrap_or_default();
        match self {
            Self::DivisionByZero => error("division by zero", span),
            Self::ArithTypeError => error("type mismatch", span),
            Self::UnopInvalidType => error("invalid type", span),
            Self::CmpTypeError => error("type mismatch", span),
            Self::NotAList => error("not a list", span),
            Self::NotAString => error("not a string", span),
            Self::InvalidArrayIndex => error("value is not a valid array index", span),
            Self::IndexOutOfBounds => error("index out of bounds", span),
            Self::NotATable => error("not a table", span),
            Self::InvalidTableKey => error("value is not a valid table key", span),
            Self::MissingKey => error("missing key", span),
            Self::NotIndexable => error("this value cannot be indexed", span),
            Self::ArityMismatch => error("invalid number of arguments", span),
            Self::NotCallable => error("this value cannot be called", span),

            Self::Host => error("<host error>", span),
        }
    }

    #[inline]
    fn is_host(self) -> bool {
        match self {
            Self::Host => true,

            Self::DivisionByZero
            | Self::ArithTypeError
            | Self::UnopInvalidType
            | Self::CmpTypeError
            | Self::NotAList
            | Self::NotAString
            | Self::InvalidArrayIndex
            | Self::IndexOutOfBounds
            | Self::NotATable
            | Self::InvalidTableKey
            | Self::MissingKey
            | Self::NotIndexable
            | Self::ArityMismatch
            | Self::NotCallable => false,
        }
    }
}

macro_rules! vm_exit {
    ($vm:ident, $ip:ident, $error:ident) => {{
        $vm.set_saved_ip($ip);
        return Control::error(VmError::$error);
    }};
}

static JT: JumpTable = jump_table! {
    nop,
    mov,

    lmvar,
    smvar,
    lcap,
    scap,
    lidx,
    lidxn,
    sidx,
    sidxn,
    lkey,
    lkeyc,
    skey,
    skeyc,

    lnil,
    lsmi,
    ltrue,
    lfalse,
    lint,
    lnum,
    lstr,
    lclosure,
    lfunc,
    lhost,
    llist,
    ltable,
    jmp,
    islt,
    isle,
    isgt,
    isge,
    iseq,
    isne,
    isnil,
    isnotnil,
    istrue,
    isfalse,
    iseqs,
    isnes,
    iseqi,
    isnei,
    iseqf,
    isnef,
    isltv,
    islev,
    isgtv,
    isgev,
    iseqv,
    isnev,
    addvv,
    addvn,
    addnv,
    subvv,
    subvn,
    subnv,
    mulvv,
    mulvn,
    mulnv,
    divvv,
    divvn,
    divnv,
    unm,
    not,
    call,
    fastcall,
    hostcall,
    ret,
    retv,
    stop,
};

#[cfg(not(any(debug_assertions, target_arch = "wasm32")))]
#[derive(Clone, Copy)]
#[repr(transparent)]
pub(crate) struct Control(u8);

#[cfg(not(any(debug_assertions, target_arch = "wasm32")))]
impl Control {
    #[inline]
    fn stop() -> Control {
        Control(0)
    }

    #[inline]
    fn error(code: VmError) -> Control {
        Control(1 & ((code as u8) << 1))
    }

    #[inline]
    fn is_stop(self) -> bool {
        self.0 == 0
    }

    #[inline]
    fn is_error(self) -> bool {
        self.0 & 1 == 1
    }

    #[inline]
    unsafe fn error_code(self) -> VmError {
        core::mem::transmute(self.0 >> 1)
    }
}

#[cfg(any(debug_assertions, target_arch = "wasm32"))]
#[derive(Clone, Copy)]
#[repr(C)]
pub(crate) enum Control {
    Stop,
    Error(VmError),
    Continue(Sp, Lp, Ip),
}

#[cfg(any(debug_assertions, target_arch = "wasm32"))]
impl Control {
    #[inline]
    fn stop() -> Control {
        Control::Stop
    }

    #[inline]
    fn error(code: VmError) -> Control {
        Control::Error(code)
    }
}

/// Dispatch instruction at `ip`
#[inline(always)]
unsafe fn dispatch_current(vm: Vm, jt: Jt, ip: Ip, sp: Sp, lp: Lp) -> Control {
    #[cfg(any(debug_assertions, target_arch = "wasm32"))]
    {
        Control::Continue(sp, lp, ip)
    }

    #[cfg(not(any(debug_assertions, target_arch = "wasm32")))]
    {
        let insn = ip.get();
        let op = jt.at(insn);
        op(vm, jt, ip, insn, sp, lp)
    }
}

/// Dispatch instruction at `ip+1`
#[inline(always)]
unsafe fn dispatch_next(vm: Vm, jt: Jt, ip: Ip, sp: Sp, lp: Lp) -> Control {
    let ip = ip.next();
    dispatch_current(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn nop(vm: Vm, jt: Jt, ip: Ip, args: Nop, sp: Sp, lp: Lp) -> Control {
    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn mov(vm: Vm, jt: Jt, ip: Ip, args: Mov, sp: Sp, lp: Lp) -> Control {
    *sp.at(args.dst()) = *sp.at(args.src());

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn lmvar(vm: Vm, jt: Jt, ip: Ip, args: Lmvar, sp: Sp, lp: Lp) -> Control {
    *sp.at(args.dst()) = *vm.var_in_current_module(args.src());

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn smvar(vm: Vm, jt: Jt, ip: Ip, args: Smvar, sp: Sp, lp: Lp) -> Control {
    *vm.var_in_current_module(args.dst()) = *sp.at(args.src());

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn lcap(vm: Vm, jt: Jt, ip: Ip, args: Lcap, sp: Sp, lp: Lp) -> Control {
    *sp.at(args.dst()) = *vm.current_captures().at(args.src());

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn scap(vm: Vm, jt: Jt, ip: Ip, args: Scap, sp: Sp, lp: Lp) -> Control {
    *vm.current_captures().at(args.dst()) = *sp.at(args.src());

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn lidx(vm: Vm, jt: Jt, ip: Ip, args: Lidx, sp: Sp, lp: Lp) -> Control {
    let dst = sp.at(args.dst());
    let target = *sp.at(args.target());
    let idx = *sp.at(args.idx());

    if let Some(list) = target.into_object::<List>() {
        let ValueRaw::Int(idx) = idx else {
            return invalid_array_index_error(ip, vm);
        };
        let idx = idx as usize;

        // UNROOTED: reachable through the stack
        let Some(value) = list.as_ref().get(idx) else {
            return index_out_of_bounds_error(ip, vm);
        };

        *dst = value.raw();
    } else if let Some(table) = target.into_object::<Table>() {
        let Some(key) = idx.into_object::<String>() else {
            return invalid_table_key_error(ip, vm);
        };

        // UNROOTED: reachable through the stack
        // TODO: inline caching
        let Some(value) = table.as_ref().get(key.as_ref()) else {
            return missing_key_error(ip, vm);
        };

        *dst = value.raw();
    } else {
        return not_indexable_error(ip, vm);
    };

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn lidxn(vm: Vm, jt: Jt, ip: Ip, args: Lidxn, sp: Sp, lp: Lp) -> Control {
    let dst = sp.at(args.dst());
    let list = *sp.at(args.target());
    let idx = lp.int_unchecked(args.idx()) as usize;

    let Some(list) = list.into_object::<List>() else {
        return not_a_list_error(ip, vm);
    };

    // UNROOTED: reachable through the stack
    let Some(value) = list.as_ref().get(idx) else {
        return index_out_of_bounds_error(ip, vm);
    };

    *dst = value.raw();

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn sidx(vm: Vm, jt: Jt, ip: Ip, args: Sidx, sp: Sp, lp: Lp) -> Control {
    let target = *sp.at(args.target());
    let idx = *sp.at(args.idx());
    let src = *sp.at(args.src());

    if let Some(list) = target.into_object::<List>() {
        let ValueRaw::Int(idx) = idx else {
            return invalid_array_index_error(ip, vm);
        };
        let idx = idx as usize;

        // UNROOTED: reachable through the stack
        let len = list.as_ref().len();
        if idx >= len {
            return index_out_of_bounds_error(ip, vm);
        }

        list.as_mut().set_raw_unchecked(idx, src);
    } else if let Some(table) = target.into_object::<Table>() {
        let Some(key) = idx.into_object::<String>() else {
            return invalid_table_key_error(ip, vm);
        };

        // UNROOTED: reachable through the stack
        // TODO: inline caching
        table.as_mut().insert_raw(key, src);
    } else {
        return not_indexable_error(ip, vm);
    };

    dispatch_next(vm, jt, ip, sp, lp)
}

#[cold]
unsafe fn not_indexable_error(ip: Ip, vm: Vm) -> Control {
    vm_exit!(vm, ip, NotIndexable);
}

#[cold]
unsafe fn invalid_array_index_error(ip: Ip, vm: Vm) -> Control {
    vm_exit!(vm, ip, InvalidArrayIndex);
}

#[inline(always)]
unsafe fn sidxn(vm: Vm, jt: Jt, ip: Ip, args: Sidxn, sp: Sp, lp: Lp) -> Control {
    let list = *sp.at(args.target());
    let idx = lp.int_unchecked(args.idx()) as usize;
    let src = *sp.at(args.src());

    let Some(list) = list.into_object::<List>() else {
        return not_a_list_error(ip, vm);
    };

    // UNROOTED: reachable through the stack
    let len = list.as_ref().len();
    if idx >= len {
        return index_out_of_bounds_error(ip, vm);
    }

    list.as_mut().set_raw_unchecked(idx, src);

    dispatch_next(vm, jt, ip, sp, lp)
}

#[cold]
unsafe fn not_a_list_error(ip: Ip, vm: Vm) -> Control {
    vm_exit!(vm, ip, NotAList);
}

#[cold]
unsafe fn index_out_of_bounds_error(ip: Ip, vm: Vm) -> Control {
    vm_exit!(vm, ip, IndexOutOfBounds);
}

#[inline(always)]
unsafe fn lkey(vm: Vm, jt: Jt, ip: Ip, args: Lkey, sp: Sp, lp: Lp) -> Control {
    let dst = sp.at(args.dst());
    let target = *sp.at(args.target());
    let key = *sp.at(args.key());

    let Some(table) = target.into_object::<Table>() else {
        return not_a_table_error(ip, vm);
    };

    let Some(key) = key.into_object::<String>() else {
        return invalid_table_key_error(ip, vm);
    };

    // UNROOTED: reachable through the stack
    // TODO: inline caching
    let Some(value) = table.as_ref().get(key.as_ref()) else {
        return missing_key_error(ip, vm);
    };

    *dst = value.raw();

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn lkeyc(vm: Vm, jt: Jt, ip: Ip, args: Lkeyc, sp: Sp, lp: Lp) -> Control {
    let dst = sp.at(args.dst());
    let target = *sp.at(args.target());
    let key = lp.str_unchecked(args.key());

    let Some(table) = target.into_object::<Table>() else {
        return not_a_table_error(ip, vm);
    };

    // UNROOTED: reachable through the stack
    // TODO: inline caching
    let Some(value) = table.as_ref().get(key.as_ref()) else {
        return missing_key_error(ip, vm);
    };

    *dst = value.raw();

    dispatch_next(vm, jt, ip, sp, lp)
}

#[cold]
unsafe fn missing_key_error(ip: Ip, vm: Vm) -> Control {
    vm_exit!(vm, ip, MissingKey);
}

#[inline(always)]
unsafe fn skey(vm: Vm, jt: Jt, ip: Ip, args: Skey, sp: Sp, lp: Lp) -> Control {
    let target = *sp.at(args.target());
    let key = *sp.at(args.key());
    let src = *sp.at(args.src());

    let Some(table) = target.into_object::<Table>() else {
        return not_a_table_error(ip, vm);
    };

    let Some(key) = key.into_object::<String>() else {
        return invalid_table_key_error(ip, vm);
    };

    // UNROOTED: reachable through the stack
    // TODO: inline caching
    table.as_mut().insert_raw(key, src);

    dispatch_next(vm, jt, ip, sp, lp)
}

#[cold]
unsafe fn not_a_table_error(ip: Ip, vm: Vm) -> Control {
    vm_exit!(vm, ip, NotATable);
}

#[cold]
unsafe fn invalid_table_key_error(ip: Ip, vm: Vm) -> Control {
    vm_exit!(vm, ip, InvalidTableKey);
}

#[inline(always)]
unsafe fn skeyc(vm: Vm, jt: Jt, ip: Ip, args: Skeyc, sp: Sp, lp: Lp) -> Control {
    let target = *sp.at(args.target());
    let key = lp.str_unchecked(args.key());
    let src = *sp.at(args.src());

    let Some(table) = target.into_object::<Table>() else {
        return not_a_table_error(ip, vm);
    };

    // UNROOTED: reachable through the stack
    // TODO: inline caching
    table.as_mut().insert_raw(key, src);

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn lnil(vm: Vm, jt: Jt, ip: Ip, args: Lnil, sp: Sp, lp: Lp) -> Control {
    *sp.at(args.dst()) = ValueRaw::Nil;

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn lsmi(vm: Vm, jt: Jt, ip: Ip, args: Lsmi, sp: Sp, lp: Lp) -> Control {
    *sp.at(args.dst()) = ValueRaw::Int(args.v().get() as i64);

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn lint(vm: Vm, jt: Jt, ip: Ip, args: Lint, sp: Sp, lp: Lp) -> Control {
    *sp.at(args.dst()) = ValueRaw::Int(lp.int_unchecked(args.id()));

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn lnum(vm: Vm, jt: Jt, ip: Ip, args: Lnum, sp: Sp, lp: Lp) -> Control {
    *sp.at(args.dst()) = ValueRaw::Float(lp.float_unchecked(args.id()));

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn lstr(vm: Vm, jt: Jt, ip: Ip, args: Lstr, sp: Sp, lp: Lp) -> Control {
    *sp.at(args.dst()) = ValueRaw::Object(lp.str_unchecked(args.id()).as_any());

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn ltrue(vm: Vm, jt: Jt, ip: Ip, args: Ltrue, sp: Sp, lp: Lp) -> Control {
    *sp.at(args.dst()) = ValueRaw::Bool(true);

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn lfalse(vm: Vm, jt: Jt, ip: Ip, args: Lfalse, sp: Sp, lp: Lp) -> Control {
    *sp.at(args.dst()) = ValueRaw::Bool(false);

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn lclosure(vm: Vm, jt: Jt, ip: Ip, args: Lclosure, sp: Sp, lp: Lp) -> Control {
    let dst = sp.at(args.dst());
    let closure = lp.closure_unchecked(args.id());

    let captures = init_captures(closure, vm, sp);
    let heap = vm.heap();
    let closure = Closure::alloc(&*heap, closure, captures);

    *dst = ValueRaw::Object(closure.as_any());

    dispatch_next(vm, jt, ip, sp, lp)
}

unsafe fn init_captures(closure: GcPtr<ClosureProto>, vm: Vm, sp: Sp) -> Box<[ValueRaw]> {
    let closure = closure.as_ref();
    let ncaptures = closure.capture_info.len();
    let mut captures = Box::new_uninit_slice(ncaptures);
    for i in 0..ncaptures {
        let value = match *closure.capture_info.get_unchecked(i) {
            CaptureInfo::Reg(reg) => *sp.at(reg),
            CaptureInfo::Cap(cap) => *vm.current_captures().at(cap),
        };
        captures.get_unchecked_mut(i).write(value);
    }
    captures.assume_init()
}

#[inline(always)]
unsafe fn lfunc(vm: Vm, jt: Jt, ip: Ip, args: Lfunc, sp: Sp, lp: Lp) -> Control {
    let dst = sp.at(args.dst());
    let func = vm.get_function_in_current_module(args.id());

    *dst = ValueRaw::Object(func.as_any());

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn lhost(vm: Vm, jt: Jt, ip: Ip, args: Lhost, sp: Sp, lp: Lp) -> Control {
    let dst = sp.at(args.dst());
    let func = vm.get_host_function(args.id());

    *dst = ValueRaw::Object(func.as_any());

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn llist(vm: Vm, jt: Jt, ip: Ip, args: Llist, sp: Sp, lp: Lp) -> Control {
    vm.maybe_gc();

    let dst = sp.at(args.dst());
    let len = args.cap().zx();

    // UNROOTED: immediately written to the stack
    let heap = vm.heap();
    let list = List::alloc_zeroed(&*heap, len);

    *dst = ValueRaw::Object(list.as_any());

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn ltable(vm: Vm, jt: Jt, ip: Ip, args: Ltable, sp: Sp, lp: Lp) -> Control {
    vm.maybe_gc();

    let dst = sp.at(args.dst());
    let len = args.cap().zx();

    // UNROOTED: immediately written to the stack
    let heap = vm.heap();
    let table = Table::alloc(&*heap, len);

    *dst = ValueRaw::Object(table.as_any());

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn jmp(vm: Vm, jt: Jt, ip: Ip, args: Jmp, sp: Sp, lp: Lp) -> Control {
    let ip = ip.offset(args.rel().sz());

    dispatch_current(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn isnil(vm: Vm, jt: Jt, ip: Ip, args: Isnil, sp: Sp, lp: Lp) -> Control {
    // isnil v
    // jmp offset

    let v = *sp.at(args.v());
    if matches!(v, ValueRaw::Nil) {
        // skip `jmp`
        let ip = ip.offset(2);
        dispatch_current(vm, jt, ip, sp, lp)
    } else {
        // execute `jmp`
        let ip = ip.offset(1);
        dispatch_current(vm, jt, ip, sp, lp)
    }
}

#[inline(always)]
unsafe fn isnotnil(vm: Vm, jt: Jt, ip: Ip, args: Isnotnil, sp: Sp, lp: Lp) -> Control {
    // isnotnil v
    // jmp offset

    let v = *sp.at(args.v());
    if !matches!(v, ValueRaw::Nil) {
        // skip `jmp`
        let ip = ip.offset(2);
        dispatch_current(vm, jt, ip, sp, lp)
    } else {
        // execute `jmp`
        let ip = ip.offset(1);
        dispatch_current(vm, jt, ip, sp, lp)
    }
}

#[inline(always)]
unsafe fn istrue(vm: Vm, jt: Jt, ip: Ip, args: Istrue, sp: Sp, lp: Lp) -> Control {
    // istrue v
    // jmp offset

    let v = *sp.at(args.v());
    if v.coerce_bool() {
        // skip `jmp`
        let ip = ip.offset(2);
        dispatch_current(vm, jt, ip, sp, lp)
    } else {
        // execute `jmp`
        let ip = ip.offset(1);
        dispatch_current(vm, jt, ip, sp, lp)
    }
}

#[inline(always)]
unsafe fn isfalse(vm: Vm, jt: Jt, ip: Ip, args: Isfalse, sp: Sp, lp: Lp) -> Control {
    // isfalse v
    // jmp offset

    let v = *sp.at(args.v());
    if !v.coerce_bool() {
        // skip `jmp`
        let ip = ip.offset(2);
        dispatch_current(vm, jt, ip, sp, lp)
    } else {
        // execute `jmp`
        let ip = ip.offset(1);
        dispatch_current(vm, jt, ip, sp, lp)
    }
}

#[inline(always)]
unsafe fn iseqs(vm: Vm, jt: Jt, ip: Ip, args: Iseqs, sp: Sp, lp: Lp) -> Control {
    // iseqs v
    // jmp offset

    // TODO: string interning
    let lhs = *sp.at(args.lhs());
    let rhs = lp.str_unchecked(args.rhs());

    let Some(lhs) = lhs.into_object::<String>() else {
        // execute `jmp`
        let ip = ip.offset(1);
        return dispatch_current(vm, jt, ip, sp, lp);
    };

    let lhs = lhs.as_ref();
    let lhs = lhs.as_str();

    let rhs = rhs.as_ref();
    let rhs = rhs.as_str();

    if lhs == rhs {
        // skip `jmp`
        let ip = ip.offset(2);
        dispatch_current(vm, jt, ip, sp, lp)
    } else {
        // execute `jmp`
        let ip = ip.offset(1);
        dispatch_current(vm, jt, ip, sp, lp)
    }
}

#[inline(always)]
unsafe fn isnes(vm: Vm, jt: Jt, ip: Ip, args: Isnes, sp: Sp, lp: Lp) -> Control {
    // isnes v
    // jmp offset

    // TODO: string interning
    let lhs = *sp.at(args.lhs());
    let rhs = lp.str_unchecked(args.rhs());

    let Some(lhs) = lhs.into_object::<String>() else {
        // execute `jmp`
        let ip = ip.offset(1);
        return dispatch_current(vm, jt, ip, sp, lp);
    };

    let lhs = lhs.as_ref();
    let lhs = lhs.as_str();

    let rhs = rhs.as_ref();
    let rhs = rhs.as_str();

    if lhs != rhs {
        // skip `jmp`
        let ip = ip.offset(2);
        dispatch_current(vm, jt, ip, sp, lp)
    } else {
        // execute `jmp`
        let ip = ip.offset(1);
        dispatch_current(vm, jt, ip, sp, lp)
    }
}

#[inline(always)]
unsafe fn iseqi(vm: Vm, jt: Jt, ip: Ip, args: Iseqi, sp: Sp, lp: Lp) -> Control {
    // iseqi v
    // jmp offset

    let lhs = *sp.at(args.lhs());
    let rhs = lp.int_unchecked(args.rhs());

    let lhs = match lhs {
        ValueRaw::Int(v) => v,
        ValueRaw::Float(v) => v as i64,
        _ => {
            // execute `jmp`
            let ip = ip.offset(1);
            return dispatch_current(vm, jt, ip, sp, lp);
        }
    };

    if lhs == rhs {
        // skip `jmp`
        let ip = ip.offset(2);
        dispatch_current(vm, jt, ip, sp, lp)
    } else {
        // execute `jmp`
        let ip = ip.offset(1);
        dispatch_current(vm, jt, ip, sp, lp)
    }
}

#[inline(always)]
unsafe fn isnei(vm: Vm, jt: Jt, ip: Ip, args: Isnei, sp: Sp, lp: Lp) -> Control {
    // isnei v
    // jmp offset

    let lhs = *sp.at(args.lhs());
    let rhs = lp.int_unchecked(args.rhs());

    let lhs = match lhs {
        ValueRaw::Int(v) => v,
        ValueRaw::Float(v) => v as i64,
        _ => {
            // execute `jmp`
            let ip = ip.offset(1);
            return dispatch_current(vm, jt, ip, sp, lp);
        }
    };

    if lhs != rhs {
        // skip `jmp`
        let ip = ip.offset(2);
        dispatch_current(vm, jt, ip, sp, lp)
    } else {
        // execute `jmp`
        let ip = ip.offset(1);
        dispatch_current(vm, jt, ip, sp, lp)
    }
}

#[inline(always)]
unsafe fn iseqf(vm: Vm, jt: Jt, ip: Ip, args: Iseqf, sp: Sp, lp: Lp) -> Control {
    // iseqf v
    // jmp offset

    let lhs = *sp.at(args.lhs());
    let rhs = lp.float_unchecked(args.rhs());

    let lhs = match lhs {
        ValueRaw::Int(v) => v as f64,
        ValueRaw::Float(v) => v,
        _ => {
            // execute `jmp`
            let ip = ip.offset(1);
            return dispatch_current(vm, jt, ip, sp, lp);
        }
    };

    if lhs == rhs {
        // skip `jmp`
        let ip = ip.offset(2);
        dispatch_current(vm, jt, ip, sp, lp)
    } else {
        // execute `jmp`
        let ip = ip.offset(1);
        dispatch_current(vm, jt, ip, sp, lp)
    }
}

#[inline(always)]
unsafe fn isnef(vm: Vm, jt: Jt, ip: Ip, args: Isnef, sp: Sp, lp: Lp) -> Control {
    // isnef v
    // jmp offset

    let lhs = *sp.at(args.lhs());
    let rhs = lp.float_unchecked(args.rhs());

    let lhs = match lhs {
        ValueRaw::Int(v) => v as f64,
        ValueRaw::Float(v) => v,
        _ => {
            // execute `jmp`
            let ip = ip.offset(1);
            return dispatch_current(vm, jt, ip, sp, lp);
        }
    };

    if lhs != rhs {
        // skip `jmp`
        let ip = ip.offset(2);
        dispatch_current(vm, jt, ip, sp, lp)
    } else {
        // execute `jmp`
        let ip = ip.offset(1);
        dispatch_current(vm, jt, ip, sp, lp)
    }
}

macro_rules! try_cmp_eval {
    ($lhs:ident, $rhs:ident, $vm:ident, $ip:ident, $op:tt) => {{
        use ValueRaw::*;
        match ($lhs, $rhs) {
            // TODO: other types support comparisons too:
            //       nil, bool, str
            (Int(lhs), Int(rhs)) => lhs $op rhs,
            (Float(lhs), Float(rhs)) => lhs $op rhs,
            (Float(lhs), Int(rhs)) => lhs $op (rhs as f64),
            (Int(lhs), Float(rhs)) => (lhs as f64) $op rhs,
            _ => vm_exit!($vm, $ip, CmpTypeError),
        }
    }};
}

#[inline(always)]
unsafe fn islt(vm: Vm, jt: Jt, ip: Ip, args: Islt, sp: Sp, lp: Lp) -> Control {
    let lhs = *sp.at(args.lhs());
    let rhs = *sp.at(args.rhs());

    if try_cmp_eval!(lhs, rhs, vm, ip, <) {
        let ip = ip.offset(2);
        dispatch_current(vm, jt, ip, sp, lp)
    } else {
        let ip = ip.offset(1);
        dispatch_current(vm, jt, ip, sp, lp)
    }
}

#[inline(always)]
unsafe fn isle(vm: Vm, jt: Jt, ip: Ip, args: Isle, sp: Sp, lp: Lp) -> Control {
    let lhs = *sp.at(args.lhs());
    let rhs = *sp.at(args.rhs());

    if try_cmp_eval!(lhs, rhs, vm, ip, <=) {
        let ip = ip.offset(2);
        dispatch_current(vm, jt, ip, sp, lp)
    } else {
        let ip = ip.offset(1);
        dispatch_current(vm, jt, ip, sp, lp)
    }
}

#[inline(always)]
unsafe fn isgt(vm: Vm, jt: Jt, ip: Ip, args: Isgt, sp: Sp, lp: Lp) -> Control {
    let lhs = *sp.at(args.lhs());
    let rhs = *sp.at(args.rhs());

    if try_cmp_eval!(lhs, rhs, vm, ip, >) {
        let ip = ip.offset(2);
        dispatch_current(vm, jt, ip, sp, lp)
    } else {
        let ip = ip.offset(1);
        dispatch_current(vm, jt, ip, sp, lp)
    }
}

#[inline(always)]
unsafe fn isge(vm: Vm, jt: Jt, ip: Ip, args: Isge, sp: Sp, lp: Lp) -> Control {
    let lhs = *sp.at(args.lhs());
    let rhs = *sp.at(args.rhs());

    if try_cmp_eval!(lhs, rhs, vm, ip, >=) {
        let ip = ip.offset(2);
        dispatch_current(vm, jt, ip, sp, lp)
    } else {
        let ip = ip.offset(1);
        dispatch_current(vm, jt, ip, sp, lp)
    }
}

#[inline(always)]
unsafe fn iseq(vm: Vm, jt: Jt, ip: Ip, args: Iseq, sp: Sp, lp: Lp) -> Control {
    let lhs = *sp.at(args.lhs());
    let rhs = *sp.at(args.rhs());

    if try_cmp_eval!(lhs, rhs, vm, ip, ==) {
        let ip = ip.offset(2);
        dispatch_current(vm, jt, ip, sp, lp)
    } else {
        let ip = ip.offset(1);
        dispatch_current(vm, jt, ip, sp, lp)
    }
}

#[inline(always)]
unsafe fn isne(vm: Vm, jt: Jt, ip: Ip, args: Isne, sp: Sp, lp: Lp) -> Control {
    let lhs = *sp.at(args.lhs());
    let rhs = *sp.at(args.rhs());

    if try_cmp_eval!(lhs, rhs, vm, ip, !=) {
        let ip = ip.offset(2);
        dispatch_current(vm, jt, ip, sp, lp)
    } else {
        let ip = ip.offset(1);
        dispatch_current(vm, jt, ip, sp, lp)
    }
}

#[inline(always)]
unsafe fn isltv(vm: Vm, jt: Jt, ip: Ip, args: Isltv, sp: Sp, lp: Lp) -> Control {
    let dst = sp.at(args.dst());
    let lhs = *sp.at(args.lhs());
    let rhs = *sp.at(args.rhs());

    let result = try_cmp_eval!(lhs, rhs, vm, ip, <);

    *dst = ValueRaw::Bool(result);

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn islev(vm: Vm, jt: Jt, ip: Ip, args: Islev, sp: Sp, lp: Lp) -> Control {
    let dst = sp.at(args.dst());
    let lhs = *sp.at(args.lhs());
    let rhs = *sp.at(args.rhs());

    let result = try_cmp_eval!(lhs, rhs, vm, ip, <=);

    *dst = ValueRaw::Bool(result);

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn isgtv(vm: Vm, jt: Jt, ip: Ip, args: Isgtv, sp: Sp, lp: Lp) -> Control {
    let dst = sp.at(args.dst());
    let lhs = *sp.at(args.lhs());
    let rhs = *sp.at(args.rhs());

    let result = try_cmp_eval!(lhs, rhs, vm, ip, >);

    *dst = ValueRaw::Bool(result);

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn isgev(vm: Vm, jt: Jt, ip: Ip, args: Isgev, sp: Sp, lp: Lp) -> Control {
    let dst = sp.at(args.dst());
    let lhs = *sp.at(args.lhs());
    let rhs = *sp.at(args.rhs());

    let result = try_cmp_eval!(lhs, rhs, vm, ip, >=);

    *dst = ValueRaw::Bool(result);

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn iseqv(vm: Vm, jt: Jt, ip: Ip, args: Iseqv, sp: Sp, lp: Lp) -> Control {
    let dst = sp.at(args.dst());
    let lhs = *sp.at(args.lhs());
    let rhs = *sp.at(args.rhs());

    let result = try_cmp_eval!(lhs, rhs, vm, ip, ==);

    *dst = ValueRaw::Bool(result);

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn isnev(vm: Vm, jt: Jt, ip: Ip, args: Isnev, sp: Sp, lp: Lp) -> Control {
    let dst = sp.at(args.dst());
    let lhs = *sp.at(args.lhs());
    let rhs = *sp.at(args.rhs());

    let result = try_cmp_eval!(lhs, rhs, vm, ip, !=);

    *dst = ValueRaw::Bool(result);

    dispatch_next(vm, jt, ip, sp, lp)
}

macro_rules! try_arith_eval {
    ($dst:ident, $lhs:ident, $rhs:ident, $vm:ident, $ip:ident, $op:tt) => {
        use ValueRaw::*;
        match ($lhs, $rhs) {
            (Int(lhs), Int(rhs)) => {
                *$dst = Int(lhs $op rhs);
            }
            (Float(lhs), Float(rhs)) => {
                *$dst = Float(lhs $op rhs);
            }
            (Float(lhs), Int(rhs)) => {
                *$dst = Float(lhs $op (rhs as f64));
            }
            (Int(lhs), Float(rhs)) => {
                *$dst = Float((lhs as f64) $op rhs);
            }
            _ => {
                vm_exit!($vm, $ip, ArithTypeError);
            }
        }
    };
}

#[inline(always)]
unsafe fn addvv(vm: Vm, jt: Jt, ip: Ip, args: Addvv, sp: Sp, lp: Lp) -> Control {
    let dst = sp.at(args.dst());
    let lhs = *sp.at(args.lhs());
    let rhs = *sp.at(args.rhs());

    try_arith_eval!(dst, lhs, rhs, vm, ip, +);

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn addvn(vm: Vm, jt: Jt, ip: Ip, args: Addvn, sp: Sp, lp: Lp) -> Control {
    let dst = sp.at(args.dst());
    let lhs = *sp.at(args.lhs());
    let rhs = lp.int_or_float_unchecked(args.rhs());

    try_arith_eval!(dst, lhs, rhs, vm, ip, +);

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn addnv(vm: Vm, jt: Jt, ip: Ip, args: Addnv, sp: Sp, lp: Lp) -> Control {
    let dst = sp.at(args.dst());
    let lhs = lp.int_or_float_unchecked(args.lhs());
    let rhs = *sp.at(args.rhs());

    try_arith_eval!(dst, lhs, rhs, vm, ip, +);

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn subvv(vm: Vm, jt: Jt, ip: Ip, args: Subvv, sp: Sp, lp: Lp) -> Control {
    let dst = sp.at(args.dst());
    let lhs = *sp.at(args.lhs());
    let rhs = *sp.at(args.rhs());

    try_arith_eval!(dst, lhs, rhs, vm, ip, -);

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn subvn(vm: Vm, jt: Jt, ip: Ip, args: Subvn, sp: Sp, lp: Lp) -> Control {
    let dst = sp.at(args.dst());
    let lhs = *sp.at(args.lhs());
    let rhs = lp.int_or_float_unchecked(args.rhs());

    try_arith_eval!(dst, lhs, rhs, vm, ip, -);

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn subnv(vm: Vm, jt: Jt, ip: Ip, args: Subnv, sp: Sp, lp: Lp) -> Control {
    let dst = sp.at(args.dst());
    let lhs = lp.int_or_float_unchecked(args.lhs());
    let rhs = *sp.at(args.rhs());

    try_arith_eval!(dst, lhs, rhs, vm, ip, -);

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn mulvv(vm: Vm, jt: Jt, ip: Ip, args: Mulvv, sp: Sp, lp: Lp) -> Control {
    let dst = sp.at(args.dst());
    let lhs = *sp.at(args.lhs());
    let rhs = *sp.at(args.rhs());

    try_arith_eval!(dst, lhs, rhs, vm, ip, *);

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn mulvn(vm: Vm, jt: Jt, ip: Ip, args: Mulvn, sp: Sp, lp: Lp) -> Control {
    let dst = sp.at(args.dst());
    let lhs = *sp.at(args.lhs());
    let rhs = lp.int_or_float_unchecked(args.rhs());

    try_arith_eval!(dst, lhs, rhs, vm, ip, *);

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn mulnv(vm: Vm, jt: Jt, ip: Ip, args: Mulnv, sp: Sp, lp: Lp) -> Control {
    let dst = sp.at(args.dst());
    let lhs = lp.int_or_float_unchecked(args.lhs());
    let rhs = *sp.at(args.rhs());

    try_arith_eval!(dst, lhs, rhs, vm, ip, *);

    dispatch_next(vm, jt, ip, sp, lp)
}

macro_rules! try_div_eval {
    ($dst:ident, $lhs:ident, $rhs:ident, $vm:ident, $ip:ident) => {
        use ValueRaw::*;
        match ($lhs, $rhs) {
            (Int(lhs), Int(rhs)) => match lhs.checked_div(rhs) {
                Some(v) => *$dst = Int(v),
                None => {
                    vm_exit!($vm, $ip, DivisionByZero)
                }
            },

            // float div by zero = inf
            (Float(lhs), Float(rhs)) => {
                *$dst = Float(lhs / rhs);
            }
            (Float(lhs), Int(rhs)) => {
                *$dst = Float(lhs / (rhs as f64));
            }
            (Int(lhs), Float(rhs)) => {
                *$dst = Float((lhs as f64) / rhs);
            }
            _ => {
                vm_exit!($vm, $ip, ArithTypeError);
            }
        }
    };
}

#[inline(always)]
unsafe fn divvv(vm: Vm, jt: Jt, ip: Ip, args: Divvv, sp: Sp, lp: Lp) -> Control {
    let dst = sp.at(args.dst());
    let lhs = *sp.at(args.lhs());
    let rhs = *sp.at(args.rhs());

    try_div_eval!(dst, lhs, rhs, vm, ip);

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn divvn(vm: Vm, jt: Jt, ip: Ip, args: Divvn, sp: Sp, lp: Lp) -> Control {
    let dst = sp.at(args.dst());
    let lhs = *sp.at(args.lhs());
    let rhs = lp.int_or_float_unchecked(args.rhs());

    try_div_eval!(dst, lhs, rhs, vm, ip);

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn divnv(vm: Vm, jt: Jt, ip: Ip, args: Divnv, sp: Sp, lp: Lp) -> Control {
    let dst = sp.at(args.dst());
    let lhs = lp.int_or_float_unchecked(args.lhs());
    let rhs = *sp.at(args.rhs());

    try_div_eval!(dst, lhs, rhs, vm, ip);

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn unm(vm: Vm, jt: Jt, ip: Ip, args: Unm, sp: Sp, lp: Lp) -> Control {
    let dst = sp.at(args.dst());
    let rhs = *sp.at(args.rhs());

    use ValueRaw::*;
    match rhs {
        Int(rhs) => {
            *dst = Int(-rhs);
        }
        ValueRaw::Float(rhs) => {
            *dst = Float(-rhs);
        }
        _ => {
            vm_exit!(vm, ip, ArithTypeError);
        }
    }

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn not(vm: Vm, jt: Jt, ip: Ip, args: Not, sp: Sp, lp: Lp) -> Control {
    let dst = sp.at(args.dst());
    let rhs = *sp.at(args.rhs());

    *dst = ValueRaw::Bool(!rhs.coerce_bool());

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn call(vm: Vm, jt: Jt, ip: Ip, args: Call, sp: Sp, lp: Lp) -> Control {
    let ret = args.dst();
    let callee = *sp.at(args.callee());
    let nargs = args.args().get();

    if let Some(callee) = callee.into_object::<FunctionProto>() {
        if callee.as_ref().nparams != nargs {
            return arity_mismatch_error(ip, vm);
        }

        let (sp, lp, ip) = do_call(callee, ret, ip, vm);

        dispatch_current(vm, jt, ip, sp, lp)
    } else if let Some(callee) = callee.into_object::<Closure>() {
        if callee.as_ref().func.as_ref().nparams != nargs {
            return arity_mismatch_error(ip, vm);
        }

        let (sp, lp, ip) = do_closure_call(callee, ret, ip, vm);

        dispatch_current(vm, jt, ip, sp, lp)
    } else if let Some(callee) = callee.into_object::<HostFunction>() {
        if callee.as_ref().arity != nargs {
            return arity_mismatch_error(ip, vm);
        }

        let callee = callee.as_ref().f;
        do_host_call(callee, vm, jt, ip, sp, lp, ret, nargs)
    } else {
        not_callable_error(ip, vm)
    }
}

#[cold]
unsafe fn arity_mismatch_error(ip: Ip, vm: Vm) -> Control {
    vm_exit!(vm, ip, ArityMismatch)
}

#[cold]
unsafe fn not_callable_error(ip: Ip, vm: Vm) -> Control {
    vm_exit!(vm, ip, NotCallable)
}

#[inline(always)]
unsafe fn fastcall(vm: Vm, jt: Jt, ip: Ip, args: Fastcall, sp: Sp, lp: Lp) -> Control {
    let ret = args.dst();
    let callee = vm.get_function_in_current_module(args.id());

    let (sp, lp, ip) = do_call(callee, ret, ip, vm);

    dispatch_current(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn hostcall(vm: Vm, jt: Jt, ip: Ip, args: Hostcall, sp: Sp, lp: Lp) -> Control {
    let ret = args.dst();
    let callee = vm.get_host_function(args.id());

    let nargs = callee.as_ref().arity;
    let callee = callee.as_ref().f;

    do_host_call(callee, vm, jt, ip, sp, lp, ret, nargs)
}

#[inline(always)]
unsafe fn ret(vm: Vm, jt: Jt, ip: Ip, args: Ret, sp: Sp, lp: Lp) -> Control {
    let (sp, lp, ip) = return_from_call(vm);

    dispatch_current(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn retv(vm: Vm, jt: Jt, ip: Ip, args: Retv, sp: Sp, lp: Lp) -> Control {
    *sp.ret() = *sp.at(args.src());

    let (sp, lp, ip) = return_from_call(vm);

    dispatch_current(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn stop(vm: Vm, jt: Jt, ip: Ip, args: Stop, sp: Sp, lp: Lp) -> Control {
    Control::stop()
}

/// Call procedure:
/// 1. Grow stack if needed
/// 2. Allocate new call frame
/// 3. Update VM state
/// 4. Jump to start of callee
///
/// This function does not check arity.
///
/// New call frame's stack overlaps with the current frame's stack
/// example: assuming 3 args, with ret at r6:
/// ```text,ignore
///   frame N:   [ 0 1 2 3 4 5 6 7 8 9 ]
///   frame N+1:             [ 6 7 8 9 ... ]
///                         ret^ ^args
/// ```
/// `r0` in the new frame will be in the same location as `r6`
/// in the previous frame.
#[inline(always)]
unsafe fn do_call(callee: GcPtr<FunctionProto>, ret: Reg, ip: Ip, vm: Vm) -> (Sp, Lp, Ip) {
    // See doc comment.
    let stack_base = vm.current_frame().stack_base() + (ret.get() as u32);
    let frame_size = callee.as_ref().stack_size();

    // Return addr points to the next instruction after the call instruction.
    let current_function_start = Ip::from_fn(vm.current_frame().callee());
    let return_addr = 1 + (ip.offset_from_unsigned(current_function_start) as u32);

    let sp: Sp = maybe_grow_stack(vm, stack_base as usize, frame_size);
    let lp: Lp = Lp::from_fn(callee);
    let ip: Ip = Ip::from_fn(callee);
    vm.set_current_module_for(callee);
    vm.push_frame(CallFrame {
        callee,
        stack_base,
        return_addr,
    });

    (sp, lp, ip)
}

#[inline(always)]
unsafe fn do_closure_call(callee: GcPtr<Closure>, ret: Reg, ip: Ip, vm: Vm) -> (Sp, Lp, Ip) {
    vm.set_current_captures_for(callee);

    do_call(callee.as_ref().func, ret, ip, vm)
}

#[inline(always)]
unsafe fn do_host_call(
    callee: HostFunctionCallback,
    vm: Vm,
    jt: Jt,
    ip: Ip,
    sp: Sp,
    lp: Lp,
    ret: Reg,
    nargs: u8,
) -> Control {
    let context = {
        let stack_base = (vm.current_frame().stack_base() + (ret.get() as u32)) as usize;
        let sp: Sp = vm.stack_at(stack_base);
        Context::new(vm, sp, nargs)
    };
    match callee(context) {
        Ok(value) => {
            *sp.at(ret) = value;

            dispatch_next(vm, jt, ip, sp, lp)
        }
        Err(err) => {
            vm.set_saved_ip(ip);
            vm.write_error(err);

            Control::error(VmError::Host)
        }
    }
}

#[inline(always)]
unsafe fn maybe_grow_stack(vm: Vm, stack_base: usize, frame_size: usize) -> Sp {
    if !vm.has_enough_stack_space(stack_base, frame_size) {
        grow_stack(vm, frame_size)
    }

    vm.stack_at(stack_base)
}

#[cold]
unsafe fn grow_stack(vm: Vm, frame_size: usize) {
    // NOTE: We allocate more than we need here, capacity doubles each time anyway.
    vm.grow_stack(frame_size);
}

#[inline(always)]
unsafe fn return_from_call(vm: Vm) -> (Sp, Lp, Ip) {
    // Only called from `ret`, meaning we are guaranteed to have
    // at least the one call frame which is currently being executed.

    let returning_from = core::ptr::replace(vm.current_frame().raw(), vm.pop_frame_unchecked());
    let returning_to = vm.current_frame();

    let stack_base = returning_to.stack_base() as usize;
    let return_addr = returning_from.return_addr as usize;

    let sp: Sp = vm.stack_at(stack_base);
    let lp: Lp = Lp::from_fn(returning_to.callee());
    let ip: Ip = Ip::from_fn(returning_to.callee()).offset(return_addr as isize);
    vm.set_current_module_for(returning_to.callee());

    (sp, lp, ip)
}

pub trait StdioWrite: std::io::Write + std::any::Any + 'static {
    fn as_any(&self) -> &dyn std::any::Any;
    fn as_any_mut(&mut self) -> &mut dyn std::any::Any;
}
impl<T: std::io::Write + std::any::Any> StdioWrite for T {
    fn as_any(&self) -> &dyn std::any::Any {
        self
    }

    fn as_any_mut(&mut self) -> &mut dyn std::any::Any {
        self
    }
}

pub struct Stdio {
    pub stdout: Box<dyn StdioWrite>,
    pub stderr: Box<dyn StdioWrite>,
}

impl Default for Stdio {
    fn default() -> Self {
        Self {
            stdout: Box::new(std::io::stdout()),
            stderr: Box::new(std::io::stderr()),
        }
    }
}

type Invariant<'a> = PhantomData<fn(&'a ()) -> &'a ()>;

#[repr(C)]
pub struct Hebi {
    /// Boxed to guarantee address stability
    inner: Box<VmState>,
}

pub(crate) struct VmState {
    /// Garbage collected heap
    heap: gc::Heap,

    /// Storage for loaded modules
    registry: ModuleRegistry,

    core: RuntimeCoreLib,

    stdio: Stdio,

    /// VM "registers"
    stack: DynArray<ValueRaw>,

    /// Invariant: Should never be empty.
    frames: DynStack<CallFrame>,

    /// When executing a function, we store the module it belongs to here,
    /// so that we can `fastcall` other functions from the same module.
    ///
    /// NOTE: This is always `Some` while the VM is executing
    current_module: Option<GcPtr<ModuleProto>>,

    current_captures: Option<Captures>,

    /// Saved error.
    ///
    /// Used to avoid bloating size of instruction handler return value.
    error: Option<Error>,

    /// Saved instruction pointer.
    ///
    /// Used when the VM encounters an error, can be traced back to its
    /// associated span by the error handler.
    saved_ip: Option<Ip>,
}

impl Hebi {
    pub fn new() -> Self {
        // 1 MiB
        const INITIAL_STACK_SIZE: usize = (1024 * 1024) / std::mem::size_of::<ValueRaw>();
        const STACK_DEPTH: usize = INITIAL_STACK_SIZE / 16;

        let heap = gc::Heap::new();

        Hebi {
            inner: Box::new(VmState {
                registry: ModuleRegistry::new(),
                core: RuntimeCoreLib::init(&heap),
                stdio: Stdio::default(),
                stack: DynArray::new(INITIAL_STACK_SIZE),
                frames: DynStack::new(STACK_DEPTH),

                current_module: None,
                current_captures: None,

                error: None,
                saved_ip: None,

                heap,
            }),
        }
    }

    pub fn with_stdio(mut self, stdio: Stdio) -> Self {
        self.inner.stdio = stdio;
        self
    }

    #[inline(always)]
    pub fn with<F>(&mut self, f: F)
    where
        F: for<'gc> FnOnce(Runtime<'gc>),
    {
        f(Runtime {
            vm: NonNull::from_mut(&mut *self.inner),
            _lifetime: PhantomData,
        })
    }
}

struct VmRoots {
    state: NonNull<VmState>,
}

impl gc::ExternalRoots for VmRoots {
    unsafe fn trace(&self, tracer: &gc::Tracer) {
        // note: call frame stack itself is not traced despite storing `Gc<FunctionProto>`,
        // because every function is also traced through the module registry.

        macro_rules! this {
            () => {
                *self.state.as_ptr()
            };
        }

        // iterate over VM stack
        let frames = &this!().frames;
        let sp = this!().stack.offset(0);
        for frame in this!().frames.iter() {
            let base = frame.stack_base as usize;
            let nstack = frame.callee.as_ref().stack_size();
            for i in base..base + nstack {
                tracer.visit_value(sp.add(i).read());
            }
        }

        for module in this!().registry.iter() {
            tracer.visit(module);
        }
    }
}

#[repr(transparent)]
pub struct Module<'vm> {
    inner: GcPtr<ModuleProto>,
    _lifetime: PhantomData<fn(&'vm ()) -> &'vm ()>,
}

pub struct Runtime<'vm> {
    vm: NonNull<VmState>,

    _lifetime: Invariant<'vm>,
}

// TODO: none of the public APIs should return `ValueRaw`.
// Currently they are kept alive by the fact that we only
// run gc during allocating instructions (`larr` and friends).
impl<'vm> Runtime<'vm> {
    pub fn stdio(&mut self) -> &mut Stdio {
        unsafe { &mut self.vm.as_mut().stdio }
    }

    /// Load a module into the VM's module registry.
    pub fn load(&mut self, m: &module::Module) -> Module<'vm> {
        let vm = unsafe { self.vm.as_mut() };
        let heap = &mut vm.heap;
        let registry = &mut vm.registry;
        let m = registry.add(heap, m);
        Module {
            inner: m,
            _lifetime: PhantomData,
        }
    }

    /// Execute the main entrypoint of the module to completion.
    ///
    /// Note that modules must go through [`Runtime::load`] first.
    pub fn run(&mut self, m: &Module<'vm>) -> Result<ValueRaw> {
        self.run_inner(m.inner)?;

        // SAFETY: Functions always store their return values in slot 0,
        // and are guaranteed to always allocate at least that slot.
        //
        // TODO: The resulting value is alive only because of implementation
        // details - we need to guarantee this _somehow_. Currently no GC happens
        // between the last `ret` and this point, and all stack values used
        // by the main entrypoint are guaranteed to not have been collected yet
        // until after its `ret`, at which point the stack frame is considered
        // dead.
        let vm = unsafe { self.vm.as_mut() };
        let value = unsafe { *vm.stack.offset(0) };
        Ok(value)
    }

    fn run_inner(&mut self, m: GcPtr<ModuleProto>) -> Result<()> {
        unsafe {
            Vm(self.vm).push_frame(CallFrame {
                callee: m.as_ref().entrypoint().as_ptr(),
                stack_base: 0,
                return_addr: 0,
            });
            self.vm.as_mut().current_module = Some(m);

            let vm: Vm = Vm(self.vm);
            let jt: Jt = JT.as_ptr();
            let sp: Sp = vm.stack_at(0);
            let lp: Lp = Lp::from_fn(vm.current_frame().callee());
            let ip: Ip = Ip::from_fn(vm.current_frame().callee());

            // In debug mode and Wasm, fall back to loop+match.
            //
            // Each instruction will return `Control::Continue`
            // instead of tail-calling the next handler.
            #[cfg(any(debug_assertions, target_arch = "wasm32"))]
            {
                let mut sp: Sp = sp;
                let mut lp: Lp = lp;
                let mut ip: Ip = ip;

                loop {
                    let insn = ip.get();
                    let op = jt.at(insn);
                    match op(vm, jt, ip, insn, sp, lp) {
                        Control::Stop => return Ok(()),
                        Control::Error(err) if err.is_host() => {
                            let err = vm.take_error();
                            return Err(err.unwrap());
                        }
                        Control::Error(err) => return Err(err.annotate(vm)),

                        #[cfg(any(debug_assertions, target_arch = "wasm32"))]
                        Control::Continue(new_sp, new_lp, new_ip) => {
                            sp = new_sp;
                            lp = new_lp;
                            ip = new_ip;
                            continue;
                        }
                    }
                }
            }

            #[cfg(not(any(debug_assertions, target_arch = "wasm32")))]
            {
                let ctrl = dispatch_current(vm, jt, ip, sp, lp);

                if ctrl.is_error() {
                    let err = ctrl.error_code();
                    if err.is_external() {
                        let err = vm.take_error();
                        return Err(err.unwrap());
                    } else {
                        return Err(err.annotate(vm));
                    }
                }

                return Ok(());
            }
        }
    }
}

#[cfg(test)]
mod tests;
