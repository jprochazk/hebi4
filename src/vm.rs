#![allow(unsafe_op_in_unsafe_fn)]

//! ## Safety in the VM
//!
//! The VM uses a _LOT_ of unsafe code. Too much, in fact. Some of it can likely be
//! removed without introducing performance overhead, but performance is the primary goal
//! of doing this in the first place. The usual approach of "take safe code, and sprinkle
//! in unsafe carefully to improve performance" never worked out for me. My understanding
//! for why is that it was always a death by a thousand cuts: Bounds checks, drop glue,
//! overly large types, extra allocations, runtime assertions, panic machinery, etc.
//!
//! Instead, the VM here attempts to start from a relatively clean slate. We try as much
//! as possible to reduce the chances for error by using type-safe wrappers and limiting
//! the usage of Rust references, which would require us to uphold their strong guarantees
//! everywhere.
//!
//! We're making a wild assumption: The code generated by the Hebi compiler is valid.
//! I believe that the compiler is simple enough that it's possible to comprehensively check
//! all of it. Extensive codegen and runtime tests (including fuzzing) are used to help
//! validate those assumptions.
//!
//! Assuming the compiler is indeed correct, it upholds certain invariants which we can use
//! to reduce runtime overhead in the VM.
//!
//! All accesses to the VM stack are guaranteed to be safe:
//! - The stack always has enough space for the maximum possible number of registers that
//!   a given function needs.
//! - A value on the stack is guaranteed to be initialized before it is first read.
//!
//! A concrete example of this is the `Reg` (register) newtype used to offset `Sp` (stack pointer),
//! the operation will _always_ yield a properly-aligned pointer into a memory located within bounds
//! of an allocation, removing the need for a bounds check, or dynamically growing the stack
//! on demand when writing to it.
//!
//! Not just that, but because a value in a register is never read before it's initialized,
//! we also don't need to pre-initialize the contents of the stack with default values.
//! It is allocated using `alloc_zeroed` (though technically `0` is a valid bit pattern for `Value`),
//! and then pointers to this allocation are handed out as needed.
//!
//! Taking advantage of all of this, Hebi's `mov` instruction can compile down to just:
//! - A decode of the instruction operands (two zero-extended movs from a register)
//! - The actual `mov` of the value between two memory locations
//! - A dispatch of the next instruction (a few `mov`s and a `jmp rax`)
//!
//! That's as low-overhead as it gets, which is the ultimate goal here.

mod array;
mod disasm;

#[macro_use]
pub mod gc;

pub mod value;

use beef::lean::Cow;

use std::marker::PhantomData;

use array::{DynArray, DynStack};

use crate::{
    codegen::opcodes::*,
    error::{Error, Result, error},
    span::Span,
    vm::value::{Literal, ValueRaw},
};

/// Represents the result of code generation: A list of functions with
/// one of them marked as the entrypoint.
///
/// Can be executed in a [`Vm`].
pub struct Chunk {
    main: FnId,
    functions: Box<[FuncInfo]>,
}

impl Chunk {
    pub(crate) fn new(main: FnId, functions: Vec<FuncInfo>) -> Self {
        Self {
            main,
            functions: functions.into_boxed_slice(),
        }
    }
}

// TODO: colocate data (?)
/// A function "prototype".
///
/// Stores the actual code and associated constants generated
/// by the compiler.
///
pub struct FuncInfo {
    name: Cow<'static, str>,
    nparams: u8,
    nstack: u8,
    // These fields aren't `Box`, because we _want_ to store
    // static/stack-allocated data in them.
    // They aren't `Cow`, because that inflates the struct size,
    // and it's already quite large.
    code: Box<[Instruction]>,
    literals: Box<[Literal]>,
    dbg: Option<Box<dbg::FuncDebugInfo>>,
}

impl FuncInfo {
    pub(crate) fn new(
        name: impl Into<Cow<'static, str>>,
        nparams: u8,
        nstack: u8,
        code: Vec<Instruction>,
        literals: Vec<Literal>,
        dbg: dbg::FuncDebugInfo,
    ) -> Self {
        Self {
            name: name.into(),
            nparams,
            nstack,
            code: code.into_boxed_slice(),
            literals: literals.into_boxed_slice(),
            dbg: Some(Box::new(dbg)),
        }
    }
}

pub mod dbg {
    use super::*;

    pub struct FuncDebugInfo {
        pub(crate) spans: Box<[Span]>,
        pub(crate) locals: Box<[Local]>,
    }

    pub struct Local {
        pub(crate) span: Span,
        pub(crate) reg: Reg,
    }
}

// NOTE: `Value` and `Literal` must be partialy bit-compatible
// TODO: check this at compile time

impl Literal {
    #[inline]
    pub fn int(&self) -> Option<i64> {
        match self {
            Self::Int(v) => Some(*v),
            _ => None,
        }
    }

    #[inline]
    pub fn float(&self) -> Option<f64> {
        match self {
            Self::Float(v) => Some(*v),
            _ => None,
        }
    }

    #[inline]
    pub fn str(&self) -> Option<&String> {
        match self {
            Self::String(v) => Some(v),
            _ => None,
        }
    }
}

#[derive(Clone, Copy)]
struct FuncInfoPtr(*const FuncInfo);

impl FuncInfoPtr {
    #[inline]
    unsafe fn code(self) -> Ip {
        Ip(
            (*self.0)
                .code
                .as_ptr()
                .cast::<Instruction>() // drop the length,
                .cast::<RawInstruction>(), // _then_ cast to raw
        )
    }

    #[inline]
    unsafe fn literals(self) -> Lp {
        Lp(
            (*self.0).literals.as_ptr().cast::<Literal>(), // drop the length
        )
    }

    #[inline]
    unsafe fn nstack(self) -> u8 {
        (*self.0).nstack
    }
}

impl Sp {
    #[inline(always)]
    pub unsafe fn at(self, r: Reg) -> *mut ValueRaw {
        self.0.offset(r.sz())
    }
}

impl Jt {
    #[inline(always)]
    pub unsafe fn at(self, inst: RawInstruction) -> OpaqueOp {
        self.0.offset(inst.tag as isize).read()
    }
}

impl Ip {
    #[inline]
    unsafe fn offset_from_unsigned(self, other: Self) -> usize {
        (self.0).offset_from_unsigned(other.0)
    }

    #[inline]
    unsafe fn offset(self, n: usize) -> Self {
        Ip((self.0).offset(n as isize))
    }

    #[inline(always)]
    unsafe fn next(self) -> Self {
        Self(self.0.offset(1))
    }

    #[inline(always)]
    unsafe fn get(self) -> RawInstruction {
        self.0.read()
    }
}

#[derive(Clone, Copy)]
struct CallFrame {
    callee: FuncInfoPtr,

    /// Stack base of _this_ frame.
    stack_base: u32,

    /// Address of the next instruction to execute
    /// after returning from _this_ call frame.
    ///
    /// Points into the previous call frame's callee,
    /// if there is one.
    return_addr: u32,
}

#[derive(Clone, Copy)]
struct CallFramePtr(*mut CallFrame);

impl CallFramePtr {
    #[inline]
    unsafe fn raw(self) -> *mut CallFrame {
        self.0
    }

    #[inline]
    unsafe fn callee(self) -> FuncInfoPtr {
        (*self.0).callee
    }

    #[inline]
    unsafe fn stack_base(self) -> u32 {
        (*self.0).stack_base
    }

    #[inline]
    unsafe fn return_addr(self) -> u32 {
        (*self.0).return_addr
    }
}

#[repr(C)]
pub(crate) struct Context {
    vm: *mut Vm,
    current_module: *const Chunk,
    error: *mut Option<Error>,
    current_frame: CallFrame,
}

impl Ctx {
    #[inline]
    unsafe fn current_frame(self) -> CallFramePtr {
        CallFramePtr(&raw mut (*self.0).current_frame)
    }

    #[inline]
    unsafe fn push_frame(self, frame: CallFrame) {
        let frames = &mut (*(*self.0).vm).frames;
        frames.push(frame);
    }

    #[inline]
    unsafe fn pop_frame_unchecked(self) -> CallFrame {
        let frames = &mut (*(*self.0).vm).frames;
        frames.pop_unchecked()
    }

    #[inline]
    unsafe fn has_enough_stack_space(self, stack_base: usize, nstack: usize) -> bool {
        let stack = &mut (*(*self.0).vm).stack;
        let remaining = stack.remaining(stack_base);
        remaining >= (nstack as isize)
    }

    #[inline]
    unsafe fn stack_at(self, stack_base: usize) -> Sp {
        let stack = &mut (*(*self.0).vm).stack;
        Sp(stack.offset(stack_base))
    }

    #[inline]
    unsafe fn get_function_in_current_module(self, id: FnId) -> FuncInfoPtr {
        let chunk = (*self.0).current_module;
        // remove len
        let functions = (*chunk).functions.as_ptr().cast::<FuncInfo>();
        FuncInfoPtr(functions.offset(id.sz()))
    }
}

enum VmError {
    DivisionByZero,
}

impl VmError {
    fn with_context(&self, ctx: &Context) -> Error {
        let span = Span::empty(); // TODO
        match self {
            VmError::DivisionByZero => error("division by zero", span),
        }
    }
}

impl From<VmError> for Error {
    fn from(value: VmError) -> Self {
        match value {
            VmError::DivisionByZero => todo!(),
        }
    }
}

static JT: JumpTable = jump_table! {
    nop,
    mov,
    lnil,
    lsmi,
    ltrue,
    lfalse,
    lint,
    lnum,
    lstr,
    lcli,
    lfni,
    larr,
    lobj,
    jmp,
    istrue,
    istruec,
    isfalse,
    isfalsec,
    islt,
    isle,
    isgt,
    isge,
    iseq,
    isne,
    iseqs,
    isnes,
    iseqn,
    isnen,
    iseqp,
    isnep,
    addvv,
    addvn,
    addnv,
    subvv,
    subvn,
    subnv,
    mulvv,
    mulvn,
    mulnv,
    divvv,
    divvn,
    divnv,
    unm,
    not,
    call,
    fastcall,
    ret,
    stop,
    trap,
};

#[derive(Clone, Copy)]
#[repr(u8)]
pub enum Control {
    Yield = 0,
    Error = 1,

    #[cfg(debug_assertions)]
    Continue(Sp, Lp, Ip),
}

#[inline(always)]
unsafe fn dispatch_current(jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    #[cfg(debug_assertions)]
    {
        Control::Continue(sp, lp, ip)
    }

    #[cfg(not(debug_assertions))]
    {
        let inst = ip.get();
        let op = jt.at(inst);
        op(inst, jt, sp, lp, ip, ctx)
    }
}

#[inline(always)]
unsafe fn dispatch_next(jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    let ip = ip.next();
    dispatch_current(jt, sp, lp, ip, ctx)
}

#[inline(always)]
unsafe fn nop(args: Nop, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    dispatch_next(jt, sp, lp, ip, ctx)
}

#[inline(always)]
unsafe fn mov(args: Mov, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn lnil(args: Lnil, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn lsmi(args: Lsmi, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    *sp.at(args.dst) = ValueRaw::Int(args.v.get() as i64);

    dispatch_next(jt, sp, lp, ip, ctx)
}

#[inline(always)]
unsafe fn lint(args: Lint, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn lnum(args: Lnum, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn lstr(args: Lstr, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn ltrue(args: Ltrue, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn lfalse(args: Lfalse, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn lcli(args: Lcli, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn lfni(args: Lfni, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn larr(args: Larr, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn lobj(args: Lobj, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn jmp(args: Jmp, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn istrue(args: Istrue, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn istruec(args: Istruec, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn isfalse(args: Isfalse, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn isfalsec(args: Isfalsec, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn islt(args: Islt, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn isle(args: Isle, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn isgt(args: Isgt, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn isge(args: Isge, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn iseq(args: Iseq, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn isne(args: Isne, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn iseqs(args: Iseqs, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn isnes(args: Isnes, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn iseqn(args: Iseqn, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn isnen(args: Isnen, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn iseqp(args: Iseqp, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn isnep(args: Isnep, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn addvv(args: Addvv, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn addvn(args: Addvn, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn addnv(args: Addnv, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn subvv(args: Subvv, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn subvn(args: Subvn, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn subnv(args: Subnv, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn mulvv(args: Mulvv, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn mulvn(args: Mulvn, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn mulnv(args: Mulnv, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn divvv(args: Divvv, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn divvn(args: Divvn, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn divnv(args: Divnv, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn unm(args: Unm, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn not(args: Not, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn call(args: Call, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    todo!()
}

#[inline(always)]
unsafe fn fastcall(args: Fastcall, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    let ret = args.dst;
    let callee = ctx.get_function_in_current_module(args.id);

    let (sp, lp, ip) = do_call(callee, ret, ip, ctx);

    dispatch_current(jt, sp, lp, ip, ctx)
}

#[inline(always)]
unsafe fn ret(args: Ret, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    let (sp, lp, ip) = return_from_call(ctx);

    dispatch_current(jt, sp, lp, ip, ctx)
}

#[inline(always)]
unsafe fn stop(args: Stop, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    Control::Yield
}

#[inline(always)]
unsafe fn trap(args: Trap, jt: Jt, sp: Sp, lp: Lp, ip: Ip, ctx: Ctx) -> Control {
    unreachable!();
}

/// call procedure:
/// 1. grow stack if needed
/// 2. allocate new call frame
/// 3. jump to start of callee
///
/// new call frame's stack overlaps with the current frame's stack
/// example: assuming 3 args, with ret at r6:
/// ```ignore
///   frame N:   [ 0 1 2 3 4 5 6 7 8 9 ]
///                            ^ ^
///                            | args
///                            ret
///   frame N+1: [ 6 7 8 9 ... ]
///                ^ ^
///                | args
///                ret
/// ```
/// `r0` in the new frame will be in the same location as `r6`
/// in the previous frame.
#[inline(always)]
unsafe fn do_call(callee: FuncInfoPtr, ret: Reg, ip: Ip, ctx: Ctx) -> (Sp, Lp, Ip) {
    // See doc comment.
    let stack_base = ctx.current_frame().stack_base() + (ret.get() as u32);

    // Return addr points to the next instruction after the call instruction.
    let current_function_start = ctx.current_frame().callee().code();
    let return_addr = 1 + (ip.offset_from_unsigned(current_function_start) as u32);

    let new_frame = CallFrame {
        callee,
        stack_base,
        return_addr,
    };

    let sp: Sp = maybe_grow_stack(ctx, &new_frame);
    let lp: Lp = new_frame.callee.literals();
    let ip: Ip = new_frame.callee.code();

    let prev_frame = core::ptr::replace(ctx.current_frame().raw(), new_frame);
    ctx.push_frame(prev_frame);

    (sp, lp, ip)
}

#[inline(always)]
unsafe fn maybe_grow_stack(ctx: Ctx, new_frame: &CallFrame) -> Sp {
    let new_stack_base = new_frame.stack_base as usize;
    let new_frame_size = new_frame.callee.nstack() as usize;
    if !ctx.has_enough_stack_space(new_stack_base, new_frame_size) {
        grow_stack(ctx, new_frame)
    }

    ctx.stack_at(new_stack_base)
}

#[inline(never)]
#[cold]
unsafe fn grow_stack(ctx: Ctx, new_frame: &CallFrame) {
    // NOTE: We allocate more than we need here, capacity doubles each time anyway.
    let stack = &mut (*(*ctx.0).vm).stack;
    stack.grow(new_frame.callee.nstack() as usize)
}

#[inline(always)]
unsafe fn return_from_call(ctx: Ctx) -> (Sp, Lp, Ip) {
    // Only called from `ret`, meaning we are guaranteed to have
    // at least the one call frame which is currently being executed.

    let returning_from = core::ptr::replace(ctx.current_frame().raw(), ctx.pop_frame_unchecked());
    let returning_to = ctx.current_frame();

    let stack_base = returning_to.stack_base() as usize;
    let return_addr = returning_from.return_addr as usize;

    let sp: Sp = ctx.stack_at(stack_base);
    let lp: Lp = returning_to.callee().literals();
    let ip: Ip = returning_to.callee().code().offset(return_addr);

    (sp, lp, ip)
}

type Invariant<'a> = PhantomData<fn(&'a ()) -> &'a ()>;

pub struct Module<'gc> {
    _lifetime: Invariant<'gc>,
}

pub struct Vm {
    stack: DynArray<ValueRaw>,

    /// Invariant: Should never be empty.
    ///
    /// The current frame is stored inline in `Context`
    /// to reduce indirection, as it is frequently accessed,
    /// and the interpreter always holds a pointer to `Context`
    /// directly in a register.
    frames: DynStack<CallFrame>,
}

impl Vm {
    pub fn new() -> Self {
        // 1 MiB
        const INITIAL_STACK_SIZE: usize = (1024 * 1024) / std::mem::size_of::<ValueRaw>();
        const STACK_DEPTH: usize = INITIAL_STACK_SIZE / 16;

        Vm {
            stack: DynArray::new(INITIAL_STACK_SIZE),
            frames: DynStack::new(STACK_DEPTH),
        }
    }

    #[inline(always)]
    pub fn with<F, R>(&mut self, f: F) -> R
    where
        F: for<'gc> FnOnce(Runtime<'gc>) -> R,
        R: 'static,
    {
        f(Runtime {
            vm: self,
            _lifetime: PhantomData,
        })
    }
}

pub struct Runtime<'gc> {
    vm: *mut Vm,

    _lifetime: Invariant<'gc>,
}

impl<'gc> Runtime<'gc> {
    /// Run the code once.
    pub fn run_once(&mut self, chunk: Chunk) -> Result<()> {
        unsafe {
            let mut error = None;

            // `ret`, assumes that it always returns _into_ something.
            // That allows `ret` to `pop_frame_unchecked`, removing a
            // branch from a very hot code path.
            // For the `main` entrypoint of a chunk, this is what it
            // will `ret` into. All it does is call the chunk's `main`,
            // and then halt the interpreter.
            let mut entry = [
                asm::fastcall(Reg::new_unchecked(0), chunk.main),
                asm::stop(),
            ];

            let entry = core::mem::ManuallyDrop::new(FuncInfo {
                name: "@entry".into(),
                nparams: 0,
                nstack: 1,
                // SAFETY: never dropped
                code: Box::from_raw(&mut entry),
                literals: Box::from_raw(&mut []),
                dbg: None,
            });

            let mut ctx = Context {
                vm: self.vm,
                current_module: &raw const chunk,
                error: &raw mut error,
                current_frame: CallFrame {
                    callee: FuncInfoPtr(&*entry),
                    stack_base: 0,
                    return_addr: 0,
                },
            };
            let ctx: Ctx = Ctx(&mut ctx);

            let jt: Jt = JT.as_ptr();
            let sp: Sp = ctx.stack_at(0);
            let lp: Lp = ctx.current_frame().callee().literals();
            let ip: Ip = ctx.current_frame().callee().code();

            // In debug mode, fall back to loop+match.
            #[cfg(debug_assertions)]
            {
                let mut sp: Sp = sp;
                let mut lp: Lp = lp;
                let mut ip: Ip = ip;

                loop {
                    let inst = ip.get();
                    let op = jt.at(inst);
                    match op(inst, jt, sp, lp, ip, ctx) {
                        Control::Yield => return Ok(()),
                        Control::Error => return Err(error.unwrap()),

                        #[cfg(debug_assertions)]
                        Control::Continue(new_sp, new_lp, new_ip) => {
                            sp = new_sp;
                            lp = new_lp;
                            ip = new_ip;
                            continue;
                        }
                    }
                }
            }

            // Release mode uses tail calls.
            #[cfg(not(debug_assertions))]
            {
                match dispatch_current(jt, sp, lp, ip, ctx) {
                    Control::Yield => return Ok(()),
                    Control::Error => return Err(error.unwrap()),
                }
            }
        }
    }

    // /// Compile a module, which gives you access to functions
    // /// declared within it.
    // ///
    // /// This will execute the top-level code in the chunk.
    // pub fn compile(&mut self, chunk: Chunk) -> Result<Gc<'gc, Module>> {
    //     todo!()
    // }
}

#[cfg(test)]
mod tests;
