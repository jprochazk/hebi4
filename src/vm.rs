//! ## Safety in the VM
//!
//! The VM uses a _LOT_ of unsafe code. Too much, in fact. Some of it can likely be
//! removed without introducing performance overhead, but performance is the primary goal
//! of doing this in the first place. The usual approach of "take safe code, and sprinkle
//! in unsafe carefully to improve performance" never worked out for me. My understanding
//! for why is that it was always a death by a thousand cuts: Bounds checks, drop glue,
//! overly large types, extra allocations, runtime assertions, panic machinery, etc.
//!
//! Instead, the VM here attempts to start from a relatively clean slate. We try as much
//! as possible to reduce the chances for error by using type-safe wrappers and limiting
//! the usage of Rust references, which would require us to uphold their strong guarantees
//! everywhere.
//!
//! We're making a wild assumption: The code generated by the Hebi compiler is valid.
//! I believe that the compiler is simple enough that it's possible to comprehensively check
//! all of it. Extensive codegen and runtime tests (including fuzzing) are used to help
//! validate those assumptions.
//!
//! Assuming the compiler is indeed correct, it upholds certain invariants which we can use
//! to reduce runtime overhead in the VM.
//!
//! All accesses to the VM stack are guaranteed to be safe:
//! - The stack always has enough space for the maximum possible number of registers that
//!   a given function needs.
//! - A value on the stack is guaranteed to be initialized before it is first read.
//!
//! A concrete example of this is the `Reg` (register) newtype used to offset `Sp` (stack pointer),
//! the operation will _always_ yield a properly-aligned pointer into a memory located within bounds
//! of an allocation, removing the need for a bounds check, or dynamically growing the stack
//! on demand when writing to it.
//!
//! Not just that, but because a value in a register is never read before it's initialized,
//! we also don't need to pre-initialize the contents of the stack with default values.
//! It is allocated using `alloc_zeroed` (though technically `0` is a valid bit pattern for `Value`),
//! and then pointers to this allocation are handed out as needed.
//!
//! Taking advantage of all of this, Hebi's `mov` instruction can compile down to just:
//! - A decode of the instruction operands (two zero-extended movs from a register)
//! - The actual `mov` of the value between two memory locations
//! - A dispatch of the next instruction (a few `mov`s and a `jmp rax`)
//!
//! That's as low-overhead as it gets, which is the ultimate goal here.

mod array;

#[macro_use]
pub mod gc;

pub mod value;

use std::{marker::PhantomData, ptr::NonNull};

use array::{DynArray, DynStack};
use gc::{GcPtr, Heap};
use value::{List, ModuleProto, Str, Table};

use crate::{
    codegen::opcodes::*,
    core::RuntimeCoreLib,
    error::{Error, Result, error_span},
    gc::GcAnyPtr,
    module::{self, ImportBinding},
    span::Span,
    tag::TaggedPtr,
    value::module::ImportBindings,
    vm::{
        array::DynStackIterRaw,
        value::{
            Closure, Function, ValueRaw,
            closure::{ClosureProto, UpvalueDescriptor},
            host_function::{Context, HostFunction},
            module::{ImportProto, ModuleRegistry},
        },
    },
};

impl Sp {
    #[inline(always)]
    pub(crate) unsafe fn at(self, r: Reg) -> *mut ValueRaw {
        self.0.offset(r.sz()).as_ptr()
    }

    #[inline(always)]
    pub(crate) unsafe fn ret(self) -> *mut ValueRaw {
        self.0.as_ptr()
    }
}

trait LpIdx {
    fn idx(self) -> isize;
}

impl LpIdx for Lit {
    #[inline(always)]
    fn idx(self) -> isize {
        self.sz()
    }
}

impl LpIdx for Lit8 {
    #[inline(always)]
    fn idx(self) -> isize {
        self.sz()
    }
}

impl Lp {
    #[inline(always)]
    unsafe fn _at(self, r: isize) -> *const ValueRaw {
        self.0.offset(r).as_ptr()
    }

    // note: these use pointer reads to avoid matching with an `unreachable` branch,
    // which for some reason still generat branches
    // TODO: could be made safer with some kind of enum `offset_of`

    #[inline(always)]
    pub(crate) unsafe fn int_or_float_unchecked(self, r: impl LpIdx) -> ValueRaw {
        let v = self._at(r.idx());

        debug_assert!(matches!(&*v, ValueRaw::Int(..) | ValueRaw::Float(..)));
        v.cast::<ValueRaw>().read()
    }

    #[inline(always)]
    pub(crate) unsafe fn int_unchecked(self, r: impl LpIdx) -> i64 {
        let v = self._at(r.idx());

        debug_assert!(matches!(&*v, ValueRaw::Int(..)));
        v.cast::<u64>().add(1).cast::<i64>().read()
    }

    #[inline(always)]
    pub(crate) unsafe fn float_unchecked(self, r: impl LpIdx) -> f64 {
        let v = self._at(r.idx());

        debug_assert!(matches!(&*v, ValueRaw::Float(..)));
        v.cast::<u64>().add(1).cast::<f64>().read()
    }

    #[inline(always)]
    pub(crate) unsafe fn str_unchecked(self, r: impl LpIdx) -> GcPtr<Str> {
        let v = self._at(r.idx());

        debug_assert!(matches!(&*v, ValueRaw::Object(gc) if gc.is::<Str>()));
        v.cast::<u64>().add(1).cast::<GcPtr<Str>>().read()
    }

    #[inline(always)]
    pub(crate) unsafe fn closure_unchecked(self, r: impl LpIdx) -> GcPtr<ClosureProto> {
        let v = self._at(r.idx());

        debug_assert!(matches!(&*v, ValueRaw::Object(gc) if gc.is::<ClosureProto>()));
        v.cast::<u64>().add(1).cast::<GcPtr<ClosureProto>>().read()
    }

    #[inline(always)]
    pub(crate) unsafe fn import_proto_unchecked(self, r: impl LpIdx) -> GcPtr<ImportProto> {
        let v = self._at(r.idx());

        debug_assert!(matches!(&*v, ValueRaw::Object(gc) if gc.is::<ImportProto>()));
        v.cast::<u64>().add(1).cast::<GcPtr<ImportProto>>().read()
    }
}

impl Jt {
    #[inline(always)]
    pub(crate) unsafe fn at(self, insn: Insn) -> OpaqueHandler {
        self.0.offset(insn.tag()).read()
    }
}

impl Ip {
    #[inline]
    unsafe fn offset_from_unsigned(self, other: Self) -> usize {
        (self.0).offset_from_unsigned(other.0)
    }

    #[inline]
    unsafe fn offset(self, n: isize) -> Self {
        Ip((self.0).offset(n as isize))
    }

    #[inline(always)]
    unsafe fn next(self) -> Self {
        Self(self.0.offset(1))
    }

    #[inline(always)]
    unsafe fn get(self) -> Insn {
        self.0.read()
    }
}

#[derive(Clone, Copy)]
#[repr(transparent)]
struct Upvalues(NonNull<ValueRaw>);

impl Upvalues {
    #[inline(always)]
    pub(crate) unsafe fn at(self, idx: Uv) -> *mut ValueRaw {
        self.0.offset(idx.sz()).as_ptr()
    }
}

#[derive(Clone, Copy)]
struct CallFrame {
    callee: TaggedCalleePtr,

    /// Stack base of _this_ frame.
    stack_base: u32,

    /// Address of the next instruction to execute
    /// after returning from _this_ call frame.
    ///
    /// Points into the previous call frame's callee,
    /// if there is one.
    return_addr: u32,
}

impl CallFrame {
    #[inline]
    fn host(callee: GcPtr<HostFunction>, stack_base: u32, return_addr: u32) -> Self {
        Self {
            callee: TaggedCalleePtr::host(callee),
            stack_base,
            return_addr,
        }
    }

    #[inline]
    fn script(callee: GcPtr<Function>, stack_base: u32, return_addr: u32) -> Self {
        Self {
            callee: TaggedCalleePtr::script(callee),
            stack_base,
            return_addr,
        }
    }
}

use tagged_callee_ptr::TaggedCalleePtr;
mod tagged_callee_ptr {
    use super::*;
    use crate::gc::GcBox;

    #[derive(Clone, Copy)]
    #[repr(transparent)]
    pub struct TaggedCalleePtr(TaggedPtr<GcBox<()>>);

    impl TaggedCalleePtr {
        const HOST_TAG: usize = 1;
        const SCRIPT_TAG: usize = 0;

        #[inline]
        pub fn host(callee: GcPtr<HostFunction>) -> Self {
            let v = Self(TaggedPtr::new(
                unsafe { callee.as_any().into_raw_gcbox() },
                Self::HOST_TAG,
            ));
            debug_assert!(v.is_host());
            v
        }

        #[inline]
        pub fn script(callee: GcPtr<Function>) -> Self {
            let v = Self(TaggedPtr::new(
                unsafe { callee.as_any().into_raw_gcbox() },
                Self::SCRIPT_TAG,
            ));
            debug_assert!(v.is_script());
            v
        }

        #[inline]
        pub fn into_host(self) -> Option<GcPtr<HostFunction>> {
            if self.is_host() {
                Some(unsafe { GcAnyPtr::from_raw_gcbox(self.0.ptr()).cast_unchecked() })
            } else {
                None
            }
        }

        #[inline]
        pub fn into_script(self) -> Option<GcPtr<Function>> {
            if self.is_script() {
                Some(unsafe { GcAnyPtr::from_raw_gcbox(self.0.ptr()).cast_unchecked() })
            } else {
                None
            }
        }

        #[inline]
        pub fn is_host(self) -> bool {
            self.0.tag() == Self::HOST_TAG
        }

        #[inline]
        pub fn is_script(self) -> bool {
            self.0.tag() == Self::SCRIPT_TAG
        }
    }
}

#[derive(Clone, Copy)]
struct CallFramePtr(*mut CallFrame);

impl CallFramePtr {
    #[inline]
    unsafe fn raw(self) -> *mut CallFrame {
        self.0
    }

    #[inline]
    unsafe fn callee(self) -> TaggedCalleePtr {
        (*self.0).callee
    }

    #[inline]
    unsafe fn name(self) -> GcPtr<Str> {
        if let Some(callee) = (*self.0).callee.into_script() {
            callee.as_ref().name
        } else if let Some(callee) = (*self.0).callee.into_host() {
            callee.as_ref().name
        } else {
            unreachable!()
        }
    }

    #[inline]
    unsafe fn stack_size(self) -> u8 {
        if let Some(callee) = (*self.0).callee.into_script() {
            callee.as_ref().nstack
        } else if let Some(callee) = (*self.0).callee.into_host() {
            1 + callee.as_ref().arity
        } else {
            unreachable!()
        }
    }

    #[inline]
    unsafe fn stack_base(self) -> u32 {
        (*self.0).stack_base
    }

    #[inline]
    unsafe fn return_addr(self) -> u32 {
        (*self.0).return_addr
    }
}

impl Vm {
    #[inline]
    unsafe fn previous_frame(self) -> CallFramePtr {
        let frames = &raw mut (*self.0.as_ptr()).frames;
        let n = (*frames).len() - 2;
        let ptr = (*frames).at_unchecked(n);
        CallFramePtr(ptr)
    }

    #[inline]
    unsafe fn current_frame(self) -> CallFramePtr {
        let ptr = (*self.0.as_ptr()).frames.top_unchecked();
        CallFramePtr(ptr)
    }

    #[inline]
    unsafe fn push_frame(self, frame: CallFrame) {
        let frames = &mut (*self.0.as_ptr()).frames;
        frames.push(frame);
    }

    #[inline]
    unsafe fn pop_frame_unchecked(self) -> CallFrame {
        let frames = &mut (*self.0.as_ptr()).frames;
        frames.pop_unchecked()
    }

    #[inline]
    unsafe fn has_enough_stack_space(self, stack_base: usize, frame_size: usize) -> bool {
        let stack = &mut (*self.0.as_ptr()).stack;
        let remaining = stack.remaining(stack_base);
        remaining >= (frame_size as isize)
    }

    #[inline]
    unsafe fn stack_at(self, stack_base: usize) -> Sp {
        let stack = &mut (*self.0.as_ptr()).stack;
        Sp(NonNull::new_unchecked(stack.offset(stack_base)))
    }

    #[inline]
    unsafe fn grow_stack(self, additional: usize) {
        let stack = &mut (*self.0.as_ptr()).stack;
        stack.grow(additional)
    }

    #[inline]
    unsafe fn get_function_in_current_module(self, id: FnId) -> GcPtr<Function> {
        self.current_module()
            .as_ref()
            .get_function_unchecked(id)
            .as_ptr()
    }

    #[inline]
    unsafe fn var_in_current_module(self, idx: Mvar) -> *mut ValueRaw {
        self.current_module()
            .as_mut()
            .module_vars
            .as_mut_ptr()
            .offset(idx.sz())
    }

    #[inline]
    unsafe fn current_module(self) -> GcPtr<ModuleProto> {
        let module = (*self.0.as_ptr()).current_module;
        debug_assert!(module.is_some());
        module.unwrap_unchecked()
    }

    #[inline]
    unsafe fn set_current_module_for(self, f: GcPtr<Function>) {
        let module = f.as_ref().module().as_ptr();
        (*self.0.as_ptr()).current_module = Some(module);
    }

    #[inline]
    unsafe fn get_host_function(self, id: HostId) -> GcPtr<HostFunction> {
        (*self.0.as_ptr())
            .core
            .functions
            .as_mut_ptr()
            .offset(id.sz())
            .read()
    }

    #[inline]
    unsafe fn current_upvalues(self) -> Upvalues {
        let upvalues = (*self.0.as_ptr()).current_upvalues;
        debug_assert!(upvalues.is_some());
        upvalues.unwrap_unchecked()
    }

    #[inline]
    unsafe fn set_current_upvalues_for(self, f: GcPtr<Closure>) {
        let upvalues = NonNull::new_unchecked(f.as_mut().upvalues.as_mut_ptr());
        (*self.0.as_ptr()).current_upvalues = Some(Upvalues(upvalues));
    }

    #[inline]
    unsafe fn write_error(self, error: Error) {
        (*self.0.as_ptr()).error = Some(error);
    }

    #[inline]
    unsafe fn take_error(mut self, err: VmError) -> Error {
        if err.is_host() {
            self.0.as_mut().error.take().unwrap().annotate(self)
        } else {
            err.annotate(self)
        }
    }

    #[inline]
    unsafe fn call_frames(self) -> DynStackIterRaw<CallFrame> {
        DynStack::iter_raw(&raw const (*self.0.as_ptr()).frames)
    }

    // TODO: nicer error with stack trace
    #[inline]
    unsafe fn unwind(mut self) {
        // The VM may have multiple host frames interleaved with
        // script frames. Only unwind script frames, and let the
        // host handle the error and pop its own frame.
        let mut new_len = self.0.as_ref().frames.len();
        for frame in self.call_frames().rev() {
            let frame = CallFramePtr(frame);
            if frame.callee().is_host() {
                break;
            }
            new_len -= 1;
        }

        #[cfg(debug_assertions)]
        if std::env::var("PRINT_STACK_TRACE").is_ok() {
            eprintln!("===== begin trace ======");
            for frame in self.call_frames().rev() {
                let frame = CallFramePtr(frame);
                let stack_size = frame.stack_size();
                let stack_base = frame.stack_base();

                let stack = self.stack_at(stack_base as usize);
                eprintln!(
                    "[{name} ({kind})] stack: ",
                    name = frame.name().as_ref().as_str(),
                    kind = if frame.callee().is_host() {
                        "host"
                    } else {
                        "script"
                    }
                );
                for i in (0..stack_size).rev() {
                    let r = unsafe { Reg::new_unchecked(i) };
                    eprintln!(
                        "{r} ({}) {:?}",
                        stack_base + r.zx() as u32,
                        (*stack.at(r)).as_ref()
                    );
                }
                eprintln!();
            }
            eprintln!("=====  end trace =====");
        }

        // At least the VM start frame should remain.
        debug_assert!(new_len > 0);
        self.0.as_mut().frames.set_length(new_len);
    }

    #[inline]
    unsafe fn get_span(self, ip: Ip) -> Option<Span> {
        let callee = {
            let mut first_non_host_callee = None;
            for frame in self.call_frames() {
                let frame = CallFramePtr(frame);
                if let Some(callee) = frame.callee().into_script() {
                    first_non_host_callee = Some(callee);
                    break;
                }
            }
            first_non_host_callee?
        };

        let pc = ip.offset_from_unsigned(Function::code_raw(callee));
        match callee.as_ref().dbg() {
            Some(dbg) => dbg.spans.get(pc).copied(),
            None => None,
        }
    }

    #[inline]
    unsafe fn set_saved_ip(self, ip: Ip) {
        let ptr = &raw mut (*self.0.as_ptr()).saved_ip;
        ptr.write(Some(ip));
    }

    #[inline]
    unsafe fn saved_ip(self) -> Option<Ip> {
        (*self.0.as_ptr()).saved_ip
    }

    #[inline]
    unsafe fn get_span_for_saved_ip(self) -> Option<Span> {
        let ip = self.saved_ip()?;
        self.get_span(ip)
    }

    #[inline]
    unsafe fn heap(self) -> *mut Heap {
        &raw mut (*self.0.as_ptr()).heap
    }

    #[inline]
    unsafe fn stdio(self) -> *mut Stdio {
        &raw mut (*self.0.as_ptr()).stdio
    }

    #[inline]
    unsafe fn entrypoint(self) -> GcPtr<HostFunction> {
        (*self.0.as_ptr()).entrypoint
    }

    #[inline]
    #[cfg_attr(debug_assertions, track_caller)]
    unsafe fn maybe_gc(self) {
        // TODO: GC thresholds
        // for now, we always GC

        self.full_gc();
    }

    #[cold]
    #[cfg_attr(debug_assertions, track_caller)]
    unsafe fn full_gc(self) {
        let heap = &raw mut (*self.0.as_ptr()).heap;
        let roots = VmRoots { state: self.0 };
        Heap::collect_with_external_roots(heap, roots);
    }
}

#[derive(Clone, Copy)]
#[repr(u8)]
pub enum VmError {
    DivisionByZero,
    ArithTypeError,
    UnopInvalidType,
    CmpTypeError,
    NotAList,
    NotAString,
    InvalidArrayIndex,
    IndexOutOfBounds,
    NotATable,
    InvalidTableKey,
    MissingKey,
    NotIndexable,
    ArityMismatch,
    NotCallable,
    ModuleNotFound,
    ModuleItemNotFound,

    Host,
}

impl Error {
    unsafe fn annotate(self, vm: Vm) -> Self {
        let span = vm.get_span_for_saved_ip().unwrap_or_default();
        self.with_span(span)
    }
}

impl VmError {
    // TODO: additional error context, somehow?
    //       for example, try reading the current `ip`, reading the operands,
    //       and checking values and other things. assuming that failures
    //       happen before any writes.
    //       or maybe there's a smarter way to transfer context, without
    //       too much overhead.
    // NOTE: assumes `vm.saved_ip` is set
    unsafe fn annotate(&self, vm: Vm) -> Error {
        let span = vm.get_span_for_saved_ip().unwrap_or_default();
        match self {
            Self::DivisionByZero => error_span("division by zero", span),
            Self::ArithTypeError => error_span("type mismatch", span),
            Self::UnopInvalidType => error_span("invalid type", span),
            Self::CmpTypeError => error_span("type mismatch", span),
            Self::NotAList => error_span("not a list", span),
            Self::NotAString => error_span("not a string", span),
            Self::InvalidArrayIndex => error_span("value is not a valid array index", span),
            Self::IndexOutOfBounds => error_span("index out of bounds", span),
            Self::NotATable => error_span("not a table", span),
            Self::InvalidTableKey => error_span("value is not a valid table key", span),
            Self::MissingKey => error_span("missing key", span),
            Self::NotIndexable => error_span("this value cannot be indexed", span),
            Self::ArityMismatch => error_span("invalid number of arguments", span),
            Self::NotCallable => error_span("this value cannot be called", span),
            Self::ModuleNotFound => error_span("module with this name does not exist", span),
            Self::ModuleItemNotFound => error_span("module has no such item", span),

            Self::Host => error_span("<host error>", span),
        }
    }

    #[inline]
    fn is_host(self) -> bool {
        match self {
            Self::Host => true,

            Self::DivisionByZero
            | Self::ArithTypeError
            | Self::UnopInvalidType
            | Self::CmpTypeError
            | Self::NotAList
            | Self::NotAString
            | Self::InvalidArrayIndex
            | Self::IndexOutOfBounds
            | Self::NotATable
            | Self::InvalidTableKey
            | Self::MissingKey
            | Self::NotIndexable
            | Self::ArityMismatch
            | Self::NotCallable
            | Self::ModuleNotFound
            | Self::ModuleItemNotFound => false,
        }
    }
}

macro_rules! vm_exit {
    ($vm:ident, $ip:ident, $error:ident) => {{
        $vm.set_saved_ip($ip);
        return Control::error(VmError::$error);
    }};
}

static JT: JumpTable = jump_table! {
    nop,
    mov,

    lmvar,
    smvar,
    luv,
    suv,
    lidx,
    lidxn,
    sidx,
    sidxn,
    lkey,
    lkeyc,
    skey,
    skeyc,

    lnil,
    lsmi,
    ltrue,
    lfalse,
    lint,
    lnum,
    lstr,
    lclosure,
    lfunc,
    lhost,
    llist,
    ltable,
    jmp,
    islt,
    isle,
    isgt,
    isge,
    iseq,
    isne,
    isnil,
    isnotnil,
    istrue,
    isfalse,
    iseqs,
    isnes,
    iseqi,
    isnei,
    iseqf,
    isnef,
    isltv,
    islev,
    isgtv,
    isgev,
    iseqv,
    isnev,
    addvv,
    addvn,
    addnv,
    subvv,
    subvn,
    subnv,
    mulvv,
    mulvn,
    mulnv,
    divvv,
    divvn,
    divnv,
    unm,
    not,
    call,
    fastcall,
    hostcall,
    import,
    ret,
    retv,
    stop,
};

#[cfg(not(any(debug_assertions, target_arch = "wasm32")))]
#[derive(Debug, Clone, Copy)]
#[repr(transparent)]
pub(crate) struct Control(u8);

#[cfg(not(any(debug_assertions, target_arch = "wasm32")))]
impl Control {
    #[inline]
    fn stop() -> Control {
        Control(0)
    }

    #[inline]
    fn error(code: VmError) -> Control {
        Control(1 | ((code as u8) << 1))
    }

    #[inline]
    fn is_stop(self) -> bool {
        self.0 == 0
    }

    #[inline]
    fn is_error(self) -> bool {
        self.0 & 1 == 1
    }

    #[inline]
    unsafe fn error_code(self) -> VmError {
        core::mem::transmute(self.0 >> 1)
    }
}

#[cfg(any(debug_assertions, target_arch = "wasm32"))]
#[derive(Clone, Copy)]
#[repr(C)]
pub(crate) enum Control {
    Stop,
    Error(VmError),
    Continue(Sp, Lp, Ip),
}

#[cfg(any(debug_assertions, target_arch = "wasm32"))]
impl Control {
    #[inline]
    fn stop() -> Control {
        Control::Stop
    }

    #[inline]
    fn error(code: VmError) -> Control {
        Control::Error(code)
    }

    #[inline]
    fn is_error(self) -> bool {
        if let Self::Error(code) = self {
            true
        } else {
            false
        }
    }
}

/// Dispatch instruction at `ip`
#[inline(always)]
unsafe fn dispatch_current(vm: Vm, jt: Jt, ip: Ip, sp: Sp, lp: Lp) -> Control {
    #[cfg(any(debug_assertions, target_arch = "wasm32"))]
    {
        Control::Continue(sp, lp, ip)
    }

    #[cfg(not(any(debug_assertions, target_arch = "wasm32")))]
    {
        let insn = ip.get();
        let op = jt.at(insn);
        op(vm, jt, ip, insn, sp, lp)
    }
}

/// Dispatch instruction at `ip+1`
#[inline(always)]
unsafe fn dispatch_next(vm: Vm, jt: Jt, ip: Ip, sp: Sp, lp: Lp) -> Control {
    let ip = ip.next();
    dispatch_current(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn nop(vm: Vm, jt: Jt, ip: Ip, args: Nop, sp: Sp, lp: Lp) -> Control {
    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn mov(vm: Vm, jt: Jt, ip: Ip, args: Mov, sp: Sp, lp: Lp) -> Control {
    *sp.at(args.dst()) = *sp.at(args.src());

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn lmvar(vm: Vm, jt: Jt, ip: Ip, args: Lmvar, sp: Sp, lp: Lp) -> Control {
    *sp.at(args.dst()) = *vm.var_in_current_module(args.src());

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn smvar(vm: Vm, jt: Jt, ip: Ip, args: Smvar, sp: Sp, lp: Lp) -> Control {
    *vm.var_in_current_module(args.dst()) = *sp.at(args.src());

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn luv(vm: Vm, jt: Jt, ip: Ip, args: Luv, sp: Sp, lp: Lp) -> Control {
    *sp.at(args.dst()) = *vm.current_upvalues().at(args.src());

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn suv(vm: Vm, jt: Jt, ip: Ip, args: Suv, sp: Sp, lp: Lp) -> Control {
    *vm.current_upvalues().at(args.dst()) = *sp.at(args.src());

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn lidx(vm: Vm, jt: Jt, ip: Ip, args: Lidx, sp: Sp, lp: Lp) -> Control {
    let dst = sp.at(args.dst());
    let target = *sp.at(args.target());
    let idx = *sp.at(args.idx());

    if let Some(list) = target.into_object::<List>() {
        let ValueRaw::Int(idx) = idx else {
            return invalid_array_index_error(ip, vm);
        };
        let idx = idx as usize;

        // UNROOTED: reachable through the stack
        let Some(value) = list.as_ref().get(idx) else {
            return index_out_of_bounds_error(ip, vm);
        };

        *dst = value.raw();
    } else if let Some(table) = target.into_object::<Table>() {
        let Some(key) = idx.into_object::<Str>() else {
            return invalid_table_key_error(ip, vm);
        };

        // UNROOTED: reachable through the stack
        // TODO: inline caching
        let Some(value) = table.as_ref().get(key.as_ref()) else {
            return missing_key_error(ip, vm);
        };

        *dst = value.raw();
    } else {
        return not_indexable_error(ip, vm);
    };

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn lidxn(vm: Vm, jt: Jt, ip: Ip, args: Lidxn, sp: Sp, lp: Lp) -> Control {
    let dst = sp.at(args.dst());
    let list = *sp.at(args.target());
    let idx = lp.int_unchecked(args.idx()) as usize;

    let Some(list) = list.into_object::<List>() else {
        return not_a_list_error(ip, vm);
    };

    // UNROOTED: reachable through the stack
    let Some(value) = list.as_ref().get(idx) else {
        return index_out_of_bounds_error(ip, vm);
    };

    *dst = value.raw();

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn sidx(vm: Vm, jt: Jt, ip: Ip, args: Sidx, sp: Sp, lp: Lp) -> Control {
    let target = *sp.at(args.target());
    let idx = *sp.at(args.idx());
    let src = *sp.at(args.src());

    if let Some(list) = target.into_object::<List>() {
        let ValueRaw::Int(idx) = idx else {
            return invalid_array_index_error(ip, vm);
        };
        let idx = idx as usize;

        // UNROOTED: reachable through the stack
        let len = list.as_ref().len();
        if idx >= len {
            return index_out_of_bounds_error(ip, vm);
        }

        list.as_mut().set_raw_unchecked(idx, src);
    } else if let Some(table) = target.into_object::<Table>() {
        let Some(key) = idx.into_object::<Str>() else {
            return invalid_table_key_error(ip, vm);
        };

        // UNROOTED: reachable through the stack
        // TODO: inline caching
        table.as_mut().insert_raw(key, src);
    } else {
        return not_indexable_error(ip, vm);
    };

    dispatch_next(vm, jt, ip, sp, lp)
}

#[cold]
unsafe fn not_indexable_error(ip: Ip, vm: Vm) -> Control {
    vm_exit!(vm, ip, NotIndexable);
}

#[cold]
unsafe fn invalid_array_index_error(ip: Ip, vm: Vm) -> Control {
    vm_exit!(vm, ip, InvalidArrayIndex);
}

#[inline(always)]
unsafe fn sidxn(vm: Vm, jt: Jt, ip: Ip, args: Sidxn, sp: Sp, lp: Lp) -> Control {
    let list = *sp.at(args.target());
    let idx = lp.int_unchecked(args.idx()) as usize;
    let src = *sp.at(args.src());

    let Some(list) = list.into_object::<List>() else {
        return not_a_list_error(ip, vm);
    };

    // UNROOTED: reachable through the stack
    let len = list.as_ref().len();
    if idx >= len {
        return index_out_of_bounds_error(ip, vm);
    }

    list.as_mut().set_raw_unchecked(idx, src);

    dispatch_next(vm, jt, ip, sp, lp)
}

#[cold]
unsafe fn not_a_list_error(ip: Ip, vm: Vm) -> Control {
    vm_exit!(vm, ip, NotAList);
}

#[cold]
unsafe fn index_out_of_bounds_error(ip: Ip, vm: Vm) -> Control {
    vm_exit!(vm, ip, IndexOutOfBounds);
}

#[inline(always)]
unsafe fn lkey(vm: Vm, jt: Jt, ip: Ip, args: Lkey, sp: Sp, lp: Lp) -> Control {
    let dst = sp.at(args.dst());
    let target = *sp.at(args.target());
    let key = *sp.at(args.key());

    let Some(table) = target.into_object::<Table>() else {
        return not_a_table_error(ip, vm);
    };

    let Some(key) = key.into_object::<Str>() else {
        return invalid_table_key_error(ip, vm);
    };

    // UNROOTED: reachable through the stack
    // TODO: inline caching
    let Some(value) = table.as_ref().get(key.as_ref()) else {
        return missing_key_error(ip, vm);
    };

    *dst = value.raw();

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn lkeyc(vm: Vm, jt: Jt, ip: Ip, args: Lkeyc, sp: Sp, lp: Lp) -> Control {
    let dst = sp.at(args.dst());
    let target = *sp.at(args.target());
    let key = lp.str_unchecked(args.key());

    let Some(table) = target.into_object::<Table>() else {
        return not_a_table_error(ip, vm);
    };

    // UNROOTED: reachable through the stack
    // TODO: inline caching
    let Some(value) = table.as_ref().get_raw(key) else {
        return missing_key_error(ip, vm);
    };

    *dst = value;

    dispatch_next(vm, jt, ip, sp, lp)
}

#[cold]
unsafe fn missing_key_error(ip: Ip, vm: Vm) -> Control {
    vm_exit!(vm, ip, MissingKey);
}

#[inline(always)]
unsafe fn skey(vm: Vm, jt: Jt, ip: Ip, args: Skey, sp: Sp, lp: Lp) -> Control {
    let target = *sp.at(args.target());
    let key = *sp.at(args.key());
    let src = *sp.at(args.src());

    let Some(table) = target.into_object::<Table>() else {
        return not_a_table_error(ip, vm);
    };

    let Some(key) = key.into_object::<Str>() else {
        return invalid_table_key_error(ip, vm);
    };

    // UNROOTED: reachable through the stack
    // TODO: inline caching
    table.as_mut().insert_raw(key, src);

    dispatch_next(vm, jt, ip, sp, lp)
}

#[cold]
unsafe fn not_a_table_error(ip: Ip, vm: Vm) -> Control {
    vm_exit!(vm, ip, NotATable);
}

#[cold]
unsafe fn invalid_table_key_error(ip: Ip, vm: Vm) -> Control {
    vm_exit!(vm, ip, InvalidTableKey);
}

#[inline(always)]
unsafe fn skeyc(vm: Vm, jt: Jt, ip: Ip, args: Skeyc, sp: Sp, lp: Lp) -> Control {
    let target = *sp.at(args.target());
    let key = lp.str_unchecked(args.key());
    let src = *sp.at(args.src());

    let Some(table) = target.into_object::<Table>() else {
        return not_a_table_error(ip, vm);
    };

    // UNROOTED: reachable through the stack
    // TODO: inline caching
    table.as_mut().insert_raw(key, src);

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn lnil(vm: Vm, jt: Jt, ip: Ip, args: Lnil, sp: Sp, lp: Lp) -> Control {
    *sp.at(args.dst()) = ValueRaw::Nil;

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn lsmi(vm: Vm, jt: Jt, ip: Ip, args: Lsmi, sp: Sp, lp: Lp) -> Control {
    *sp.at(args.dst()) = ValueRaw::Int(args.v().get() as i64);

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn lint(vm: Vm, jt: Jt, ip: Ip, args: Lint, sp: Sp, lp: Lp) -> Control {
    *sp.at(args.dst()) = ValueRaw::Int(lp.int_unchecked(args.id()));

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn lnum(vm: Vm, jt: Jt, ip: Ip, args: Lnum, sp: Sp, lp: Lp) -> Control {
    *sp.at(args.dst()) = ValueRaw::Float(lp.float_unchecked(args.id()));

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn lstr(vm: Vm, jt: Jt, ip: Ip, args: Lstr, sp: Sp, lp: Lp) -> Control {
    *sp.at(args.dst()) = ValueRaw::Object(lp.str_unchecked(args.id()).as_any());

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn ltrue(vm: Vm, jt: Jt, ip: Ip, args: Ltrue, sp: Sp, lp: Lp) -> Control {
    *sp.at(args.dst()) = ValueRaw::Bool(true);

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn lfalse(vm: Vm, jt: Jt, ip: Ip, args: Lfalse, sp: Sp, lp: Lp) -> Control {
    *sp.at(args.dst()) = ValueRaw::Bool(false);

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn lclosure(vm: Vm, jt: Jt, ip: Ip, args: Lclosure, sp: Sp, lp: Lp) -> Control {
    vm.maybe_gc();

    let dst = sp.at(args.dst());
    let proto = lp.closure_unchecked(args.id());

    let heap = vm.heap();
    let closure = Closure::alloc(&*heap, proto);

    {
        let nuv = proto.as_ref().upvalues.len();
        for i in 0..nuv {
            let value = match *proto.as_ref().upvalues.get_unchecked(i) {
                UpvalueDescriptor::Rec => ValueRaw::Object(closure.as_any()),
                UpvalueDescriptor::Reg(reg) => *sp.at(reg),
                UpvalueDescriptor::Uv(uv) => *vm.current_upvalues().at(uv),
            };
            *closure.as_mut().upvalues.get_unchecked_mut(i) = value;
        }
    }

    *dst = ValueRaw::Object(closure.as_any());

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn lfunc(vm: Vm, jt: Jt, ip: Ip, args: Lfunc, sp: Sp, lp: Lp) -> Control {
    let dst = sp.at(args.dst());
    let func = vm.get_function_in_current_module(args.id());

    *dst = ValueRaw::Object(func.as_any());

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn lhost(vm: Vm, jt: Jt, ip: Ip, args: Lhost, sp: Sp, lp: Lp) -> Control {
    let dst = sp.at(args.dst());
    let func = vm.get_host_function(args.id());

    *dst = ValueRaw::Object(func.as_any());

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn llist(vm: Vm, jt: Jt, ip: Ip, args: Llist, sp: Sp, lp: Lp) -> Control {
    vm.maybe_gc();

    let dst = sp.at(args.dst());
    let len = args.cap().zx();

    // UNROOTED: immediately written to the stack
    let heap = vm.heap();
    let list = List::alloc_zeroed(&*heap, len);

    *dst = ValueRaw::Object(list.as_any());

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn ltable(vm: Vm, jt: Jt, ip: Ip, args: Ltable, sp: Sp, lp: Lp) -> Control {
    vm.maybe_gc();

    let dst = sp.at(args.dst());
    let len = args.cap().zx();

    // UNROOTED: immediately written to the stack
    let heap = vm.heap();
    let table = Table::alloc(&*heap, len);

    *dst = ValueRaw::Object(table.as_any());

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn jmp(vm: Vm, jt: Jt, ip: Ip, args: Jmp, sp: Sp, lp: Lp) -> Control {
    let ip = ip.offset(args.rel().sz());

    dispatch_current(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn isnil(vm: Vm, jt: Jt, ip: Ip, args: Isnil, sp: Sp, lp: Lp) -> Control {
    // isnil v
    // jmp offset

    let v = *sp.at(args.v());
    if matches!(v, ValueRaw::Nil) {
        // skip `jmp`
        let ip = ip.offset(2);
        dispatch_current(vm, jt, ip, sp, lp)
    } else {
        // execute `jmp`
        let ip = ip.offset(1);
        dispatch_current(vm, jt, ip, sp, lp)
    }
}

#[inline(always)]
unsafe fn isnotnil(vm: Vm, jt: Jt, ip: Ip, args: Isnotnil, sp: Sp, lp: Lp) -> Control {
    // isnotnil v
    // jmp offset

    let v = *sp.at(args.v());
    if !matches!(v, ValueRaw::Nil) {
        // skip `jmp`
        let ip = ip.offset(2);
        dispatch_current(vm, jt, ip, sp, lp)
    } else {
        // execute `jmp`
        let ip = ip.offset(1);
        dispatch_current(vm, jt, ip, sp, lp)
    }
}

#[inline(always)]
unsafe fn istrue(vm: Vm, jt: Jt, ip: Ip, args: Istrue, sp: Sp, lp: Lp) -> Control {
    // istrue v
    // jmp offset

    let v = *sp.at(args.v());
    if v.coerce_bool() {
        // skip `jmp`
        let ip = ip.offset(2);
        dispatch_current(vm, jt, ip, sp, lp)
    } else {
        // execute `jmp`
        let ip = ip.offset(1);
        dispatch_current(vm, jt, ip, sp, lp)
    }
}

#[inline(always)]
unsafe fn isfalse(vm: Vm, jt: Jt, ip: Ip, args: Isfalse, sp: Sp, lp: Lp) -> Control {
    // isfalse v
    // jmp offset

    let v = *sp.at(args.v());
    if !v.coerce_bool() {
        // skip `jmp`
        let ip = ip.offset(2);
        dispatch_current(vm, jt, ip, sp, lp)
    } else {
        // execute `jmp`
        let ip = ip.offset(1);
        dispatch_current(vm, jt, ip, sp, lp)
    }
}

#[inline(always)]
unsafe fn iseqs(vm: Vm, jt: Jt, ip: Ip, args: Iseqs, sp: Sp, lp: Lp) -> Control {
    // iseqs v
    // jmp offset

    // TODO: string interning
    let lhs = *sp.at(args.lhs());
    let rhs = lp.str_unchecked(args.rhs());

    let Some(lhs) = lhs.into_object::<Str>() else {
        // execute `jmp`
        let ip = ip.offset(1);
        return dispatch_current(vm, jt, ip, sp, lp);
    };

    let lhs = lhs.as_ref();
    let lhs = lhs.as_str();

    let rhs = rhs.as_ref();
    let rhs = rhs.as_str();

    if lhs == rhs {
        // skip `jmp`
        let ip = ip.offset(2);
        dispatch_current(vm, jt, ip, sp, lp)
    } else {
        // execute `jmp`
        let ip = ip.offset(1);
        dispatch_current(vm, jt, ip, sp, lp)
    }
}

#[inline(always)]
unsafe fn isnes(vm: Vm, jt: Jt, ip: Ip, args: Isnes, sp: Sp, lp: Lp) -> Control {
    // isnes v
    // jmp offset

    // TODO: string interning
    let lhs = *sp.at(args.lhs());
    let rhs = lp.str_unchecked(args.rhs());

    let Some(lhs) = lhs.into_object::<Str>() else {
        // skip `jmp`
        let ip = ip.offset(2);
        return dispatch_current(vm, jt, ip, sp, lp);
    };

    let lhs = lhs.as_ref();
    let lhs = lhs.as_str();

    let rhs = rhs.as_ref();
    let rhs = rhs.as_str();

    if lhs != rhs {
        // skip `jmp`
        let ip = ip.offset(2);
        dispatch_current(vm, jt, ip, sp, lp)
    } else {
        // execute `jmp`
        let ip = ip.offset(1);
        dispatch_current(vm, jt, ip, sp, lp)
    }
}

#[inline(always)]
unsafe fn iseqi(vm: Vm, jt: Jt, ip: Ip, args: Iseqi, sp: Sp, lp: Lp) -> Control {
    // iseqi v
    // jmp offset

    let lhs = *sp.at(args.lhs());
    let rhs = lp.int_unchecked(args.rhs());

    let lhs = match lhs {
        ValueRaw::Int(v) => v,
        ValueRaw::Float(v) => v as i64,
        _ => {
            // execute `jmp`
            let ip = ip.offset(1);
            return dispatch_current(vm, jt, ip, sp, lp);
        }
    };

    if lhs == rhs {
        // skip `jmp`
        let ip = ip.offset(2);
        dispatch_current(vm, jt, ip, sp, lp)
    } else {
        // execute `jmp`
        let ip = ip.offset(1);
        dispatch_current(vm, jt, ip, sp, lp)
    }
}

#[inline(always)]
unsafe fn isnei(vm: Vm, jt: Jt, ip: Ip, args: Isnei, sp: Sp, lp: Lp) -> Control {
    // isnei v
    // jmp offset

    let lhs = *sp.at(args.lhs());
    let rhs = lp.int_unchecked(args.rhs());

    let lhs = match lhs {
        ValueRaw::Int(v) => v,
        ValueRaw::Float(v) => v as i64,
        _ => {
            // skip `jmp`
            let ip = ip.offset(2);
            return dispatch_current(vm, jt, ip, sp, lp);
        }
    };

    if lhs != rhs {
        // skip `jmp`
        let ip = ip.offset(2);
        dispatch_current(vm, jt, ip, sp, lp)
    } else {
        // execute `jmp`
        let ip = ip.offset(1);
        dispatch_current(vm, jt, ip, sp, lp)
    }
}

#[inline(always)]
unsafe fn iseqf(vm: Vm, jt: Jt, ip: Ip, args: Iseqf, sp: Sp, lp: Lp) -> Control {
    // iseqf v
    // jmp offset

    let lhs = *sp.at(args.lhs());
    let rhs = lp.float_unchecked(args.rhs());

    let lhs = match lhs {
        ValueRaw::Int(v) => v as f64,
        ValueRaw::Float(v) => v,
        _ => {
            // execute `jmp`
            let ip = ip.offset(1);
            return dispatch_current(vm, jt, ip, sp, lp);
        }
    };

    if lhs == rhs {
        // skip `jmp`
        let ip = ip.offset(2);
        dispatch_current(vm, jt, ip, sp, lp)
    } else {
        // execute `jmp`
        let ip = ip.offset(1);
        dispatch_current(vm, jt, ip, sp, lp)
    }
}

#[inline(always)]
unsafe fn isnef(vm: Vm, jt: Jt, ip: Ip, args: Isnef, sp: Sp, lp: Lp) -> Control {
    // isnef v
    // jmp offset

    let lhs = *sp.at(args.lhs());
    let rhs = lp.float_unchecked(args.rhs());

    let lhs = match lhs {
        ValueRaw::Int(v) => v as f64,
        ValueRaw::Float(v) => v,
        _ => {
            // skip `jmp`
            let ip = ip.offset(2);
            return dispatch_current(vm, jt, ip, sp, lp);
        }
    };

    if lhs != rhs {
        // skip `jmp`
        let ip = ip.offset(2);
        dispatch_current(vm, jt, ip, sp, lp)
    } else {
        // execute `jmp`
        let ip = ip.offset(1);
        dispatch_current(vm, jt, ip, sp, lp)
    }
}

macro_rules! try_compare {
    ($lhs:ident, $rhs:ident, $vm:ident, $ip:ident, $op:tt, fail:$fail:expr) => {{
        use ValueRaw::*;
        match ($lhs, $rhs) {
            (Nil, Nil) => () $op (),
            (Bool(lhs), Bool(rhs)) => lhs $op rhs,
            (Int(lhs), Int(rhs)) => lhs $op rhs,
            (Float(lhs), Float(rhs)) => lhs $op rhs,
            (Float(lhs), Int(rhs)) => lhs $op (rhs as f64),
            (Int(lhs), Float(rhs)) => (lhs as f64) $op rhs,
            (Object(lhs), Object(rhs)) => {
                // TODO: string interning
                if let Some(lhs) = lhs.cast::<Str>()
                && let Some(rhs) = rhs.cast::<Str>() {
                    lhs.as_ref().as_str() $op rhs.as_ref().as_str()
                } else {
                    if $fail {
                        vm_exit!($vm, $ip, CmpTypeError)
                    } else {
                        false
                    }
                }
            }
            _ => if $fail {
                vm_exit!($vm, $ip, CmpTypeError)
            } else {
                false
            },
        }
    }};
}

#[inline(always)]
unsafe fn islt(vm: Vm, jt: Jt, ip: Ip, args: Islt, sp: Sp, lp: Lp) -> Control {
    let lhs = *sp.at(args.lhs());
    let rhs = *sp.at(args.rhs());

    if try_compare!(lhs, rhs, vm, ip, <, fail:true) {
        let ip = ip.offset(2);
        dispatch_current(vm, jt, ip, sp, lp)
    } else {
        let ip = ip.offset(1);
        dispatch_current(vm, jt, ip, sp, lp)
    }
}

#[inline(always)]
unsafe fn isle(vm: Vm, jt: Jt, ip: Ip, args: Isle, sp: Sp, lp: Lp) -> Control {
    let lhs = *sp.at(args.lhs());
    let rhs = *sp.at(args.rhs());

    if try_compare!(lhs, rhs, vm, ip, <=, fail:true) {
        let ip = ip.offset(2);
        dispatch_current(vm, jt, ip, sp, lp)
    } else {
        let ip = ip.offset(1);
        dispatch_current(vm, jt, ip, sp, lp)
    }
}

#[inline(always)]
unsafe fn isgt(vm: Vm, jt: Jt, ip: Ip, args: Isgt, sp: Sp, lp: Lp) -> Control {
    let lhs = *sp.at(args.lhs());
    let rhs = *sp.at(args.rhs());

    if try_compare!(lhs, rhs, vm, ip, >, fail:true) {
        let ip = ip.offset(2);
        dispatch_current(vm, jt, ip, sp, lp)
    } else {
        let ip = ip.offset(1);
        dispatch_current(vm, jt, ip, sp, lp)
    }
}

#[inline(always)]
unsafe fn isge(vm: Vm, jt: Jt, ip: Ip, args: Isge, sp: Sp, lp: Lp) -> Control {
    let lhs = *sp.at(args.lhs());
    let rhs = *sp.at(args.rhs());

    if try_compare!(lhs, rhs, vm, ip, >=, fail:true) {
        let ip = ip.offset(2);
        dispatch_current(vm, jt, ip, sp, lp)
    } else {
        let ip = ip.offset(1);
        dispatch_current(vm, jt, ip, sp, lp)
    }
}

#[inline(always)]
unsafe fn iseq(vm: Vm, jt: Jt, ip: Ip, args: Iseq, sp: Sp, lp: Lp) -> Control {
    let lhs = *sp.at(args.lhs());
    let rhs = *sp.at(args.rhs());

    if try_compare!(lhs, rhs, vm, ip, ==, fail:false) {
        let ip = ip.offset(2);
        dispatch_current(vm, jt, ip, sp, lp)
    } else {
        let ip = ip.offset(1);
        dispatch_current(vm, jt, ip, sp, lp)
    }
}

#[inline(always)]
unsafe fn isne(vm: Vm, jt: Jt, ip: Ip, args: Isne, sp: Sp, lp: Lp) -> Control {
    let lhs = *sp.at(args.lhs());
    let rhs = *sp.at(args.rhs());

    if try_compare!(lhs, rhs, vm, ip, !=, fail:false) {
        let ip = ip.offset(2);
        dispatch_current(vm, jt, ip, sp, lp)
    } else {
        let ip = ip.offset(1);
        dispatch_current(vm, jt, ip, sp, lp)
    }
}

#[inline(always)]
unsafe fn isltv(vm: Vm, jt: Jt, ip: Ip, args: Isltv, sp: Sp, lp: Lp) -> Control {
    let dst = sp.at(args.dst());
    let lhs = *sp.at(args.lhs());
    let rhs = *sp.at(args.rhs());

    let result = try_compare!(lhs, rhs, vm, ip, <, fail:true);

    *dst = ValueRaw::Bool(result);

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn islev(vm: Vm, jt: Jt, ip: Ip, args: Islev, sp: Sp, lp: Lp) -> Control {
    let dst = sp.at(args.dst());
    let lhs = *sp.at(args.lhs());
    let rhs = *sp.at(args.rhs());

    let result = try_compare!(lhs, rhs, vm, ip, <=, fail:true);

    *dst = ValueRaw::Bool(result);

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn isgtv(vm: Vm, jt: Jt, ip: Ip, args: Isgtv, sp: Sp, lp: Lp) -> Control {
    let dst = sp.at(args.dst());
    let lhs = *sp.at(args.lhs());
    let rhs = *sp.at(args.rhs());

    let result = try_compare!(lhs, rhs, vm, ip, >, fail:true);

    *dst = ValueRaw::Bool(result);

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn isgev(vm: Vm, jt: Jt, ip: Ip, args: Isgev, sp: Sp, lp: Lp) -> Control {
    let dst = sp.at(args.dst());
    let lhs = *sp.at(args.lhs());
    let rhs = *sp.at(args.rhs());

    let result = try_compare!(lhs, rhs, vm, ip, >=, fail:true);

    *dst = ValueRaw::Bool(result);

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn iseqv(vm: Vm, jt: Jt, ip: Ip, args: Iseqv, sp: Sp, lp: Lp) -> Control {
    let dst = sp.at(args.dst());
    let lhs = *sp.at(args.lhs());
    let rhs = *sp.at(args.rhs());

    let result = try_compare!(lhs, rhs, vm, ip, ==, fail:false);

    *dst = ValueRaw::Bool(result);

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn isnev(vm: Vm, jt: Jt, ip: Ip, args: Isnev, sp: Sp, lp: Lp) -> Control {
    let dst = sp.at(args.dst());
    let lhs = *sp.at(args.lhs());
    let rhs = *sp.at(args.rhs());

    let result = try_compare!(lhs, rhs, vm, ip, !=, fail:false);

    *dst = ValueRaw::Bool(result);

    dispatch_next(vm, jt, ip, sp, lp)
}

macro_rules! try_arith_eval {
    ($dst:ident, $lhs:ident, $rhs:ident, $vm:ident, $ip:ident, $op:tt) => {
        use ValueRaw::*;
        match ($lhs, $rhs) {
            (Int(lhs), Int(rhs)) => {
                *$dst = Int(lhs $op rhs);
            }
            (Float(lhs), Float(rhs)) => {
                *$dst = Float(lhs $op rhs);
            }
            (Float(lhs), Int(rhs)) => {
                *$dst = Float(lhs $op (rhs as f64));
            }
            (Int(lhs), Float(rhs)) => {
                *$dst = Float((lhs as f64) $op rhs);
            }
            _ => {
                vm_exit!($vm, $ip, ArithTypeError);
            }
        }
    };
}

#[inline(always)]
unsafe fn addvv(vm: Vm, jt: Jt, ip: Ip, args: Addvv, sp: Sp, lp: Lp) -> Control {
    let dst = sp.at(args.dst());
    let lhs = *sp.at(args.lhs());
    let rhs = *sp.at(args.rhs());

    try_arith_eval!(dst, lhs, rhs, vm, ip, +);

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn addvn(vm: Vm, jt: Jt, ip: Ip, args: Addvn, sp: Sp, lp: Lp) -> Control {
    let dst = sp.at(args.dst());
    let lhs = *sp.at(args.lhs());
    let rhs = lp.int_or_float_unchecked(args.rhs());

    try_arith_eval!(dst, lhs, rhs, vm, ip, +);

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn addnv(vm: Vm, jt: Jt, ip: Ip, args: Addnv, sp: Sp, lp: Lp) -> Control {
    let dst = sp.at(args.dst());
    let lhs = lp.int_or_float_unchecked(args.lhs());
    let rhs = *sp.at(args.rhs());

    try_arith_eval!(dst, lhs, rhs, vm, ip, +);

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn subvv(vm: Vm, jt: Jt, ip: Ip, args: Subvv, sp: Sp, lp: Lp) -> Control {
    let dst = sp.at(args.dst());
    let lhs = *sp.at(args.lhs());
    let rhs = *sp.at(args.rhs());

    try_arith_eval!(dst, lhs, rhs, vm, ip, -);

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn subvn(vm: Vm, jt: Jt, ip: Ip, args: Subvn, sp: Sp, lp: Lp) -> Control {
    let dst = sp.at(args.dst());
    let lhs = *sp.at(args.lhs());
    let rhs = lp.int_or_float_unchecked(args.rhs());

    try_arith_eval!(dst, lhs, rhs, vm, ip, -);

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn subnv(vm: Vm, jt: Jt, ip: Ip, args: Subnv, sp: Sp, lp: Lp) -> Control {
    let dst = sp.at(args.dst());
    let lhs = lp.int_or_float_unchecked(args.lhs());
    let rhs = *sp.at(args.rhs());

    try_arith_eval!(dst, lhs, rhs, vm, ip, -);

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn mulvv(vm: Vm, jt: Jt, ip: Ip, args: Mulvv, sp: Sp, lp: Lp) -> Control {
    let dst = sp.at(args.dst());
    let lhs = *sp.at(args.lhs());
    let rhs = *sp.at(args.rhs());

    try_arith_eval!(dst, lhs, rhs, vm, ip, *);

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn mulvn(vm: Vm, jt: Jt, ip: Ip, args: Mulvn, sp: Sp, lp: Lp) -> Control {
    let dst = sp.at(args.dst());
    let lhs = *sp.at(args.lhs());
    let rhs = lp.int_or_float_unchecked(args.rhs());

    try_arith_eval!(dst, lhs, rhs, vm, ip, *);

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn mulnv(vm: Vm, jt: Jt, ip: Ip, args: Mulnv, sp: Sp, lp: Lp) -> Control {
    let dst = sp.at(args.dst());
    let lhs = lp.int_or_float_unchecked(args.lhs());
    let rhs = *sp.at(args.rhs());

    try_arith_eval!(dst, lhs, rhs, vm, ip, *);

    dispatch_next(vm, jt, ip, sp, lp)
}

macro_rules! try_div_eval {
    ($dst:ident, $lhs:ident, $rhs:ident, $vm:ident, $ip:ident) => {
        use ValueRaw::*;
        match ($lhs, $rhs) {
            (Int(lhs), Int(rhs)) => match lhs.checked_div(rhs) {
                Some(v) => *$dst = Int(v),
                None => {
                    vm_exit!($vm, $ip, DivisionByZero)
                }
            },

            // float div by zero = inf
            (Float(lhs), Float(rhs)) => {
                *$dst = Float(lhs / rhs);
            }
            (Float(lhs), Int(rhs)) => {
                *$dst = Float(lhs / (rhs as f64));
            }
            (Int(lhs), Float(rhs)) => {
                *$dst = Float((lhs as f64) / rhs);
            }
            _ => {
                vm_exit!($vm, $ip, ArithTypeError);
            }
        }
    };
}

#[inline(always)]
unsafe fn divvv(vm: Vm, jt: Jt, ip: Ip, args: Divvv, sp: Sp, lp: Lp) -> Control {
    let dst = sp.at(args.dst());
    let lhs = *sp.at(args.lhs());
    let rhs = *sp.at(args.rhs());

    try_div_eval!(dst, lhs, rhs, vm, ip);

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn divvn(vm: Vm, jt: Jt, ip: Ip, args: Divvn, sp: Sp, lp: Lp) -> Control {
    let dst = sp.at(args.dst());
    let lhs = *sp.at(args.lhs());
    let rhs = lp.int_or_float_unchecked(args.rhs());

    try_div_eval!(dst, lhs, rhs, vm, ip);

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn divnv(vm: Vm, jt: Jt, ip: Ip, args: Divnv, sp: Sp, lp: Lp) -> Control {
    let dst = sp.at(args.dst());
    let lhs = lp.int_or_float_unchecked(args.lhs());
    let rhs = *sp.at(args.rhs());

    try_div_eval!(dst, lhs, rhs, vm, ip);

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn unm(vm: Vm, jt: Jt, ip: Ip, args: Unm, sp: Sp, lp: Lp) -> Control {
    let dst = sp.at(args.dst());
    let rhs = *sp.at(args.rhs());

    use ValueRaw::*;
    match rhs {
        Int(rhs) => {
            *dst = Int(-rhs);
        }
        ValueRaw::Float(rhs) => {
            *dst = Float(-rhs);
        }
        _ => {
            vm_exit!(vm, ip, ArithTypeError);
        }
    }

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn not(vm: Vm, jt: Jt, ip: Ip, args: Not, sp: Sp, lp: Lp) -> Control {
    let dst = sp.at(args.dst());
    let rhs = *sp.at(args.rhs());

    *dst = ValueRaw::Bool(!rhs.coerce_bool());

    dispatch_next(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn call(vm: Vm, jt: Jt, ip: Ip, args: Call, sp: Sp, lp: Lp) -> Control {
    let ret = args.dst();
    let callee = *sp.at(args.callee());
    let nargs = args.args().get();

    do_generic_call(callee, ret, nargs, vm, jt, sp, lp, ip)
}

#[cold]
unsafe fn arity_mismatch_error(ip: Ip, vm: Vm) -> Control {
    vm_exit!(vm, ip, ArityMismatch)
}

#[cold]
unsafe fn not_callable_error(ip: Ip, vm: Vm) -> Control {
    vm_exit!(vm, ip, NotCallable)
}

#[inline(always)]
unsafe fn fastcall(vm: Vm, jt: Jt, ip: Ip, args: Fastcall, sp: Sp, lp: Lp) -> Control {
    let ret = args.dst();
    let callee = vm.get_function_in_current_module(args.id());

    // SAFETY: We're in the middle of executing bytecode, so current call frame is guaranteed
    // to have a script callee.
    let current_function_start =
        Function::code_raw(vm.current_frame().callee().into_script().unwrap_unchecked());
    // Return addr points to the next instruction after the call instruction.
    let return_addr = 1 + ip.offset_from_unsigned(current_function_start) as u32;

    let (sp, lp, ip) = prepare_call(callee, ret, vm, return_addr);

    dispatch_current(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn hostcall(vm: Vm, jt: Jt, ip: Ip, args: Hostcall, sp: Sp, lp: Lp) -> Control {
    let ret = args.dst();
    let callee = vm.get_host_function(args.id());

    let nargs = callee.as_ref().arity;

    // SAFETY: We're in the middle of executing bytecode, so current call frame is guaranteed
    // to have a script callee.
    let current_function_start =
        Function::code_raw(vm.current_frame().callee().into_script().unwrap_unchecked());
    // Return addr points to the next instruction after the call instruction.
    let return_addr = 1 + ip.offset_from_unsigned(current_function_start) as u32;

    do_host_call(callee, vm, jt, sp, lp, ip, ret, nargs, return_addr)
}

unsafe fn import(vm: Vm, jt: Jt, ip: Ip, args: Import, sp: Sp, lp: Lp) -> Control {
    let info = lp.import_proto_unchecked(args.id());

    let spec = info.as_ref().spec();
    let Some(module) = (*vm.0.as_ptr()).registry.get(spec.as_ptr()) else {
        return module_not_found_error(ip, vm);
    };

    match info.as_ref().bindings() {
        ImportBindings::Bare(ImportBinding::Reg(dst)) => {
            // load the whole module object
            *sp.at(*dst) = ValueRaw::Object(module.as_any());
        }
        ImportBindings::Bare(ImportBinding::Mvar(dst)) => {
            *vm.var_in_current_module(*dst) = ValueRaw::Object(module.as_any());
        }
        ImportBindings::Named(items) => {
            // load parts of the module
            for (key, dst) in items {
                let Some(item) = module.as_ref().get(*key) else {
                    return module_item_not_found_error(ip, vm);
                };
                match dst {
                    ImportBinding::Reg(dst) => *sp.at(*dst) = ValueRaw::Object(item),
                    ImportBinding::Mvar(dst) => {
                        *vm.var_in_current_module(*dst) = ValueRaw::Object(item)
                    }
                }
            }
        }
    }

    dispatch_next(vm, jt, ip, sp, lp)
}

#[cold]
unsafe fn module_not_found_error(ip: Ip, vm: Vm) -> Control {
    vm_exit!(vm, ip, ModuleNotFound);
}

#[cold]
unsafe fn module_item_not_found_error(ip: Ip, vm: Vm) -> Control {
    vm_exit!(vm, ip, ModuleItemNotFound);
}

#[inline(always)]
unsafe fn ret(vm: Vm, jt: Jt, ip: Ip, args: Ret, sp: Sp, lp: Lp) -> Control {
    let (sp, lp, ip) = match return_from_call(vm) {
        Some(v) => v,
        None => return Control::stop(),
    };

    dispatch_current(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn retv(vm: Vm, jt: Jt, ip: Ip, args: Retv, sp: Sp, lp: Lp) -> Control {
    *sp.ret() = *sp.at(args.src());

    let (sp, lp, ip) = match return_from_call(vm) {
        Some(v) => v,
        None => return Control::stop(),
    };

    dispatch_current(vm, jt, ip, sp, lp)
}

#[inline(always)]
unsafe fn stop(vm: Vm, jt: Jt, ip: Ip, args: Stop, sp: Sp, lp: Lp) -> Control {
    Control::stop()
}

#[inline(always)]
unsafe fn do_generic_call(
    callee: ValueRaw,
    ret: Reg,
    nargs: u8,
    vm: Vm,
    jt: Jt,
    sp: Sp,
    lp: Lp,
    ip: Ip,
) -> Control {
    // SAFETY: We're in the middle of executing bytecode, so current call frame is guaranteed
    // to have a script callee.
    let current_function_start =
        Function::code_raw(vm.current_frame().callee().into_script().unwrap_unchecked());
    // Return addr points to the next instruction after the call instruction.
    let return_addr = 1 + ip.offset_from_unsigned(current_function_start) as u32;

    if let Some(callee) = callee.into_object::<Function>() {
        if callee.as_ref().nparams != nargs {
            return arity_mismatch_error(ip, vm);
        }

        let jt = JT.as_ptr();
        let (sp, lp, ip) = prepare_call(callee, ret, vm, return_addr);

        dispatch_current(vm, jt, ip, sp, lp)
    } else if let Some(callee) = callee.into_object::<Closure>() {
        if callee.as_ref().func.as_ref().nparams != nargs {
            return arity_mismatch_error(ip, vm);
        }

        let jt = JT.as_ptr();
        let (sp, lp, ip) = prepare_closure_call(callee, ret, vm, return_addr);

        dispatch_current(vm, jt, ip, sp, lp)
    } else if let Some(callee) = callee.into_object::<HostFunction>() {
        if callee.as_ref().arity != nargs {
            return arity_mismatch_error(ip, vm);
        }

        do_host_call(callee, vm, jt, sp, lp, ip, ret, nargs, return_addr)
    } else {
        not_callable_error(ip, vm)
    }
}

/// Call procedure:
/// 1. Grow stack if needed
/// 2. Allocate new call frame
/// 3. Update VM state
/// 4. Jump to start of callee
///
/// This function does not check arity.
///
/// New call frame's stack overlaps with the current frame's stack
/// example: assuming 3 args, with ret at r6:
/// ```text,ignore
///   frame N:   [ 0 1 2 3 4 5 6 7 8 9 ]
///   frame N+1:             [ 6 7 8 9 ... ]
///                         ret^ ^args
/// ```
/// `r0` in the new frame will be in the same location as `r6`
/// in the previous frame.
#[inline(always)]
unsafe fn prepare_call(
    callee: GcPtr<Function>,
    ret: Reg,
    vm: Vm,
    return_addr: u32,
) -> (Sp, Lp, Ip) {
    // See doc comment.
    let stack_base = vm.current_frame().stack_base() + (ret.get() as u32);
    let frame_size = callee.as_ref().stack_size();

    let sp: Sp = maybe_grow_stack(vm, stack_base as usize, frame_size);
    let lp: Lp = Function::literals_raw(callee);
    let ip: Ip = Function::code_raw(callee);
    vm.set_current_module_for(callee);
    vm.push_frame(CallFrame::script(callee, stack_base, return_addr));

    (sp, lp, ip)
}

/// Exactly like `do_call`, but also sets upvalue ptr.
#[inline(always)]
unsafe fn prepare_closure_call(
    callee: GcPtr<Closure>,
    ret: Reg,
    vm: Vm,
    return_addr: u32,
) -> (Sp, Lp, Ip) {
    vm.set_current_upvalues_for(callee);

    prepare_call(callee.as_ref().func, ret, vm, return_addr)
}

/// Call a host function.
///
/// This uses the native stack for the host function, which it invokes by function ptr.
#[inline(always)]
unsafe fn do_host_call(
    callee: GcPtr<HostFunction>,

    vm: Vm,
    jt: Jt,
    sp: Sp,
    lp: Lp,
    ip: Ip,
    ret: Reg,
    nargs: u8,
    return_addr: u32,
) -> Control {
    let context = {
        let stack_base = vm.current_frame().stack_base() + (ret.get() as u32);
        debug_assert!(
            vm.has_enough_stack_space(stack_base as usize, 1 + callee.as_ref().arity as usize)
        );
        vm.push_frame(CallFrame::host(callee, stack_base, return_addr));
        let sp: Sp = vm.stack_at(stack_base as usize);
        Context::new(vm, sp, nargs)
    };

    let result = (callee.as_ref().f)(context);

    match result {
        Ok(value) => {
            *sp.at(ret) = value;

            debug_assert!(
                vm.current_frame()
                    .callee()
                    .into_host()
                    .is_some_and(|c| { c.into_raw() == callee.into_raw() })
            );
            vm.pop_frame_unchecked();

            dispatch_next(vm, jt, ip, sp, lp)
        }
        Err(err) => {
            vm.set_saved_ip(ip);
            vm.write_error(err);

            vm.unwind();
            debug_assert!(
                vm.current_frame()
                    .callee()
                    .into_host()
                    .is_some_and(|c| { c.into_raw() == callee.into_raw() }),
                "expected frame {} but got {}",
                callee.as_ref().name.as_ref().as_str(),
                vm.current_frame().name().as_ref().as_str(),
            );
            vm.pop_frame_unchecked();

            Control::error(VmError::Host)
        }
    }
}

/// If the stack does not have enough space, grow it.
#[inline(always)]
unsafe fn maybe_grow_stack(vm: Vm, stack_base: usize, frame_size: usize) -> Sp {
    // TODO(?): always grow a fixed 255
    if !vm.has_enough_stack_space(stack_base, frame_size) {
        grow_stack(vm, frame_size);
    }

    vm.stack_at(stack_base)
}

#[cold]
unsafe fn grow_stack(vm: Vm, frame_size: usize) {
    // NOTE: We allocate more than we need here, capacity doubles each time anyway.
    vm.grow_stack(frame_size);
}

#[inline(always)]
unsafe fn return_from_call(vm: Vm) -> Option<(Sp, Lp, Ip)> {
    // Only called from `ret`, meaning we are guaranteed to have
    // at least the one call frame which is currently being executed.

    let returning_from = vm.pop_frame_unchecked();
    let returning_to = vm.current_frame();

    let Some(callee) = returning_to.callee().into_script() else {
        // Yield to host
        return None;
    };
    debug_assert!(callee.as_any().is::<Function>());

    let stack_base = returning_to.stack_base() as usize;
    let return_addr = returning_from.return_addr as usize;

    let sp: Sp = vm.stack_at(stack_base);
    let lp: Lp = Function::literals_raw(callee);
    let ip: Ip = Function::code_raw(callee).offset(return_addr as isize);
    vm.set_current_module_for(callee);

    Some((sp, lp, ip))
}

pub trait StdioWrite: std::io::Write + std::any::Any + 'static {
    fn as_any(&self) -> &dyn std::any::Any;
    fn as_any_mut(&mut self) -> &mut dyn std::any::Any;
}
impl<T: std::io::Write + std::any::Any> StdioWrite for T {
    fn as_any(&self) -> &dyn std::any::Any {
        self
    }

    fn as_any_mut(&mut self) -> &mut dyn std::any::Any {
        self
    }
}

pub struct Stdio {
    pub stdout: Box<dyn StdioWrite>,
    pub stderr: Box<dyn StdioWrite>,
}

impl Default for Stdio {
    fn default() -> Self {
        Self {
            stdout: Box::new(std::io::stdout()),
            stderr: Box::new(std::io::stderr()),
        }
    }
}

type Invariant<'a> = PhantomData<fn(&'a ()) -> &'a ()>;

#[repr(C)]
pub struct Hebi {
    /// Boxed to guarantee address stability
    inner: Box<VmState>,
}

pub(crate) struct VmState {
    /// Garbage collected heap
    heap: gc::Heap,

    /// Storage for loaded modules
    registry: ModuleRegistry,

    core: RuntimeCoreLib,

    stdio: Stdio,

    /// VM "registers"
    stack: DynArray<ValueRaw>,

    /// Invariant: Should never be empty.
    frames: DynStack<CallFrame>,

    /// When executing a function, we store the module it belongs to here,
    /// so that we can `fastcall` other functions from the same module.
    ///
    /// NOTE: This is always `Some` while the VM is executing
    current_module: Option<GcPtr<ModuleProto>>,

    current_upvalues: Option<Upvalues>,

    entrypoint: GcPtr<HostFunction>,

    /// Saved error.
    ///
    /// Used to avoid bloating size of instruction handler return value.
    error: Option<Error>,

    /// Saved instruction pointer.
    ///
    /// Used when the VM encounters an error, can be traced back to its
    /// associated span by the error handler.
    saved_ip: Option<Ip>,
}

impl Hebi {
    pub fn new() -> Self {
        // 1 MiB
        const INITIAL_STACK_SIZE: usize = (1024 * 1024) / std::mem::size_of::<ValueRaw>();
        const STACK_DEPTH: usize = INITIAL_STACK_SIZE / 16;

        let heap = gc::Heap::new();

        let entrypoint = {
            let name = Str::alloc(&heap, "@start");
            HostFunction::alloc(&heap, name, 0, std::rc::Rc::new(|_| unreachable!()))
        };

        Hebi {
            inner: Box::new(VmState {
                registry: ModuleRegistry::new(),
                core: RuntimeCoreLib::init(&heap),
                stdio: Stdio::default(),
                stack: DynArray::new(INITIAL_STACK_SIZE),
                frames: DynStack::new(STACK_DEPTH),

                current_module: None,
                current_upvalues: None,

                entrypoint,

                error: None,
                saved_ip: None,

                heap,
            }),
        }
    }

    pub fn with_stdio(mut self, stdio: Stdio) -> Self {
        self.inner.stdio = stdio;
        self
    }

    #[inline(always)]
    pub fn enter<'gc>(&'gc mut self) -> Runtime<'gc> {
        Runtime {
            vm: NonNull::from_mut(&mut *self.inner),
            _guard: vm_guard::VmGuard::new(),
            _lifetime: PhantomData,
        }
    }
}

mod vm_guard {
    thread_local! {
        static ENTERED: std::cell::Cell<bool> = const { std::cell::Cell::new(false) };
    }

    pub(super) struct VmGuard {}

    impl VmGuard {
        pub(super) fn new() -> Self {
            assert!(!ENTERED.get(), "Only one VM may be active per thread");
            ENTERED.set(true);
            Self {}
        }
    }

    impl Drop for VmGuard {
        fn drop(&mut self) {
            ENTERED.set(false)
        }
    }
}

struct VmRoots {
    state: NonNull<VmState>,
}

impl gc::ExternalRoots for VmRoots {
    unsafe fn trace(&self, tracer: &gc::Tracer) {
        // NOTE: When adding new state, make sure to trace it as external roots!
        fn _reminder(v: &VmState) {
            let VmState {
                heap,
                registry,
                core,
                stdio,
                stack,
                frames,
                current_module,
                current_upvalues,
                entrypoint,
                error,
                saved_ip,
            } = todo!();
        }

        macro_rules! this {
            () => {
                *self.state.as_ptr()
            };
        }

        debug_print!(
            "trace vm roots (nframes={}, nstack={})",
            this!().frames.len(),
            this!()
                .frames
                .top()
                .map(|f| 1
                    + ((*f).stack_base as usize)
                    + (*f)
                        .callee
                        .into_script()
                        .map(|v| v.as_ref().stack_size())
                        .unwrap_or_default())
                .unwrap_or_default()
        );

        // call stack (holds functions)
        for frame in this!().frames.iter() {
            if let Some(callee) = frame.callee.into_host() {
                tracer.visit(callee);
            } else if let Some(callee) = frame.callee.into_script() {
                tracer.visit(callee);
            } else {
                unreachable!();
            }
        }

        // value stack (registers)
        let frames = &mut this!().frames;
        let sp = this!().stack.offset(0);
        if let Some(frame) = frames.top() {
            if let Some(callee) = (*frame).callee.into_script() {
                let base = (*frame).stack_base as usize;
                let nstack = callee.as_ref().stack_size();
                let stack_len = base + nstack;

                debug_print!("stack len: {stack_len}");
                for i in 0..stack_len {
                    debug_print!("stack[{i}]");
                    tracer.visit_value(sp.add(i).read());
                }
                debug_print!("stack end");
            }
        }

        tracer.visit(this!().entrypoint);

        // modules
        this!().registry.trace(tracer);
        if let Some(m) = this!().current_module {
            tracer.visit(m);
        }

        // core lib
        this!().core.trace(tracer);
    }
}

#[repr(transparent)]
pub struct Module<'vm> {
    inner: GcPtr<ModuleProto>,
    _lifetime: PhantomData<fn(&'vm ()) -> &'vm ()>,
}

pub struct Runtime<'vm> {
    vm: NonNull<VmState>,

    _guard: vm_guard::VmGuard,
    _lifetime: Invariant<'vm>,
}

// TODO: none of the public APIs should return `ValueRaw`.
// Currently they are kept alive by the fact that we only
// run gc during allocating instructions (`larr` and friends).
impl<'vm> Runtime<'vm> {
    pub fn stdio(&mut self) -> &mut Stdio {
        unsafe { &mut self.vm.as_mut().stdio }
    }

    /// Load a module into the VM's module registry.
    ///
    /// If you want to load a _native_ module, use [`Self::register`] instead.
    ///
    /// NOTE: Currently, only native modules may be imported by scripts.
    pub fn load(&mut self, m: &module::Module) -> Module<'vm> {
        let vm = unsafe { self.vm.as_mut() };
        let heap = &mut vm.heap;
        let registry = &mut vm.registry;
        let m = registry.add(heap, m);
        Module {
            inner: m,
            _lifetime: PhantomData,
        }
    }

    /// Register a native module in the VM's module registry,
    /// allowing it to be imported by scripts.
    pub fn register(&mut self, m: &module::NativeModule) {
        let vm = unsafe { self.vm.as_mut() };
        let heap = &mut vm.heap;
        let registry = &mut vm.registry;
        registry.add_native(heap, m);
    }

    /// Execute the main entrypoint of the module to completion.
    ///
    /// Note that modules must go through [`Runtime::load`] first.
    pub fn run(&mut self, m: &Module<'vm>) -> Result<ValueRaw> {
        let vm = Vm(self.vm);
        let jt = JT.as_ptr();

        unsafe {
            vm.push_frame(CallFrame::host(vm.entrypoint(), 0, 0));
        }

        let main = unsafe { m.inner.as_ref().main().as_ptr() };
        let value = unsafe {
            let (sp, lp, ip) = prepare_call(
                main,
                Reg::new_unchecked(0),
                vm,
                0, // return to host
            );

            dispatch_loop(vm, jt, ip, sp, lp)
        };

        let result = match value {
            Ok(value) => Ok(value),
            Err(err) => unsafe {
                let err = vm.take_error(err);
                vm.unwind();
                Err(err)
            },
        };

        unsafe {
            debug_assert!((*self.vm.as_ptr()).frames.len() == 1);
            debug_assert!(
                (*self.vm.as_ptr())
                    .frames
                    .top()
                    .is_some_and(|frame| (*frame).callee.into_host().unwrap().into_raw()
                        == (*self.vm.as_ptr()).entrypoint.into_raw())
            );

            vm.pop_frame_unchecked();
        }

        result
    }

    #[cfg(feature = "async")]
    pub async fn run_async(&mut self, m: &Module<'vm>) -> Result<ValueRaw> {
        stackful::stackful(|| self.run(m)).await
    }
}

#[inline]
unsafe fn dispatch_loop(vm: Vm, jt: Jt, ip: Ip, sp: Sp, lp: Lp) -> Result<ValueRaw, VmError> {
    // In debug mode and Wasm, fall back to loop+match.
    //
    // Each instruction will return `Control::Continue`
    // instead of tail-calling the next handler.
    #[cfg(any(debug_assertions, target_arch = "wasm32"))]
    {
        let mut sp: Sp = sp;
        let mut lp: Lp = lp;
        let mut ip: Ip = ip;

        loop {
            let insn = ip.get();
            let op = jt.at(insn);
            match op(vm, jt, ip, insn, sp, lp) {
                Control::Stop => return Ok(*sp.ret()),
                Control::Error(err) => return Err(err),

                #[cfg(any(debug_assertions, target_arch = "wasm32"))]
                Control::Continue(new_sp, new_lp, new_ip) => {
                    sp = new_sp;
                    lp = new_lp;
                    ip = new_ip;
                    continue;
                }
            }
        }
    }

    #[cfg(not(any(debug_assertions, target_arch = "wasm32")))]
    {
        let ctrl = dispatch_current(vm, jt, ip, sp, lp);

        if ctrl.is_error() {
            let err = ctrl.error_code();
            return Err(err);
        }

        return Ok(*sp.ret());
    }
}

#[cfg(test)]
mod tests;
